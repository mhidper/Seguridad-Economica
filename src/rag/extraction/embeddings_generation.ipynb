{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\anaconda3\\envs\\dependencias\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-01 11:54:22,410 - INFO - NumExpr defaulting to 16 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de chunks: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\rag\\extraction\\chunks\\processed\\all_chunks.json\n",
      "Directorio de embeddings: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\rag\\models\\embeddings\n",
      "Directorio de base de datos: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\rag\\models\\indexes\\chroma_db\n",
      "Modelo a utilizar: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 11:54:32,331 - INFO - Cargando chunks desde c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\rag\\extraction\\chunks\\processed\\all_chunks.json\n",
      "2025-04-01 11:54:32,400 - INFO - Cargados 6625 chunks\n",
      "2025-04-01 11:54:32,400 - INFO - Cargando modelo all-MiniLM-L6-v2...\n",
      "2025-04-01 11:54:32,428 - INFO - Use pytorch device_name: cuda\n",
      "2025-04-01 11:54:32,428 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "c:\\Users\\Usuario\\anaconda3\\envs\\dependencias\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "2025-04-01 11:54:38,830 - INFO - Modelo cargado. Generando embeddings...\n",
      "Batches: 100%|██████████| 208/208 [00:10<00:00, 20.54it/s]\n",
      "2025-04-01 11:54:49,260 - INFO - Generados 6625 embeddings de dimensión 384\n",
      "2025-04-01 11:54:49,263 - INFO - Embeddings guardados en c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\rag\\models\\embeddings\\all-MiniLM-L6-v2_embeddings.npy\n",
      "2025-04-01 11:54:49,352 - INFO - Metadatos y textos guardados en c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\rag\\models\\embeddings\\all-MiniLM-L6-v2_metadata.json\n",
      "2025-04-01 11:54:49,353 - INFO - Creando base de datos vectorial en c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\rag\\models\\indexes\\chroma_db\n",
      "2025-04-01 11:54:49,373 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-04-01 11:54:49,932 - INFO - Creada nueva colección 'bibliography'\n",
      "2025-04-01 11:54:57,127 - INFO - Añadidos 6625 documentos a la colección 'bibliography'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# Notebook: embeddings_generation.ipynb\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Intentar importar las bibliotecas necesarias\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "    logger.warning(\"sentence-transformers no está instalado. Necesario para generar embeddings.\")\n",
    "    \n",
    "try:\n",
    "    import chromadb\n",
    "    CHROMADB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CHROMADB_AVAILABLE = False\n",
    "    logger.warning(\"chromadb no está instalado. Necesario para la base de datos vectorial.\")\n",
    "\n",
    "\n",
    "def load_chunks(chunks_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Carga los chunks procesados desde un archivo JSON.\n",
    "    \n",
    "    Args:\n",
    "        chunks_path: Ruta al archivo JSON con todos los chunks\n",
    "        \n",
    "    Returns:\n",
    "        Lista de diccionarios con los chunks y sus metadatos\n",
    "    \"\"\"\n",
    "    logger.info(f\"Cargando chunks desde {chunks_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "            chunks = json.load(f)\n",
    "        logger.info(f\"Cargados {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar chunks: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def generate_embeddings(chunks: List[Dict[str, Any]], model_name: str = \"all-MiniLM-L6-v2\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Genera embeddings para cada chunk de texto utilizando un modelo de sentence-transformers.\n",
    "    \n",
    "    Args:\n",
    "        chunks: Lista de diccionarios con los chunks y sus metadatos\n",
    "        model_name: Nombre del modelo de sentence-transformers a utilizar\n",
    "        \n",
    "    Returns:\n",
    "        Diccionario con textos, embeddings y metadatos\n",
    "    \"\"\"\n",
    "    if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "        raise ImportError(\"sentence-transformers es necesario para generar embeddings.\")\n",
    "    \n",
    "    logger.info(f\"Cargando modelo {model_name}...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    logger.info(\"Modelo cargado. Generando embeddings...\")\n",
    "    \n",
    "    # Extraer textos y metadatos\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    metadatas = [chunk[\"metadata\"] for chunk in chunks]\n",
    "    \n",
    "    # Generar embeddings (usando GPU si está disponible)\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, batch_size=32)\n",
    "    \n",
    "    logger.info(f\"Generados {len(embeddings)} embeddings de dimensión {embeddings.shape[1]}\")\n",
    "    \n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"metadatas\": metadatas\n",
    "    }\n",
    "\n",
    "\n",
    "def save_embeddings(embeddings_data: Dict[str, Any], output_dir: str, model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda los embeddings generados en archivos.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_data: Diccionario con textos, embeddings y metadatos\n",
    "        output_dir: Directorio donde guardar los embeddings\n",
    "        model_name: Nombre del modelo utilizado (para nombrar los archivos)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Guardar embeddings como numpy array\n",
    "    embeddings_path = os.path.join(output_dir, f\"{model_name.replace('/', '_')}_embeddings.npy\")\n",
    "    np.save(embeddings_path, embeddings_data[\"embeddings\"])\n",
    "    logger.info(f\"Embeddings guardados en {embeddings_path}\")\n",
    "    \n",
    "    # Guardar textos y metadatos en JSON\n",
    "    metadata_path = os.path.join(output_dir, f\"{model_name.replace('/', '_')}_metadata.json\")\n",
    "    with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"texts\": embeddings_data[\"texts\"],\n",
    "            \"metadatas\": embeddings_data[\"metadatas\"]\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    logger.info(f\"Metadatos y textos guardados en {metadata_path}\")\n",
    "\n",
    "\n",
    "def create_vector_database(embeddings_data: Dict[str, Any], db_path: str, collection_name: str = \"bibliography\") -> None:\n",
    "    \"\"\"\n",
    "    Crea una base de datos vectorial con Chroma.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_data: Diccionario con textos, embeddings y metadatos\n",
    "        db_path: Ruta donde guardar la base de datos\n",
    "        collection_name: Nombre de la colección en la base de datos\n",
    "    \"\"\"\n",
    "    if not CHROMADB_AVAILABLE:\n",
    "        raise ImportError(\"chromadb es necesario para crear la base de datos vectorial.\")\n",
    "    \n",
    "    logger.info(f\"Creando base de datos vectorial en {db_path}\")\n",
    "    \n",
    "    # Crear cliente y colección persistente\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    # Intentar obtener la colección existente o crear una nueva\n",
    "    try:\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        logger.info(f\"Colección existente '{collection_name}' encontrada. Se añadirán/actualizarán documentos.\")\n",
    "    except Exception:\n",
    "        collection = client.create_collection(name=collection_name)\n",
    "        logger.info(f\"Creada nueva colección '{collection_name}'\")\n",
    "    \n",
    "    # Preparar IDs para los documentos\n",
    "    ids = [f\"chunk_{i}\" for i in range(len(embeddings_data[\"texts\"]))]\n",
    "    \n",
    "    # Añadir documentos a la colección\n",
    "    collection.add(\n",
    "        embeddings=embeddings_data[\"embeddings\"].tolist(),\n",
    "        documents=embeddings_data[\"texts\"],\n",
    "        metadatas=embeddings_data[\"metadatas\"],\n",
    "        ids=ids\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Añadidos {len(ids)} documentos a la colección '{collection_name}'\")\n",
    "\n",
    "\n",
    "# Definir rutas\n",
    "current_dir = pathlib.Path(os.getcwd())\n",
    "chunks_path = os.path.join(current_dir, \"chunks\", \"processed\", \"all_chunks.json\")\n",
    "embeddings_dir = os.path.join(current_dir.parent, \"models\", \"embeddings\")\n",
    "db_dir = os.path.join(current_dir.parent, \"models\", \"indexes\", \"chroma_db\")\n",
    "\n",
    "# Configuración del modelo de embeddings\n",
    "model_name = \"all-MiniLM-L6-v2\"  # Modelo pequeño y rápido, pero puedes usar otros como \"paraphrase-multilingual-MiniLM-L12-v2\" para mejor soporte multilingüe\n",
    "\n",
    "# Imprimir las rutas para verificar\n",
    "print(f\"Archivo de chunks: {chunks_path}\")\n",
    "print(f\"Directorio de embeddings: {embeddings_dir}\")\n",
    "print(f\"Directorio de base de datos: {db_dir}\")\n",
    "print(f\"Modelo a utilizar: {model_name}\")\n",
    "\n",
    "# Preguntar al usuario si las rutas son correctas antes de continuar\n",
    "confirmation = input(\"¿Son correctas estas rutas y configuración? (s/n): \")\n",
    "if confirmation.lower() != 's':\n",
    "    print(\"Proceso cancelado.\")\n",
    "else:\n",
    "    # Cargar chunks\n",
    "    chunks = load_chunks(chunks_path)\n",
    "    \n",
    "    if not chunks:\n",
    "        print(\"No se pudieron cargar los chunks. Verifica la ruta y el formato del archivo.\")\n",
    "    else:\n",
    "        # Generar embeddings\n",
    "        embeddings_data = generate_embeddings(chunks, model_name)\n",
    "        \n",
    "        # Guardar embeddings\n",
    "        save_embeddings(embeddings_data, embeddings_dir, model_name)\n",
    "        \n",
    "        # Crear base de datos vectorial\n",
    "        create_vector_database(embeddings_data, db_dir, \"bibliography\")\n",
    "        \n",
    "        print(\"Proceso completado exitosamente.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dependencias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
