{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook: text_processing.ipynb\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Importar LangChain para el procesamiento de texto\n",
    "try:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    LANGCHAIN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LANGCHAIN_AVAILABLE = False\n",
    "    logger.warning(\"LangChain no está instalado. Por favor, instala langchain.\")\n",
    "\n",
    "\n",
    "def load_text_files(input_dir: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Carga todos los archivos de texto de un directorio.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directorio con archivos de texto\n",
    "        \n",
    "    Returns:\n",
    "        Diccionario con nombres de archivo como claves y contenido como valores\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_dir):\n",
    "        raise FileNotFoundError(f\"El directorio {input_dir} no existe\")\n",
    "    \n",
    "    text_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.txt')]\n",
    "    logger.info(f\"Se encontraron {len(text_files)} archivos de texto en {input_dir}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for text_file in text_files:\n",
    "        text_path = os.path.join(input_dir, text_file)\n",
    "        try:\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            results[text_file] = content\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al leer {text_file}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n",
    "    \"\"\"\n",
    "    Divide un texto en chunks más pequeños usando LangChain.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a dividir\n",
    "        chunk_size: Tamaño aproximado de cada chunk\n",
    "        chunk_overlap: Superposición entre chunks consecutivos\n",
    "        \n",
    "    Returns:\n",
    "        Lista de chunks de texto\n",
    "    \"\"\"\n",
    "    if not LANGCHAIN_AVAILABLE:\n",
    "        raise ImportError(\"LangChain es necesario para dividir el texto. Por favor, instala langchain.\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_all_texts(texts: Dict[str, str], chunk_size: int = 1000, chunk_overlap: int = 200) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Procesa todos los textos y los divide en chunks.\n",
    "    \n",
    "    Args:\n",
    "        texts: Diccionario con nombres de archivo como claves y contenido como valores\n",
    "        chunk_size: Tamaño aproximado de cada chunk\n",
    "        chunk_overlap: Superposición entre chunks consecutivos\n",
    "        \n",
    "    Returns:\n",
    "        Diccionario con nombres de archivo como claves y listas de chunks como valores\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for filename, text in texts.items():\n",
    "        logger.info(f\"Procesando {filename}...\")\n",
    "        \n",
    "        # Extraer nombre base sin extensión\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Dividir en chunks\n",
    "        try:\n",
    "            chunks = split_text_into_chunks(text, chunk_size, chunk_overlap)\n",
    "            \n",
    "            # Crear metadatos para cada chunk\n",
    "            processed_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                processed_chunks.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"source\": filename,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"document\": base_name\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            results[filename] = processed_chunks\n",
    "            logger.info(f\"Se generaron {len(chunks)} chunks para {filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al procesar {filename}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def save_chunks(chunks_dict: Dict[str, List[Dict[str, Any]]], output_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Guarda los chunks procesados en archivos JSON.\n",
    "    \n",
    "    Args:\n",
    "        chunks_dict: Diccionario con nombres de archivo como claves y listas de chunks como valores\n",
    "        output_dir: Directorio donde guardar los chunks\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Guardar cada documento en su propio archivo\n",
    "    for filename, chunks in chunks_dict.items():\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        output_path = os.path.join(output_dir, f\"{base_name}_chunks.json\")\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(f\"Chunks guardados en {output_path}\")\n",
    "    \n",
    "    # Guardar todos los chunks en un solo archivo (útil para cargar todo de una vez)\n",
    "    all_chunks = []\n",
    "    for chunks_list in chunks_dict.values():\n",
    "        all_chunks.extend(chunks_list)\n",
    "    \n",
    "    all_chunks_path = os.path.join(output_dir, \"all_chunks.json\")\n",
    "    with open(all_chunks_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    logger.info(f\"Todos los chunks ({len(all_chunks)}) guardados en {all_chunks_path}\")\n",
    "\n",
    "\n",
    "# Definir rutas\n",
    "current_dir = pathlib.Path(os.getcwd())\n",
    "input_dir = os.path.join(current_dir, \"chunks\", \"raw_text\")  # Donde se guardaron los textos extraídos\n",
    "output_dir = os.path.join(current_dir, \"chunks\", \"processed\")  # Donde guardar los chunks procesados\n",
    "\n",
    "# Imprimir las rutas para verificar\n",
    "print(f\"Directorio de entrada: {input_dir}\")\n",
    "print(f\"Directorio de salida: {output_dir}\")\n",
    "\n",
    "# Preguntar al usuario si las rutas son correctas antes de continuar\n",
    "confirmation = input(\"¿Son correctas estas rutas? (s/n): \")\n",
    "if confirmation.lower() != 's':\n",
    "    print(\"Proceso cancelado.\")\n",
    "else:\n",
    "    # Cargar textos\n",
    "    texts = load_text_files(input_dir)\n",
    "    print(f\"Se cargaron {len(texts)} archivos de texto\")\n",
    "    \n",
    "    # Procesar textos\n",
    "    chunk_size = 1000  # Tamaño de chunk en caracteres (ajustar según necesidad)\n",
    "    chunk_overlap = 200  # Superposición entre chunks (ajustar según necesidad)\n",
    "    chunks_dict = process_all_texts(texts, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Guardar chunks procesados\n",
    "    save_chunks(chunks_dict, output_dir)\n",
    "    \n",
    "    # Mostrar ejemplo de un chunk\n",
    "    if chunks_dict:\n",
    "        first_file = list(chunks_dict.keys())[0]\n",
    "        if chunks_dict[first_file]:\n",
    "            first_chunk = chunks_dict[first_file][0]\n",
    "            print(\"\\nEjemplo de chunk procesado:\")\n",
    "            print(f\"Documento: {first_chunk['metadata']['source']}\")\n",
    "            print(f\"Chunk ID: {first_chunk['metadata']['chunk_id']}\")\n",
    "            print(f\"Texto: {first_chunk['text'][:300]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dependencias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
