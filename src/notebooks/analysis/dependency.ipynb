{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\notebooks\\analysis\\dependency.ipynb to html\n",
      "[NbConvertApp] Writing 603478 bytes to C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\notebooks\\analysis\\dependency.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html \"C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\notebooks\\analysis\\dependency.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 align=\"center\"><strong>Cálculo de dependencia cruzada para todos los países</strong></h1>\n",
    "<h4 align=\"center\"><strong>Manuel Alejandro Hidalgo y Jorge Díaz Lanchas</strong></h4>\n",
    "<h4 align=\"center\"><strong>Fundación Real Instituto Elcano</strong></h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esquema para Índice de Seguridad Económica - Real Instituto Elcano\n",
    "\n",
    "## 1. Introducción y Marco Conceptual\n",
    "\n",
    "- **Objetivo**: Desarrollar un índice que cuantifique la seguridad económica de los países\n",
    "- **Definición**: La seguridad económica como la capacidad de un país para resistir disrupciones en sus cadenas de suministro y comercio internacional\n",
    "- **Relevancia**: Contexto actual de fragmentación geoeconómica y tensiones comerciales\n",
    "\n",
    "## 2. Metodología\n",
    "\n",
    "### 2.1 Fuentes de Datos\n",
    "- Base de datos International Trade and Production Database (ITP)\n",
    "- Datos comerciales bilaterales por industria (año 2019)\n",
    "- Otros indicadores macroeconómicos complementarios\n",
    "\n",
    "### 2.2 Procesamiento de Datos\n",
    "```python\n",
    "# Procesamiento y carga de datos ITP\n",
    "itp2019, codigos_countries = procesar_datos_itp()\n",
    "```\n",
    "\n",
    "### 2.3 Creación de Matrices de Comercio\n",
    "```python\n",
    "# Generación de matrices bilaterales por industria\n",
    "matrices_comercio = crear_matriz_comercio(data.groupby('industry_descr'), codigos_paises)\n",
    "```\n",
    "\n",
    "### 2.4 Limpieza y Filtrado\n",
    "```python\n",
    "# Eliminar relaciones comerciales insignificantes\n",
    "mat_clean = eliminar_filas_columnas_cero(mat, threshold_pct=0.05)\n",
    "```\n",
    "\n",
    "### 2.5 Cálculo de Dependencias Económicas\n",
    "```python\n",
    "# Cálculo de dependencias directas e indirectas\n",
    "results = analyze_dependencies(X, country_names)\n",
    "```\n",
    "\n",
    "## 3. Componentes del Índice\n",
    "\n",
    "### 3.1 Dependencia Directa\n",
    "- Medición de la dependencia inmediata entre países\n",
    "- Fórmula: $D_{ij} = \\frac{X_{ji}}{∑_k X_{ki}}$, donde $X_{ji}$ es el comercio del país j al país i\n",
    "\n",
    "### 3.2 Dependencia Indirecta\n",
    "- Medición de dependencias a través de cadenas de suministro\n",
    "- Incorporación de países intermediarios en las relaciones comerciales\n",
    "- Análisis de caminos hasta longitud 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import torch\n",
    "import dask.dataframe as dd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***No ejecutar este código a menos que se quiera comprimir***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error durante el proceso: No se encuentra el archivo: Datos\\ITP\\ITPD_E_R02.csv\n"
     ]
    }
   ],
   "source": [
    "def comprimir_dividir_archivo(archivo_original, tamano_maximo=100, directorio_salida=None):\n",
    "    # Asegúrate de que el archivo original existe\n",
    "    archivo_original = Path(archivo_original)\n",
    "    if not archivo_original.exists():\n",
    "        raise FileNotFoundError(f\"No se encuentra el archivo: {archivo_original}\")\n",
    "    \n",
    "    # Si no se especifica directorio de salida, usar src/data/raw/ITP/\n",
    "    if directorio_salida is None:\n",
    "        # Obtener el directorio raíz del proyecto (donde está src/)\n",
    "        proyecto_root = Path(__file__).parent.parent.parent\n",
    "        directorio_salida = proyecto_root / 'src' / 'data' / 'raw' / 'ITP'\n",
    "    else:\n",
    "        directorio_salida = Path(directorio_salida)\n",
    "    \n",
    "    # Crear el directorio de salida si no existe\n",
    "    directorio_salida.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Abre el archivo original en modo de lectura binaria\n",
    "    with open(archivo_original, 'rb') as f_in:\n",
    "        # Lee el contenido del archivo original\n",
    "        contenido = f_in.read()\n",
    "        \n",
    "        # Determina el número de partes necesarias\n",
    "        num_partes = (len(contenido) + tamano_maximo - 1) // tamano_maximo\n",
    "        \n",
    "        # Divide el contenido en partes y escribe cada parte comprimida\n",
    "        for i in range(num_partes):\n",
    "            parte = contenido[i * tamano_maximo: (i + 1) * tamano_maximo]\n",
    "            archivo_salida = directorio_salida / f'ITPD_E_R02.csv.parte{i}.gz'\n",
    "            with gzip.open(archivo_salida, 'wb') as f_out:\n",
    "                f_out.write(parte)\n",
    "            print(f\"Parte {i} creada en: {archivo_salida}\")\n",
    "\n",
    "# Tamaño máximo por parte (1GB)\n",
    "tamano_maximo = 1000 * 1024 * 1024\n",
    "\n",
    "try:\n",
    "    # Ruta al archivo original\n",
    "    archivo_original = Path(r'Datos/ITP/ITPD_E_R02.csv')\n",
    "    \n",
    "    # Comprimir y dividir el archivo original\n",
    "    comprimir_dividir_archivo(archivo_original, tamano_maximo)\n",
    "    print(\"Proceso completado con éxito\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el proceso: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Descomprimir, carga de datos y borrado de archivo***\n",
    "\n",
    "La compresión se hace para poder trabajar con git sin porblemas de tamaño de ficheros.\n",
    "Se descomprime, se importa y luego se borra el fichero descomprimido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Directorio fuente: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\raw\\ITP\n",
      "Directorio destino: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\n",
      "Combinando archivos comprimidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo CSV...\n",
      "Usando Dask para procesamiento en paralelo\n",
      "Filtrando datos de 2019...\n",
      "Archivo temporal eliminado\n",
      "Total de países únicos encontrados: 237\n",
      "Procesamiento completado con éxito\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FASE 1: PREPARACIÓN Y CARGA DE DATOS\n",
    "Este script procesa la base de datos International Trade and Production Database (ITP)\n",
    "que viene dividida en múltiples archivos comprimidos, utilizando aceleración GPU\n",
    "cuando está disponible.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def procesar_datos_itp():\n",
    "   try:\n",
    "       # Verificar si GPU está disponible\n",
    "       gpu_disponible = torch.cuda.is_available()\n",
    "       if gpu_disponible:\n",
    "           print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "       else:\n",
    "           print(\"GPU no disponible, se usará CPU\")\n",
    "       \n",
    "       # Definición de rutas usando Path y la estructura de tu proyecto\n",
    "       try:\n",
    "           base_path = Path(__file__).parent\n",
    "       except NameError:  # Estamos en un notebook\n",
    "           base_path = Path.cwd().parent.parent  # Asumiendo que el notebook está en /notebooks/\n",
    "\n",
    "       source_directory = base_path / \"data\" / \"raw\" / \"ITP\"\n",
    "       target_directory = base_path / \"data\" / \"processed\"\n",
    "       target_filename = 'ITPD_E_R02.csv'\n",
    "\n",
    "       # Imprimir las rutas para verificación\n",
    "       print(f\"Directorio fuente: {source_directory}\")\n",
    "       print(f\"Directorio destino: {target_directory}\")\n",
    "\n",
    "       # Asegurar que los directorios existen\n",
    "       target_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "       # Verificar que el directorio fuente existe\n",
    "       if not source_directory.exists():\n",
    "           raise FileNotFoundError(f\"No se encuentra el directorio fuente: {source_directory}\")\n",
    "\n",
    "       # Listar archivos comprimidos\n",
    "       chunk_filenames = sorted([f for f in os.listdir(source_directory) \n",
    "                        if f.startswith('ITPD_E_R02.csv.parte') and f.endswith('.gz')])\n",
    "\n",
    "       # Control de errores: verificar que existen archivos para procesar\n",
    "       if not chunk_filenames:\n",
    "           raise FileNotFoundError(f\"No se encontraron archivos .gz en {source_directory}\")\n",
    "\n",
    "       # Construir la ruta completa para el archivo combinado\n",
    "       target_filepath = target_directory / target_filename\n",
    "\n",
    "       # Función para descomprimir un archivo en paralelo\n",
    "       def descomprimir_archivo(chunk_filename):\n",
    "           chunk_filepath = source_directory / chunk_filename\n",
    "           with gzip.open(chunk_filepath, 'rb') as chunk_file:\n",
    "               return chunk_file.read()\n",
    "\n",
    "       print(\"Combinando archivos comprimidos...\")\n",
    "       with open(target_filepath, 'wb') as target_file:\n",
    "           # Usar ThreadPoolExecutor para paralelizar la descompresión\n",
    "           with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "               for data in tqdm(\n",
    "                   executor.map(descomprimir_archivo, chunk_filenames),\n",
    "                   total=len(chunk_filenames),\n",
    "                   desc=\"Procesando archivos\"\n",
    "               ):\n",
    "                   target_file.write(data)\n",
    "\n",
    "       print(\"Leyendo archivo CSV...\")\n",
    "       \n",
    "       # Usar Dask para procesamiento paralelo\n",
    "       print(\"Usando Dask para procesamiento en paralelo\")\n",
    "       dask_df = dd.read_csv(target_filepath, sep=\",\")\n",
    "       \n",
    "       # Filtrar año 2019 eficientemente\n",
    "       print(\"Filtrando datos de 2019...\")\n",
    "       itp2019 = dask_df[dask_df['year'] == 2019].compute()\n",
    "       \n",
    "       # Limpieza: eliminar archivo temporal\n",
    "       os.remove(target_filepath)\n",
    "       print(f\"Archivo temporal eliminado\")\n",
    "\n",
    "       # Obtener lista única de países importadores - usando GPU si disponible\n",
    "       if gpu_disponible:\n",
    "           # Transferir a GPU para operaciones de unique más rápidas\n",
    "           importer_tensor = torch.tensor(\n",
    "               pd.factorize(itp2019['importer_iso3'])[0], \n",
    "               device='cuda'\n",
    "           )\n",
    "           unique_indices = torch.unique(importer_tensor).cpu().numpy()\n",
    "           # Mapear índices únicos de vuelta a códigos ISO\n",
    "           factorize_result = pd.factorize(itp2019['importer_iso3'])\n",
    "           codigos_countries = [factorize_result[1][idx] for idx in unique_indices]\n",
    "       else:\n",
    "           codigos_countries = list(itp2019['importer_iso3'].unique())\n",
    "           \n",
    "       print(f\"Total de países únicos encontrados: {len(codigos_countries)}\")\n",
    "\n",
    "       return itp2019, codigos_countries\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"Error durante el procesamiento: {e}\")\n",
    "       raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data, countries = procesar_datos_itp()\n",
    "        print(\"Procesamiento completado con éxito\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la ejecución principal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Asumiendo que tu DataFrame se llama 'df' y las columnas son 'codigo_industria' y 'nombre_industria'\n",
    "# Primero, elimina los duplicados para quedarte solo con las combinaciones únicas\n",
    "df_unico = data[['industry_id', 'industry_descr']].drop_duplicates()\n",
    "\n",
    "# Convierte las dos columnas a un diccionario\n",
    "diccionario_industrias = dict(zip(df_unico['industry_id'], df_unico['industry_descr']))\n",
    "\n",
    "industrias = pd.DataFrame(diccionario_industrias.items(), columns=['industry_id', 'industry_descr'])\n",
    "industrias.to_csv(r'C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\Dependencias consolidadas\\industrias_id_nombre.csv', sep=\";\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices de Comercio Internacional: Creación Optimizada con Aceleración GPU\n",
    "\n",
    "El siguiente código implementa una función optimizada para crear matrices de comercio bilateral para cada industria a partir de datos comerciales internacionales. Esta implementación utiliza técnicas avanzadas para mejorar significativamente el rendimiento:\n",
    "\n",
    "### Características principales:\n",
    "\n",
    "- **Aceleración por GPU**: Detecta automáticamente si hay una GPU disponible y la utiliza para acelerar el procesamiento cuando hay suficientes datos.\n",
    "- **Operaciones vectorizadas**: Minimiza las iteraciones fila por fila usando enfoques vectorizados más eficientes.\n",
    "- **Gestión de memoria optimizada**: Reutiliza estructuras para reducir la fragmentación y el consumo de memoria.\n",
    "- **Adaptabilidad**: Ajusta automáticamente la estrategia de procesamiento según el volumen de datos y el hardware disponible.\n",
    "\n",
    "El resultado es un diccionario donde cada clave representa una industria, y cada valor es una matriz completa de comercio bilateral entre todos los países del conjunto de datos, lo que facilita los análisis posteriores de dependencias comerciales y flujos económicos.\n",
    "\n",
    "Este enfoque es particularmente útil para el cálculo de índices de dependencia comercial, ya que proporciona una representación eficiente de los flujos comerciales entre países para cada sector industrial.\n",
    "\n",
    "### Limpieza de Matrices de Comercio: Eliminación de Flujos No Significativos\n",
    "\n",
    "El siguiente código implementa una función para eliminar del análisis aquellos países con flujos comerciales no significativos. Este paso es crucial para mejorar la precisión del análisis de dependencias comerciales, ya que permite enfocarse en relaciones económicas realmente relevantes.\n",
    "\n",
    "**Funcionamiento**:\n",
    "\n",
    "- **Umbral de significancia**: Establece un umbral mínimo (por defecto 0.05%) del comercio mundial total. Cualquier flujo comercial por debajo de este umbral se considera no significativo y se convierte a cero.\n",
    "- **Identificación de países no relevantes**: Detecta países que, después de aplicar el umbral, no tienen flujos comerciales significativos (tanto como exportadores como importadores).\n",
    "- **Limpieza de la matriz**: Elimina estos países de la matriz, reduciendo su dimensionalidad y concentrando el análisis en actores relevantes del comercio internacional.\n",
    "\n",
    "Esta función de preprocesamiento es especialmente importante para estudios de dependencia económica, ya que:\n",
    "1. Reduce el ruido en los datos al eliminar relaciones comerciales marginales\n",
    "2. Mejora la eficiencia computacional al trabajar con matrices más pequeñas\n",
    "3. Permite concentrarse en dependencias significativas que podrían representar vulnerabilidades reales\n",
    "\n",
    "El resultado es una matriz \"limpia\" que contiene únicamente los países y flujos comerciales relevantes para el análisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU para procesamiento\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando matrices de comercio: 100%|██████████| 170/170 [00:03<00:00, 50.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV de totales guardado en: C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\Dependencias consolidadas\\totales_comercio_por_pais_industria.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def crear_matriz_comercio_optimizado(grouped_data, codigos_paises: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Crea matrices de comercio bilateral para cada industria a partir de datos agrupados,\n",
    "    aprovechando operaciones vectorizadas y aceleración por GPU cuando está disponible.\n",
    "\n",
    "    Args:\n",
    "        grouped_data: DataFrame agrupado por industria.\n",
    "        codigos_paises: Lista de códigos de países ISO3.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Diccionario de matrices de comercio.\n",
    "    \"\"\"\n",
    "    matrices = {}\n",
    "    required_columns = {'exporter_iso3', 'importer_iso3', 'trade'}\n",
    "    \n",
    "    # Para el subproducto: crear una lista para almacenar los datos de totales\n",
    "    totales_data = []\n",
    "    \n",
    "    # Verificar columnas requeridas\n",
    "    if not required_columns.issubset(grouped_data.obj.columns):\n",
    "        raise ValueError(f\"Los datos deben contener las columnas: {required_columns}\")\n",
    "    \n",
    "    # Crear un mapeo de códigos de países a índices para operaciones más rápidas\n",
    "    pais_a_indice = {pais: i for i, pais in enumerate(codigos_paises)}\n",
    "    \n",
    "    # Verificar si hay GPU disponible\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "    print(f\"Usando {'GPU' if use_gpu else 'CPU'} para procesamiento\")\n",
    "    \n",
    "    # Crear una matriz vacía template para reutilizar\n",
    "    matriz_template = pd.DataFrame(\n",
    "        0.0,\n",
    "        index=codigos_paises,\n",
    "        columns=codigos_paises\n",
    "    )\n",
    "    \n",
    "    # Preprocesar datos para evitar iteraciones lentas\n",
    "    for industry, group in tqdm(grouped_data, desc=\"Creando matrices de comercio\"):\n",
    "        # Filtrar solo las filas con países válidos\n",
    "        valid_trades = group[\n",
    "            group['exporter_iso3'].isin(codigos_paises) & \n",
    "            group['importer_iso3'].isin(codigos_paises)\n",
    "        ]\n",
    "        \n",
    "        if valid_trades.empty:\n",
    "            # Reutilizar la matriz template (hacer una copia para evitar modificar el original)\n",
    "            matrices[industry] = matriz_template.copy()\n",
    "            \n",
    "            # Añadir entradas con valores cero para este caso\n",
    "            for pais in codigos_paises:\n",
    "                totales_data.append({\n",
    "                    'pais': pais,\n",
    "                    'industria': industry,\n",
    "                    'valor_importado': 0.0\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # Enfoque vectorizado para construir la matriz\n",
    "        if use_gpu and len(valid_trades) > 1000:  # Solo usar GPU si hay suficientes datos\n",
    "            # Convertir a índices numéricos para operaciones más rápidas\n",
    "            exporters = valid_trades['exporter_iso3'].map(pais_a_indice).values\n",
    "            importers = valid_trades['importer_iso3'].map(pais_a_indice).values\n",
    "            values = valid_trades['trade'].values\n",
    "            \n",
    "            # Crear tensor en GPU\n",
    "            matrix = torch.zeros((len(codigos_paises), len(codigos_paises)), device=device)\n",
    "            \n",
    "            # Usar scatter_add_ para construir la matriz eficientemente\n",
    "            indices = torch.tensor(np.vstack([exporters, importers]), device=device, dtype=torch.long)\n",
    "            matrix.index_put_(\n",
    "                indices=(torch.tensor(exporters, device=device, dtype=torch.long),\n",
    "                         torch.tensor(importers, device=device, dtype=torch.long)),\n",
    "                values=torch.tensor(values, device=device, dtype=torch.float),\n",
    "                accumulate=True\n",
    "            )\n",
    "            \n",
    "            # Convertir de vuelta a DataFrame\n",
    "            matrix_df = pd.DataFrame(\n",
    "                matrix.cpu().numpy(),\n",
    "                index=codigos_paises,\n",
    "                columns=codigos_paises\n",
    "            )\n",
    "        else:\n",
    "            # Crear una matriz completa de ceros y luego llenarla de una sola vez\n",
    "            # Empezamos con una copia de la matriz template\n",
    "            matrix_df = matriz_template.copy()\n",
    "            \n",
    "            # Preparar datos para actualización rápida\n",
    "            for _, row in valid_trades.iterrows():\n",
    "                exp = row['exporter_iso3']\n",
    "                imp = row['importer_iso3']\n",
    "                matrix_df.at[exp, imp] = row['trade']\n",
    "                \n",
    "        matrices[industry] = matrix_df\n",
    "        \n",
    "        # Calcular y guardar los totales importados por país para esta industria\n",
    "        for pais in codigos_paises:\n",
    "            # Sumar todos los valores de la columna (importaciones totales del país)\n",
    "            valor_total = matrix_df[pais].sum()\n",
    "            \n",
    "            # Añadir a nuestros datos de totales\n",
    "            totales_data.append({\n",
    "                'pais': pais,\n",
    "                'industria': industry,\n",
    "                'valor_importado': valor_total\n",
    "            })\n",
    "        \n",
    "    # Crear DataFrame con los totales\n",
    "    df_totales = pd.DataFrame(totales_data)\n",
    "    \n",
    "    # Guardar en CSV\n",
    "    ruta_csv = r\"C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\Dependencias consolidadas\\totales_comercio_por_pais_industria.csv\"\n",
    "    df_totales.to_csv(ruta_csv, index=False, sep=\";\")\n",
    "    print(f\"Archivo CSV de totales guardado en: {ruta_csv}\")\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "# Define la lista de códigos de países\n",
    "codigos_paises = sorted(data['importer_iso3'].unique().tolist())\n",
    "\n",
    "# Llama a la función optimizada\n",
    "matrices_comercio = crear_matriz_comercio_optimizado(data.groupby('industry_descr'), codigos_paises)\n",
    "\n",
    "def eliminar_filas_columnas_cero(df, threshold_pct=0.005):\n",
    "    \"\"\"Filtra una matriz de comercio aplicando un umbral relativo a cada país importador.\"\"\"\n",
    "    # Crear una copia para no modificar el original\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Para cada país importador (columna), aplicar su propio umbral\n",
    "    for country in df.columns:\n",
    "        # Calcular el total de importaciones del país\n",
    "        country_total_imports = df[country].sum()\n",
    "        \n",
    "        # Calcular el umbral específico para este país\n",
    "        country_threshold = country_total_imports * (threshold_pct)\n",
    "        \n",
    "        # Convertir a cero los valores por debajo del umbral solo para este país\n",
    "        df_filtered.loc[:, country] = df[country].where(df[country] >= country_threshold, 0)\n",
    "    \n",
    "    # Ahora identificar países con todos ceros\n",
    "    zero_rows = df_filtered.index[df_filtered.sum(axis=1) == 0]\n",
    "    zero_cols = df_filtered.columns[df_filtered.sum(axis=0) == 0]\n",
    "    countries_to_drop = list(set(zero_rows) & set(zero_cols))\n",
    "    \n",
    "    # Eliminar los países identificados\n",
    "    mat_clean = df_filtered.drop(countries_to_drop, axis=0).drop(countries_to_drop, axis=1)\n",
    "    \n",
    "    return mat_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `calculate_path_dependency` está diseñada para calcular la dependencia a lo largo de una cadena específica de nodos (países o entidades) en un análisis de flujos económicos.\n",
    "\n",
    "Paso a paso:\n",
    "\n",
    "1. **Entrada de la función**:\n",
    "   - `X_clean`: Una matriz que representa los flujos entre nodos (países). Cada valor `X_clean[i, j]` representa el flujo desde el nodo `i` al nodo `j`.\n",
    "   - `path`: Una lista que define un camino específico a través de varios nodos (países).\n",
    "   - `denominators`: Un array que contiene los valores de normalización para cada nodo.\n",
    "\n",
    "2. **Proceso**:\n",
    "   - Inicia calculando el flujo directo entre los dos primeros nodos del camino: `flujo_actual = X_clean[path[0], path[1]]`.\n",
    "   \n",
    "   - Luego itera a través de cada nodo intermedio en el camino para calcular cómo el flujo se propaga:\n",
    "     ```python\n",
    "     for i in range(1, len(path)-1):\n",
    "         nodo_actual = path[i]\n",
    "     ```\n",
    "\n",
    "   - Para cada nodo, verifica si puede haber flujo saliente (si el denominador es 0, no puede haber flujo):\n",
    "     ```python\n",
    "     if denominators[nodo_actual] == 0:\n",
    "         return 0\n",
    "     ```\n",
    "\n",
    "   - Calcula qué proporción del flujo que llega al nodo actual continúa hacia el siguiente nodo:\n",
    "     ```python\n",
    "     proporcion = X_clean[path[i], path[i+1]] / denominators[nodo_actual]\n",
    "     ```\n",
    "     Esta proporción representa la fracción del flujo total saliente del nodo actual que va específicamente al siguiente nodo.\n",
    "\n",
    "   - Multiplica el flujo actual por esta proporción para actualizar el valor:\n",
    "     ```python\n",
    "     flujo_actual = flujo_actual * proporcion\n",
    "     ```\n",
    "     Este paso acumula el efecto de cada transición a lo largo del camino.\n",
    "\n",
    "3. **Resultado**:\n",
    "   - Finalmente, normaliza el flujo resultante por el denominador del nodo final:\n",
    "     ```python\n",
    "     return flujo_actual / denominators[path[-1]] if denominators[path[-1]] > 0 else 0\n",
    "     ```\n",
    "     Esto garantiza que la dependencia se exprese como una proporción relativa al flujo total del nodo destino.\n",
    "\n",
    "En resumen, esta función calcula cuánto de un flujo inicial sobrevive a través de toda una cadena de nodos, considerando en cada paso qué proporción del flujo continúa al siguiente nodo. Es útil para analizar dependencias en cadenas de suministro, flujos comerciales o relaciones económicas donde los recursos o bienes fluyen a través de múltiples intermediarios.\n",
    "\n",
    "La función `calculate_path_dependency` puede expresarse formalmente usando la siguiente formulación en LaTeX:\n",
    "\n",
    "$$\\text{Dependencia}(p) = \\frac{f_{p_1, p_2}}{d_{p_n}} \\prod_{i=2}^{n-1} \\frac{f_{p_i, p_{i+1}}}{d_{p_i}}$$\n",
    "\n",
    "Donde:\n",
    "- $p = [p_1, p_2, \\ldots, p_n]$ es el camino (path) que consiste en $n$ nodos\n",
    "- $f_{i,j}$ representa el flujo desde el nodo $i$ al nodo $j$ (corresponde a `X_clean[i, j]`)\n",
    "- $d_i$ es el denominador para el nodo $i$ (el valor de normalización, típicamente la suma de todos los flujos salientes)\n",
    "\n",
    "Esta fórmula se puede interpretar de la siguiente manera:\n",
    "1. Comenzamos con el flujo inicial $f_{p_1, p_2}$ del primer nodo al segundo nodo\n",
    "2. Para cada nodo intermedio $p_i$ (donde $2 \\leq i \\leq n-1$), calculamos la proporción del flujo que continúa al siguiente nodo $p_{i+1}$, expresado como $\\frac{f_{p_i, p_{i+1}}}{d_{p_i}}$\n",
    "3. Multiplicamos todas estas proporciones junto con el flujo inicial\n",
    "4. Finalmente, normalizamos por el denominador del nodo final $d_{p_n}$\n",
    "\n",
    "Si cualquiera de los denominadores $d_{p_i}$ es cero, entonces la dependencia completa se define como cero, ya que no puede haber flujo a través de ese nodo.\n",
    "\n",
    "Formalmente, la condición completa se puede expresar como:\n",
    "\n",
    "$$\\text{Dependencia}(p) = \n",
    "\\begin{cases} \n",
    "0 & \\text{si } \\exists i \\in \\{2, 3, \\ldots, n\\} : d_{p_i} = 0 \\\\\n",
    "\\frac{f_{p_1, p_2}}{d_{p_n}} \\prod_{i=2}^{n-1} \\frac{f_{p_i, p_{i+1}}}{d_{p_i}} & \\text{en otro caso}\n",
    "\\end{cases}$$\n",
    "\n",
    "Esta formulación captura la naturaleza secuencial de la propagación del flujo a lo largo de todo el camino, considerando las proporciones de transición en cada etapa.\n",
    "\n",
    "## Ejemplo sencillo de dependencia de camino\n",
    "\n",
    "Imaginemos un sistema con 4 países (A, B, C, D) con los siguientes flujos comerciales entre ellos:\n",
    "\n",
    "- De A a B: 100 unidades\n",
    "- De B a C: 50 unidades\n",
    "- De C a D: 30 unidades\n",
    "- De B a otros destinos: 150 unidades (total saliente de B = 200)\n",
    "- De C a otros destinos: 70 unidades (total saliente de C = 100)\n",
    "- Flujo total que recibe D: 80 unidades\n",
    "\n",
    "Queremos calcular la dependencia a lo largo del camino p = [A, B, C, D].\n",
    "\n",
    "### Paso 1: Definimos nuestros datos\n",
    "- Matriz de flujos `X_clean`:\n",
    "  ```\n",
    "  X_clean = [\n",
    "    [0, 100, 0, 0],  # Flujos desde A\n",
    "    [0, 0, 50, 0],   # Flujos desde B\n",
    "    [0, 0, 0, 30],   # Flujos desde C\n",
    "    [0, 0, 0, 0]     # Flujos desde D\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "- Denominadores (totales de flujos salientes/entrantes según corresponda):\n",
    "  ```\n",
    "  denominators = [100, 200, 100, 80]\n",
    "  ```\n",
    "\n",
    "- Camino a analizar:\n",
    "  ```\n",
    "  path = [0, 1, 2, 3]  # Corresponde a A→B→C→D\n",
    "  ```\n",
    "\n",
    "### Paso 2: Calculamos la dependencia\n",
    "\n",
    "1. Flujo inicial: flujo_actual = X_clean[0, 1] = 100\n",
    "\n",
    "2. Para el nodo B (índice 1):\n",
    "   - Proporción que va a C = X_clean[1, 2] / denominators[1] = 50/200 = 0.25\n",
    "   - flujo_actual = 100 * 0.25 = 25\n",
    "\n",
    "3. Para el nodo C (índice 2):\n",
    "   - Proporción que va a D = X_clean[2, 3] / denominators[2] = 30/100 = 0.3\n",
    "   - flujo_actual = 25 * 0.3 = 7.5\n",
    "\n",
    "4. Normalización final:\n",
    "   - Dependencia = flujo_actual / denominators[3] = 7.5/80 = 0.09375\n",
    "\n",
    "### Interpretación\n",
    "\n",
    "La dependencia de 0.09375 (o 9.375%) indica que este camino específico A→B→C→D representa aproximadamente el 9.4% del flujo total que recibe D.\n",
    "\n",
    "En términos prácticos, esto podría significar que:\n",
    "- El 9.4% de los bienes que llegan a D provienen originalmente de A, pasando por B y C.\n",
    "- Si hubiera una interrupción en cualquier punto de esta cadena, aproximadamente el 9.4% de las importaciones de D estarían en riesgo.\n",
    "\n",
    "Este cálculo captura la \"dilución\" del flujo original a medida que pasa por múltiples intermediarios, donde en cada paso solo una fracción continúa por el camino específico que estamos analizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_path_dependency(X_clean, path, denominators):\n",
    "    \"\"\"Calcula la dependencia de un camino específico.\"\"\"\n",
    "    fuerza_camino = 1.0\n",
    "    for k in range(len(path) - 1):\n",
    "        if denominators[path[k+1]] > 0:\n",
    "            fuerza_camino *= X_clean[path[k], path[k+1]] / denominators[path[k+1]]\n",
    "        else:\n",
    "            fuerza_camino = 0  # Si el denominador es cero, la fuerza del camino es cero\n",
    "    return fuerza_camino\n",
    "\n",
    "def calculate_intermediary_centrality(intermediary_frequency, intermediary_strength, country_names):\n",
    "    \"\"\"\n",
    "    Calcula métricas de centralidad para intermediarios.\n",
    "    \n",
    "    Esta función es la misma que la original.\n",
    "    \"\"\"\n",
    "    # Implementación existente\n",
    "    centrality = []\n",
    "    \n",
    "    # Normalizar\n",
    "    max_freq = max(intermediary_frequency.values()) if intermediary_frequency.values() else 1\n",
    "    max_strength = max(intermediary_strength.values()) if intermediary_strength.values() else 1\n",
    "    \n",
    "    for country in country_names:\n",
    "        norm_freq = intermediary_frequency[country] / max_freq if max_freq > 0 else 0\n",
    "        norm_strength = intermediary_strength[country] / max_strength if max_strength > 0 else 0\n",
    "        \n",
    "        combined_score = 0.4 * norm_freq + 0.6 * norm_strength\n",
    "        \n",
    "        centrality.append((country, intermediary_frequency[country], \n",
    "                          intermediary_strength[country], combined_score))\n",
    "    \n",
    "    centrality.sort(key=lambda x: x[3], reverse=True)\n",
    "    return centrality\n",
    "\n",
    "def process_country_pair(i, j, X_clean, denominators, country_names, max_possible_length, \n",
    "                        convergence_threshold, path_strength_threshold):\n",
    "    \"\"\"\n",
    "    Procesa un par de países para calcular sus dependencias.\n",
    "    Esta función será ejecutada en paralelo.\n",
    "    \"\"\"\n",
    "    # Inicialización\n",
    "    current_total = 0\n",
    "    indirect_total = 0\n",
    "    dependencies_by_length = {}\n",
    "    significant_paths = []\n",
    "    length = 1\n",
    "    \n",
    "    # Dependencia directa\n",
    "    trade_value = X_clean[j, i]\n",
    "    direct_dependency = X_clean[j, i] / denominators[i] if denominators[i] > 0 else 0\n",
    "    dependencies_by_length[1] = direct_dependency\n",
    "    current_total = direct_dependency\n",
    "    \n",
    "    # Clave para identificar el par de países\n",
    "    pair_key = f\"{country_names[j]}->{country_names[i]}\"\n",
    "    \n",
    "    # Dependencias indirectas\n",
    "    for length in range(2, max_possible_length + 1):\n",
    "        DI_ij_l = 0\n",
    "        middle_countries = [k for k in range(X_clean.shape[0]) if k != i and k != j]\n",
    "\n",
    "        # Examinar cada combinación posible de intermediarios\n",
    "        for intermediaries in combinations(middle_countries, length - 1):\n",
    "            path = (j,) + intermediaries + (i,)\n",
    "            fuerza_camino = calculate_path_dependency(X_clean, path, denominators)\n",
    "            DI_ij_l += fuerza_camino\n",
    "            \n",
    "            # Registrar caminos significativos\n",
    "            if fuerza_camino > path_strength_threshold:\n",
    "                # Convertir índices a nombres de países\n",
    "                path_countries = [country_names[idx] for idx in path]\n",
    "                \n",
    "                # Registrar este camino\n",
    "                path_info = {\n",
    "                    'exportador': path_countries[0],\n",
    "                    'importador': path_countries[-1],\n",
    "                    'intermediarios': path_countries[1:-1],\n",
    "                    'fuerza': fuerza_camino,\n",
    "                    'longitud': length\n",
    "                }\n",
    "                \n",
    "                significant_paths.append(path_info)\n",
    "                \n",
    "                # No podemos actualizar directamente las estructuras globales en paralelo\n",
    "                # Devolveremos estos valores para actualizar después\n",
    "                intermediary_updates = []\n",
    "                for interm_idx, interm_country in enumerate(path_countries[1:-1]):\n",
    "                    weight_factor = 1.0 / (interm_idx + 1)\n",
    "                    intermediary_updates.append((interm_country, weight_factor * fuerza_camino))\n",
    "                \n",
    "        dependencies_by_length[length] = DI_ij_l\n",
    "        indirect_total += DI_ij_l\n",
    "\n",
    "        prev_total = current_total\n",
    "        current_total = direct_dependency + indirect_total\n",
    "\n",
    "        # Criterio de convergencia\n",
    "        if length > 1 and abs(current_total - prev_total) < convergence_threshold:\n",
    "            break\n",
    "\n",
    "    # La dependencia total es la suma de la directa y la indirecta\n",
    "    total_dependency = direct_dependency + indirect_total\n",
    "\n",
    "    result = {\n",
    "        'importador': country_names[i],\n",
    "        'exportador': country_names[j],\n",
    "        'trade_value': trade_value,\n",
    "        'dependencia_directa': direct_dependency,\n",
    "        'dependencia_indirecta': indirect_total,\n",
    "        'dependencia_total': total_dependency,\n",
    "        'longitud_optima': length,\n",
    "        'dependencias_por_longitud': dependencies_by_length\n",
    "    }\n",
    "    \n",
    "    # Ordenar caminos significativos\n",
    "    significant_paths.sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    \n",
    "    # Resultados a devolver para este par de países\n",
    "    return {\n",
    "        'pair_key': pair_key,\n",
    "        'result': result,\n",
    "        'top_dependency': (country_names[i], country_names[j], direct_dependency,\n",
    "                         indirect_total, total_dependency, length),\n",
    "        'significant_paths': significant_paths,\n",
    "        'length_converged': length if length > 1 else 0\n",
    "    }\n",
    "\n",
    "def calculate_all_dependencies_parallel(X, country_names=None, convergence_threshold=0.01, \n",
    "                                       max_possible_length=3, \n",
    "                                       path_strength_threshold=0.001, n_jobs=None, use_gpu=True, \n",
    "                                       debug_mode=False):\n",
    "    \"\"\"\n",
    "    Versión paralelizada del cálculo de dependencias que mantiene EXACTAMENTE\n",
    "    la misma salida que la versión original.\n",
    "    \n",
    "    El parámetro debug_mode permite verificar que el número de dependencias\n",
    "    coincida con la versión original.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Versión paralelizada del cálculo de dependencias.\n",
    "    \n",
    "    Parameters adicionales:\n",
    "    -----------------------\n",
    "    n_jobs : int, opcional\n",
    "        Número de trabajos paralelos. Si es None, usa todos los núcleos disponibles.\n",
    "    use_gpu : bool, default=True\n",
    "        Si se debe intentar usar GPU para acelerar algunos cálculos.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "\n",
    "    if country_names is None:\n",
    "        country_names = [f\"País {i}\" for i in range(n)]\n",
    "\n",
    "    if len(country_names) != n:\n",
    "        raise ValueError(f\"La longitud de country_names ({len(country_names)}) no coincide con la dimensión de X ({n})\")\n",
    "\n",
    "    # Configurar paralelización\n",
    "    if n_jobs is None:\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "    \n",
    "    # Verificar disponibilidad de GPU\n",
    "    gpu_available = torch.cuda.is_available() and use_gpu\n",
    "    X_clean = X\n",
    "\n",
    "    denominators = np.sum(X, axis=0)\n",
    "\n",
    "    # Acelerar cálculos directos con GPU si está disponible\n",
    "    if gpu_available:\n",
    "        # Transferir datos a GPU\n",
    "        X_gpu = torch.tensor(X_clean, device='cuda', dtype=torch.float32)\n",
    "        denom_gpu = torch.tensor(denominators, device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        # Calcular dependencias directas en forma vectorizada\n",
    "        direct_deps = torch.zeros_like(X_gpu)\n",
    "        for i in range(n):\n",
    "            # Evitar división por cero\n",
    "            if denom_gpu[i] > 0:\n",
    "                direct_deps[:, i] = X_gpu[:, i] / denom_gpu[i]\n",
    "        \n",
    "        # Transferir resultados de vuelta a CPU\n",
    "        direct_dependencies = direct_deps.cpu().numpy()\n",
    "        \n",
    "        # Usar estas dependencias directas precalculadas en el procesamiento posterior\n",
    "        # (Aunque en esta implementación seguimos calculándolas en process_country_pair para\n",
    "        # mantener cambios mínimos en el código)\n",
    "\n",
    "    # Estructura de resultados extendida\n",
    "    results = {\n",
    "        'dependencies': [],\n",
    "        'top_dependencies': [],\n",
    "        'avg_dependencies': {},\n",
    "        'length_distribution': np.zeros(max_possible_length),\n",
    "        'critical_intermediaries': {},     # Intermediarios críticos por relación\n",
    "        'intermediary_frequency': {},      # Frecuencia de países como intermediarios\n",
    "        'critical_paths': [],              # Rutas críticas completas\n",
    "        'intermediary_strength': {}        # Fuerza de cada país como intermediario\n",
    "    }\n",
    "    \n",
    "    # Inicializar contadores para intermediarios\n",
    "    for country in country_names:\n",
    "        results['intermediary_frequency'][country] = 0\n",
    "        results['intermediary_strength'][country] = 0.0\n",
    "\n",
    "    # Preparar pares de países para procesamiento paralelo \n",
    "    # Mantenemos la misma estructura de iteración del código original\n",
    "    # Primero por importador (i) y luego por exportador (j)\n",
    "    country_pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                country_pairs.append((i, j))\n",
    "    \n",
    "    # Procesar pares de países en paralelo\n",
    "    with Parallel(n_jobs=n_jobs) as parallel:\n",
    "        pair_results = parallel(\n",
    "            delayed(process_country_pair)(\n",
    "                i, j, X_clean, denominators, country_names, \n",
    "                max_possible_length, convergence_threshold, path_strength_threshold\n",
    "            ) \n",
    "            for i, j in tqdm(country_pairs, desc=\"Calculando dependencias\")\n",
    "        )\n",
    "        \n",
    "    # Agrupar resultados por país importador\n",
    "    results_by_importer = {}\n",
    "    for res in pair_results:\n",
    "        importer = res['result']['importador']\n",
    "        if importer not in results_by_importer:\n",
    "            results_by_importer[importer] = []\n",
    "        results_by_importer[importer].append(res)\n",
    "    \n",
    "    # Recolectar critical paths de todos los pares para ordenarlos después\n",
    "    all_critical_paths = []\n",
    "    \n",
    "    # Procesar los resultados manteniendo el mismo orden que el código original\n",
    "    for i in range(n):\n",
    "        importer = country_names[i]\n",
    "        total_dep = 0.0\n",
    "        num_deps = 0\n",
    "        \n",
    "        if importer in results_by_importer:\n",
    "            for res in results_by_importer[importer]:\n",
    "                # Agregar a dependencies\n",
    "                results['dependencies'].append(res['result'])\n",
    "                \n",
    "                # Agregar a top_dependencies\n",
    "                results['top_dependencies'].append(res['top_dependency'])\n",
    "                \n",
    "                # Actualizar critical_intermediaries\n",
    "                results['critical_intermediaries'][res['pair_key']] = res['significant_paths']\n",
    "                \n",
    "                # Recolectar critical paths\n",
    "                all_critical_paths.extend(res['significant_paths'])\n",
    "                \n",
    "                # Actualizar length_distribution si convergió\n",
    "                if res['length_converged'] > 1:\n",
    "                    results['length_distribution'][res['length_converged'] - 1] += 1\n",
    "                \n",
    "                # Actualizar dependencia promedio\n",
    "                total_dep += res['result']['dependencia_total']\n",
    "                num_deps += 1\n",
    "                \n",
    "                # Actualizar estadísticas de intermediarios\n",
    "                for path in res['significant_paths']:\n",
    "                    for idx, interm in enumerate(path['intermediarios']):\n",
    "                        # Incrementar frecuencia\n",
    "                        results['intermediary_frequency'][interm] += 1\n",
    "                        \n",
    "                        # Incrementar fuerza ponderada\n",
    "                        weight_factor = 1.0 / (idx + 1)\n",
    "                        results['intermediary_strength'][interm] += path['fuerza'] * weight_factor\n",
    "        \n",
    "        # Guardar dependencia promedio para este importador\n",
    "        results['avg_dependencies'][importer] = total_dep / num_deps if num_deps > 0 else 0\n",
    "    \n",
    "    # Añadir y ordenar los critical paths (igual que el original)\n",
    "    results['critical_paths'] = all_critical_paths\n",
    "    \n",
    "    # Ordenar top dependencies\n",
    "    results['top_dependencies'].sort(key=lambda x: x[4], reverse=True)\n",
    "    results['top_dependencies'] = results['top_dependencies'][:90]\n",
    "    \n",
    "    # Ordenar critical_paths por fuerza\n",
    "    results['critical_paths'].sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    \n",
    "    # Calcular métricas de centralidad para intermediarios\n",
    "    results['intermediary_centrality'] = calculate_intermediary_centrality(\n",
    "        results['intermediary_frequency'], \n",
    "        results['intermediary_strength'],\n",
    "        country_names\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Para mantener compatibilidad, redefinimos la función original\n",
    "# para que utilice la versión paralelizada\n",
    "def calculate_all_dependencies(X, country_names=None, convergence_threshold=0.01, \n",
    "                              max_possible_length=3, threshold_pct=0.01, \n",
    "                              path_strength_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Calcula todas las dependencias entre países con análisis de intermediarios críticos.\n",
    "    \n",
    "    Esta función mantiene EXACTAMENTE la misma firma y resultados que la original,\n",
    "    pero utiliza internamente paralelización y GPU para acelerar los cálculos.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Matriz de comercio\n",
    "    country_names : list, opcional\n",
    "        Nombres de los países\n",
    "    convergence_threshold : float, default=0.01\n",
    "        Umbral para determinar la convergencia\n",
    "    max_possible_length : int, default=5\n",
    "        Longitud máxima de caminos a considerar\n",
    "    threshold_pct : float, default=0.01\n",
    "        Umbral para filtrar valores de comercio insignificantes (porcentaje)\n",
    "    path_strength_threshold : float, default=0.001\n",
    "        Umbral mínimo para considerar una ruta como significativa\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Diccionario con todos los resultados del análisis\n",
    "    \"\"\"\n",
    "    # Determinar si usar paralelización basado en el tamaño del problema\n",
    "    use_parallel = X.shape[0] > 5  # Para matrices muy pequeñas no vale la pena paralelizar\n",
    "    \n",
    "    # Verificar disponibilidad de GPU\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    # Aquí agregas el segundo log\n",
    "    print(f\"Usando paralelización: {use_parallel}, GPU disponible: {use_gpu}\")\n",
    "    if use_gpu:\n",
    "        print(f\"GPU en uso: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Configurar número de trabajos para paralelización\n",
    "    n_countries = X.shape[0]\n",
    "    n_jobs = min(multiprocessing.cpu_count(), n_countries)  # Limitar al número de países\n",
    "    \n",
    "    \n",
    "    if use_parallel:\n",
    "        # Usar la versión paralelizada\n",
    "        return calculate_all_dependencies_parallel(\n",
    "            X, country_names, convergence_threshold, \n",
    "            max_possible_length, path_strength_threshold,\n",
    "            n_jobs=n_jobs, use_gpu=use_gpu, debug_mode=False\n",
    "        )\n",
    "    else:\n",
    "        # Para matrices muy pequeñas, usar un solo proceso\n",
    "        return calculate_all_dependencies_parallel(\n",
    "            X, country_names, convergence_threshold, \n",
    "            max_possible_length, path_strength_threshold,\n",
    "            n_jobs=1, use_gpu=use_gpu, debug_mode=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar y guardar resultados de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Diccionario para guardar todos los resultados\n",
    "all_results = {}\n",
    "\n",
    "# Procesar cada industria\n",
    "for industry, mat in matrices_comercio.items():\n",
    "    print(f\"\\nProcesando industria: {industry}\")\n",
    "    \n",
    "    # Limpiar matriz\n",
    "    mat_clean = eliminar_filas_columnas_cero(mat)\n",
    "    \n",
    "    # Si la matriz limpia está vacía o es muy pequeña, continuamos con la siguiente\n",
    "    if mat_clean.shape[0] < 2:\n",
    "        print(f\"Matriz demasiado pequeña para industria {industry}\")\n",
    "        continue\n",
    "        \n",
    "    # Convertir a numpy array y obtener nombres de países\n",
    "    X = mat_clean.values\n",
    "    country_names = mat_clean.columns.tolist()\n",
    "\n",
    "    # Aquí agregas solo el log del tamaño de la matriz\n",
    "    print(f\"Matriz para {industry}: {mat_clean.shape}\")\n",
    "    \n",
    "    # Verificar disponibilidad de GPU (solo para información)\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"GPU disponible: {gpu_available}\")\n",
    "    if gpu_available:\n",
    "        print(f\"GPU en uso: {torch.cuda.get_device_name(0)}\")\n",
    "     \n",
    "    # Calcular dependencias\n",
    "    results = calculate_all_dependencies(X, country_names)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    all_results[industry] = {\n",
    "        'results': results,\n",
    "        'country_names': country_names,\n",
    "        'matrix_shape': mat_clean.shape\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar y guadar datos de intermediarios Críticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_intermediaries(results, top_n=10):\n",
    "    \"\"\"\n",
    "    Obtiene los países más importantes como intermediarios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del análisis con intermediarios\n",
    "    top_n : int\n",
    "        Número de países a mostrar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Lista de tuplas (país, score, frecuencia, fuerza) ordenadas por importancia\n",
    "    \"\"\"\n",
    "    # La estructura de intermediary_centrality ha cambiado en la nueva implementación\n",
    "    # Ahora es una lista de tuplas (país, frecuencia, fuerza, score_combinado)\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementación: ya está ordenada por score combinado\n",
    "        return results['intermediary_centrality'][:top_n]\n",
    "    else:\n",
    "        # Implementación anterior (por compatibilidad)\n",
    "        centrality_scores = [(country, metrics) \n",
    "                            for country, metrics in results['intermediary_centrality'].items()]\n",
    "        centrality_scores.sort(key=lambda x: x[1]['centrality_score'], reverse=True)\n",
    "        return centrality_scores[:top_n]\n",
    "\n",
    "def analyze_country_intermediary_role(results, country):\n",
    "    \"\"\"\n",
    "    Analiza el papel de un país específico como intermediario.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del análisis con intermediarios\n",
    "    country : str\n",
    "        País a analizar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Información detallada sobre el rol del país como intermediario\n",
    "    \"\"\"\n",
    "    # Verificar si el país existe en los resultados\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementación - lista de tuplas\n",
    "        country_entry = next((entry for entry in results['intermediary_centrality'] \n",
    "                             if entry[0] == country), None)\n",
    "        \n",
    "        if country_entry is None:\n",
    "            return {\"error\": f\"El país {country} no está en los resultados\"}\n",
    "            \n",
    "        # Extraer métricas\n",
    "        centrality = {\n",
    "            \"frequency\": country_entry[1],\n",
    "            \"strength\": country_entry[2],\n",
    "            \"centrality_score\": country_entry[3]\n",
    "        }\n",
    "    else:\n",
    "        # Implementación anterior\n",
    "        if country not in results['intermediary_centrality']:\n",
    "            return {\"error\": f\"El país {country} no está en los resultados\"}\n",
    "        centrality = results['intermediary_centrality'][country]\n",
    "    \n",
    "    # Encontrar las rutas más importantes donde este país actúa como intermediario\n",
    "    top_paths = []\n",
    "    for path in results['critical_paths']:\n",
    "        if country in path['intermediarios']:\n",
    "            top_paths.append(path)\n",
    "    \n",
    "    # Ordenar por fuerza del camino\n",
    "    top_paths.sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    \n",
    "    # Filtrar los 10 caminos más importantes\n",
    "    top_paths = top_paths[:10]\n",
    "    \n",
    "    return {\n",
    "        \"metrics\": centrality,\n",
    "        \"top_paths\": top_paths,\n",
    "        \"role_summary\": {\n",
    "            \"total_paths\": len(top_paths),\n",
    "            \"average_path_strength\": sum(p['fuerza'] for p in top_paths) / len(top_paths) if top_paths else 0,\n",
    "            \"max_path_strength\": max(p['fuerza'] for p in top_paths) if top_paths else 0,\n",
    "            \"unique_exporters\": len(set(p['exportador'] for p in top_paths)),\n",
    "            \"unique_importers\": len(set(p['importador'] for p in top_paths))\n",
    "        }\n",
    "    }\n",
    "\n",
    "def summarize_country_dependencies(results, country, top_n=5):\n",
    "    \"\"\"\n",
    "    Genera un resumen de las dependencias comerciales de un país específico.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del análisis de dependencias\n",
    "    country : str\n",
    "        País a analizar\n",
    "    top_n : int\n",
    "        Número de dependencias principales a mostrar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Resumen de dependencias del país\n",
    "    \"\"\"\n",
    "    # Dependencias como importador (otros países exportan a este país)\n",
    "    import_dependencies = []\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['importador'] == country:\n",
    "            import_dependencies.append(dep)\n",
    "    \n",
    "    # Ordenar por dependencia total\n",
    "    import_dependencies.sort(key=lambda x: x['dependencia_total'], reverse=True)\n",
    "    \n",
    "    # Dependencias como exportador (este país exporta a otros)\n",
    "    export_dependencies = []\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['exportador'] == country:\n",
    "            export_dependencies.append(dep)\n",
    "    \n",
    "    # Ordenar por dependencia total\n",
    "    export_dependencies.sort(key=lambda x: x['dependencia_total'], reverse=True)\n",
    "    \n",
    "    # Calcular dependencia promedio\n",
    "    avg_dependency = results['avg_dependencies'].get(country, 0)\n",
    "    \n",
    "    # Analizar el papel como intermediario\n",
    "    intermediary_role = None\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementación\n",
    "        intermediary_info = next((x for x in results['intermediary_centrality'] if x[0] == country), None)\n",
    "        if intermediary_info:\n",
    "            intermediary_role = {\n",
    "                \"frequency\": intermediary_info[1],\n",
    "                \"strength\": intermediary_info[2],\n",
    "                \"centrality_score\": intermediary_info[3],\n",
    "                \"rank\": next((i+1 for i, x in enumerate(results['intermediary_centrality']) \n",
    "                             if x[0] == country), None)\n",
    "            }\n",
    "    else:\n",
    "        # Implementación anterior\n",
    "        if country in results['intermediary_centrality']:\n",
    "            intermediary_role = results['intermediary_centrality'][country]\n",
    "            # Calcular rango\n",
    "            countries_sorted = sorted(results['intermediary_centrality'].keys(), \n",
    "                                     key=lambda x: results['intermediary_centrality'][x]['centrality_score'],\n",
    "                                     reverse=True)\n",
    "            intermediary_role[\"rank\"] = countries_sorted.index(country) + 1\n",
    "    \n",
    "    return {\n",
    "        \"country\": country,\n",
    "        \"avg_dependency\": avg_dependency,\n",
    "        \"top_import_dependencies\": import_dependencies[:top_n],\n",
    "        \"top_export_dependencies\": export_dependencies[:top_n],\n",
    "        \"total_import_dependencies\": len(import_dependencies),\n",
    "        \"total_export_dependencies\": len(export_dependencies),\n",
    "        \"intermediary_role\": intermediary_role\n",
    "    }\n",
    "\n",
    "def identify_critical_trade_relationships(results, threshold=0.7, min_paths=3):\n",
    "    \"\"\"\n",
    "    Identifica relaciones comerciales críticas con alta dependencia.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del análisis de dependencias\n",
    "    threshold : float\n",
    "        Umbral de dependencia para considerar una relación como crítica\n",
    "    min_paths : int\n",
    "        Número mínimo de caminos alternativos para evitar criticidad\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Lista de relaciones críticas\n",
    "    \"\"\"\n",
    "    critical_relationships = []\n",
    "    \n",
    "    # Analizar cada dependencia\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['dependencia_total'] >= threshold:\n",
    "            # Buscar caminos alternativos\n",
    "            pair_key = f\"{dep['exportador']}->{dep['importador']}\"\n",
    "            alternative_paths = []\n",
    "            \n",
    "            if pair_key in results['critical_intermediaries']:\n",
    "                alternative_paths = results['critical_intermediaries'][pair_key]\n",
    "            \n",
    "            # Si hay pocos caminos alternativos, es una relación crítica\n",
    "            if len(alternative_paths) < min_paths:\n",
    "                critical_relationships.append({\n",
    "                    \"exportador\": dep['exportador'],\n",
    "                    \"importador\": dep['importador'],\n",
    "                    \"dependencia_total\": dep['dependencia_total'],\n",
    "                    \"dependencia_directa\": dep['dependencia_directa'],\n",
    "                    \"caminos_alternativos\": len(alternative_paths),\n",
    "                    \"criticidad\": 1.0 - (len(alternative_paths) / min_paths) \n",
    "                                     if min_paths > 0 else 1.0\n",
    "                })\n",
    "    \n",
    "    # Ordenar por criticidad\n",
    "    critical_relationships.sort(key=lambda x: x['criticidad'], reverse=True)\n",
    "    \n",
    "    return critical_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Aircraft and spacecraft'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m interm \u001b[38;5;241m=\u001b[39m get_top_intermediaries(all_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAircraft and spacecraft\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m role \u001b[38;5;241m=\u001b[39m analyze_country_intermediary_role(results, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDEU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Aircraft and spacecraft'"
     ]
    }
   ],
   "source": [
    "interm = get_top_intermediaries(all_results['Aircraft and spacecraft']['results'])\n",
    "role = analyze_country_intermediary_role(results, 'DEU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo Dataframes para guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dependencies_dataframe(all_results):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con los resultados de dependencias para todas las industrias.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_results : dict\n",
    "        Diccionario con los resultados por industria\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame con las columnas: industria, importador, exportador, \n",
    "        dependencia_total, dependencia_directa, dependencia_indirecta, longitud_optima\n",
    "    \"\"\"\n",
    "    # Lista para almacenar los datos de todas las industrias\n",
    "    all_data = []\n",
    "    \n",
    "    # Procesar cada industria\n",
    "    for industry, data in all_results.items():\n",
    "        # Obtener los resultados de esta industria\n",
    "        industry_results = data['results']['dependencies']\n",
    "        \n",
    "        # Añadir cada fila de resultados\n",
    "        for result in industry_results:\n",
    "            row = {\n",
    "                'industria': industry,\n",
    "                'importador': result['importador'],\n",
    "                'exportador': result['exportador'],\n",
    "                'dependencia_total': result['dependencia_total'],\n",
    "                'dependencia_directa': result['dependencia_directa'],\n",
    "                'dependencia_indirecta': result['dependencia_indirecta'],\n",
    "                'trade_value': result['trade_value'],\n",
    "                'longitud_optima': result['longitud_optima']\n",
    "            }\n",
    "            all_data.append(row)\n",
    "    \n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Ordenar el DataFrame\n",
    "    df = df.sort_values(['industria', 'dependencia_total'], ascending=[True, False])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso:\n",
    "df = create_dependencies_dataframe(all_results)\n",
    "\n",
    "# Definir el nuevo mapeo de nombres de columnas\n",
    "nuevo_nombres = {\n",
    "    'industria': 'industry',\n",
    "    'importador': 'dependent_country',\n",
    "    'exportador': 'supplier_country',\n",
    "    'dependencia_total': 'dependency_value',\n",
    "    'dependencia_directa': 'direct_dependency',\n",
    "    'dependencia_indirecta': 'indirect_dependency',\n",
    "    'trade_value': 'trade_value',\n",
    "    'longitud_optima': 'longitud_optima'\n",
    "}\n",
    "\n",
    "# Renombrar las columnas\n",
    "df = df.rename(columns=nuevo_nombres)\n",
    "\n",
    "# Guardar como gzip\n",
    "with gzip.open(r\"C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\Dependencias consolidadas\\dependencias.csv.gz\", 'wt') as f:\n",
    "    df.to_csv(f, sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lo que propone CHATGPT para hacer la seccion 6 del paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_26692\\135950425.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  weighted_dependencies = df.groupby(['dependent_country', 'supplier_country']).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Todos los archivos CSV han sido generados en: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\ficheros_paper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = Path.cwd().parent.parent\n",
    "output_dir = current_dir / \"data\" / \"processed\" / \"ficheros_paper\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 1. Dependencias completas (industria-país-par)\n",
    "df = create_dependencies_dataframe(all_results)\n",
    "\n",
    "# Renombrado (ya implementado antes)\n",
    "df = df.rename(columns={\n",
    "    'industria': 'industry',\n",
    "    'importador': 'dependent_country',\n",
    "    'exportador': 'supplier_country',\n",
    "    'dependencia_total': 'dependency_value',\n",
    "    'dependencia_directa': 'direct_dependency',\n",
    "    'dependencia_indirecta': 'indirect_dependency',\n",
    "    'trade_value': 'trade_value',\n",
    "    'longitud_optima': 'optimal_length'\n",
    "})\n",
    "\n",
    "# Guardar\n",
    "df.to_csv(os.path.join(output_dir, \"dependencies_full.csv.gz\"), sep=\";\", index=False, compression=\"gzip\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2. Dependencias ponderadas bilateralmente\n",
    "def safe_weighted_average(group):\n",
    "    if group['trade_value'].sum() == 0:\n",
    "        return np.nan\n",
    "    return np.average(group['dependency_value'], weights=group['trade_value'])\n",
    "\n",
    "weighted_dependencies = df.groupby(['dependent_country', 'supplier_country']).apply(\n",
    "    safe_weighted_average\n",
    ").reset_index(name='weighted_dependency')\n",
    "\n",
    "weighted_dependencies.dropna().to_csv(\n",
    "    os.path.join(output_dir, \"weighted_dependencies.csv.gz\"), sep=\";\", index=False, compression=\"gzip\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3. Relaciones críticas\n",
    "critical_relationships = []\n",
    "\n",
    "for industry, data in all_results.items():\n",
    "    crits = identify_critical_trade_relationships(data['results'], threshold=0.7, min_paths=3)\n",
    "    for c in crits:\n",
    "        c['industry'] = industry\n",
    "    critical_relationships.extend(crits)\n",
    "\n",
    "df_critical = pd.DataFrame(critical_relationships)\n",
    "df_critical.to_csv(os.path.join(output_dir, \"critical_relations.csv.gz\"), sep=\";\", index=False, compression=\"gzip\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 4. Intermediarios por industria\n",
    "intermediary_centrality_all = []\n",
    "\n",
    "for industry, data in all_results.items():\n",
    "    for c in data['results']['intermediary_centrality']:\n",
    "        intermediary_centrality_all.append({\n",
    "            'industry': industry,\n",
    "            'country': c[0],\n",
    "            'frequency': c[1],\n",
    "            'strength': c[2],\n",
    "            'centrality_score': c[3]\n",
    "        })\n",
    "\n",
    "df_centrality = pd.DataFrame(intermediary_centrality_all)\n",
    "df_centrality.to_csv(os.path.join(output_dir, \"intermediary_roles.csv.gz\"), sep=\";\", index=False, compression=\"gzip\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 5. Centralidad global agregada\n",
    "global_centrality = df_centrality.groupby(\"country\")[[\"frequency\", \"strength\", \"centrality_score\"]].sum()\n",
    "global_centrality[\"centrality_rank\"] = global_centrality[\"centrality_score\"].rank(ascending=False)\n",
    "global_centrality = global_centrality.sort_values(\"centrality_score\", ascending=False)\n",
    "global_centrality.reset_index().to_csv(\n",
    "    os.path.join(output_dir, \"intermediary_summary.csv.gz\"), sep=\";\", index=False, compression=\"gzip\"\n",
    ")\n",
    "\n",
    "print(\"✅ Todos los archivos CSV han sido generados en:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sección 6.3 - Agregados de dependencia ponderada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Promedio ponderado de dependencia por industria\n",
    "industry_avg = df.groupby(\"industry\").apply(safe_weighted_average).reset_index(name=\"avg_weighted_dependency\")\n",
    "\n",
    "# 2. Top 10 dependencias ponderadas bilaterales\n",
    "top_10_bilateral = weighted_dependencies.sort_values(\"weighted_dependency\", ascending=False).head(10)\n",
    "\n",
    "# 3. Clasificación por tramos de dependencia\n",
    "df[\"dependency_level\"] = pd.cut(\n",
    "    df[\"dependency_value\"],\n",
    "    bins=[0, 0.3, 0.7, 0.9, 1],\n",
    "    labels=[\"baja\", \"media\", \"alta\", \"crítica\"]\n",
    ")\n",
    "\n",
    "dependency_levels_summary = df[\"dependency_level\"].value_counts().reset_index(name=\"num_relaciones\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sección 6.4 - Relaciones críticas con baja redundancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumiendo que 'all_results' está disponible (output del cálculo de dependencias por industria)\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Extraer relaciones críticas por industria\n",
    "critical_relations_all = []\n",
    "\n",
    "for industry, data in all_results.items():\n",
    "    crits = identify_critical_trade_relationships(data['results'], threshold=0.7, min_paths=3)\n",
    "    for c in crits:\n",
    "        c[\"industry\"] = industry\n",
    "    critical_relations_all.extend(crits)\n",
    "\n",
    "# 2. Crear dataframe consolidado\n",
    "df_critical = pd.DataFrame(critical_relations_all)\n",
    "\n",
    "# 3. Ranking de países más afectados como importadores\n",
    "importadores_riesgo = df_critical.groupby(\"importador\").size().reset_index(name=\"relaciones_criticas\")\n",
    "\n",
    "# 4. Ranking como exportadores\n",
    "exportadores_riesgo = df_critical.groupby(\"exportador\").size().reset_index(name=\"exportaciones_criticas\")\n",
    "\n",
    "# 5. Exportar si se desea\n",
    "# df_critical.to_csv(\"relaciones_criticas.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sección 6.5 - Intermediarios críticos y centralidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Consolidar centralidad de intermediarios para todas las industrias\n",
    "all_centrality = []\n",
    "\n",
    "for industry, data in all_results.items():\n",
    "    cent = data['results']['intermediary_centrality']\n",
    "    for entry in cent:\n",
    "        all_centrality.append({\n",
    "            \"industry\": industry,\n",
    "            \"country\": entry[0],\n",
    "            \"frequency\": entry[1],\n",
    "            \"strength\": entry[2],\n",
    "            \"centrality_score\": entry[3]\n",
    "        })\n",
    "\n",
    "df_centrality = pd.DataFrame(all_centrality)\n",
    "\n",
    "# 2. Top 10 intermediarios globales (acumulando por país)\n",
    "global_centrality = df_centrality.groupby(\"country\")[[\"frequency\", \"strength\", \"centrality_score\"]].sum()\n",
    "global_centrality[\"centrality_rank\"] = global_centrality[\"centrality_score\"].rank(ascending=False)\n",
    "global_centrality = global_centrality.sort_values(\"centrality_score\", ascending=False)\n",
    "\n",
    "# 3. Exportar si se desea\n",
    "# df_centrality.to_csv(\"centralidad_intermediarios_por_industria.csv\", index=False)\n",
    "# global_centrality.to_csv(\"intermediarios_globales.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizaciones sugeridas (iniciales en matplotlib/seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Heatmap: dependencia promedio por industria y país\n",
    "pivot_df = df.pivot_table(index=\"dependent_country\", columns=\"industry\", values=\"dependency_value\", aggfunc=\"mean\")\n",
    "plt.figure(figsize=(18, 10))\n",
    "sns.heatmap(pivot_df, cmap=\"YlGnBu\", center=0.5)\n",
    "plt.title(\"Dependencia media por país e industria\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Burbujas: Intermediarios\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=global_centrality.reset_index(),\n",
    "    x=\"frequency\", y=\"strength\", size=\"centrality_score\", hue=\"centrality_score\", sizes=(100, 1000), palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Intermediarios clave según frecuencia y fuerza\")\n",
    "plt.xlabel(\"Frecuencia como intermediario\")\n",
    "plt.ylabel(\"Fuerza acumulada\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dispersión: criticidad vs. caminos alternativos\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_critical, x=\"caminos_alternativos\", y=\"dependencia_total\", hue=\"criticidad\", palette=\"coolwarm\")\n",
    "plt.title(\"Relaciones críticas: dependencia vs. redundancia\")\n",
    "plt.xlabel(\"Número de caminos alternativos\")\n",
    "plt.ylabel(\"Dependencia total\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dependencias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
