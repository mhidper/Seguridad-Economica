{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 align=\"center\"><strong>C√°lculo de dependencia cruzada para todos los pa√≠ses</strong></h1>\n",
    "<h4 align=\"center\"><strong>Manuel Alejandro Hidalgo y Jorge D√≠az Lanchas</strong></h4>\n",
    "<h4 align=\"center\"><strong>Fundaci√≥n Real Instituto Elcano</strong></h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esquema para √çndice de Seguridad Econ√≥mica - Real Instituto Elcano\n",
    "\n",
    "## 1. Introducci√≥n y Marco Conceptual\n",
    "\n",
    "- **Objetivo**: Desarrollar un √≠ndice que cuantifique la seguridad econ√≥mica de los pa√≠ses\n",
    "- **Definici√≥n**: La seguridad econ√≥mica como la capacidad de un pa√≠s para resistir disrupciones en sus cadenas de suministro y comercio internacional\n",
    "- **Relevancia**: Contexto actual de fragmentaci√≥n geoecon√≥mica y tensiones comerciales\n",
    "\n",
    "## 2. Metodolog√≠a\n",
    "\n",
    "### 2.1 Fuentes de Datos\n",
    "- Base de datos International Trade and Production Database (ITP)\n",
    "- Datos comerciales bilaterales por industria (a√±o 2019)\n",
    "- Otros indicadores macroecon√≥micos complementarios\n",
    "\n",
    "### 2.2 Procesamiento de Datos\n",
    "```python\n",
    "# Procesamiento y carga de datos ITP\n",
    "itp2019, codigos_countries = procesar_datos_itp()\n",
    "```\n",
    "\n",
    "### 2.3 Creaci√≥n de Matrices de Comercio\n",
    "```python\n",
    "# Generaci√≥n de matrices bilaterales por industria\n",
    "matrices_comercio = crear_matriz_comercio(data.groupby('industry_descr'), codigos_paises)\n",
    "```\n",
    "\n",
    "### 2.4 Limpieza y Filtrado\n",
    "```python\n",
    "# Eliminar relaciones comerciales insignificantes\n",
    "mat_clean = eliminar_filas_columnas_cero(mat, threshold_pct=0.05)\n",
    "```\n",
    "\n",
    "### 2.5 C√°lculo de Dependencias Econ√≥micas\n",
    "```python\n",
    "# C√°lculo de dependencias directas e indirectas\n",
    "results = analyze_dependencies(X, country_names)\n",
    "```\n",
    "\n",
    "## 3. Componentes del √çndice\n",
    "\n",
    "### 3.1 Dependencia Directa\n",
    "- Medici√≥n de la dependencia inmediata entre pa√≠ses\n",
    "- F√≥rmula: $D_{ij} = \\frac{X_{ji}}{‚àë_k X_{ki}}$, donde $X_{ji}$ es el comercio del pa√≠s j al pa√≠s i\n",
    "\n",
    "### 3.2 Dependencia Indirecta\n",
    "- Medici√≥n de dependencias a trav√©s de cadenas de suministro\n",
    "- Incorporaci√≥n de pa√≠ses intermediarios en las relaciones comerciales\n",
    "- An√°lisis de caminos hasta longitud 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limitar hilos de BLAS antes de importar NumPy (evita oversubscription cuando paralelicemos)\n",
    "import os as _os\n",
    "_os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "_os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import torch\n",
    "\n",
    "# A√±adimos soporte esparso para futuros c√°lculos eficientes\n",
    "from scipy import sparse  # <- nuevo\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# (Opcional) detecci√≥n segura de CuPy sin importarlo directamente (evita aviso del linter)\n",
    "import importlib.util as _importlib_util\n",
    "_cp_spec = _importlib_util.find_spec(\"cupy\")\n",
    "cp = None  # placeholder para evitar NameError si luego lo referenciamos\n",
    "if _cp_spec is not None:\n",
    "    # Solo importamos si realmente est√° instalado\n",
    "    import importlib as _importlib\n",
    "    cp = _importlib.import_module(\"cupy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***No ejecutar este c√≥digo a menos que se quiera comprimir***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprimir_dividir_archivo(archivo_original, tamano_maximo=100, directorio_salida=None):\n",
    "    # Aseg√∫rate de que el archivo original existe\n",
    "    archivo_original = Path(archivo_original)\n",
    "    if not archivo_original.exists():\n",
    "        raise FileNotFoundError(f\"No se encuentra el archivo: {archivo_original}\")\n",
    "    \n",
    "    # Si no se especifica directorio de salida, usar src/data/raw/ITP/\n",
    "    if directorio_salida is None:\n",
    "        # Obtener el directorio ra√≠z del proyecto (donde est√° src/)\n",
    "        proyecto_root = Path.cwd().parent.parent\n",
    "        directorio_salida = proyecto_root /'data' / 'raw' / 'ITP' / 'ITPD_E_R03'\n",
    "    else:\n",
    "        directorio_salida = Path(directorio_salida)\n",
    "    \n",
    "    # Crear el directorio de salida si no existe\n",
    "    directorio_salida.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Abre el archivo original en modo de lectura binaria\n",
    "    with open(archivo_original, 'rb') as f_in:\n",
    "        # Lee el contenido del archivo original\n",
    "        contenido = f_in.read()\n",
    "        \n",
    "        # Determina el n√∫mero de partes necesarias\n",
    "        num_partes = (len(contenido) + tamano_maximo - 1) // tamano_maximo\n",
    "        \n",
    "        # Divide el contenido en partes y escribe cada parte comprimida\n",
    "        for i in range(num_partes):\n",
    "            parte = contenido[i * tamano_maximo: (i + 1) * tamano_maximo]\n",
    "            archivo_salida = directorio_salida / f'ITPD_E_R03.csv.parte{i}.gz'\n",
    "            with gzip.open(archivo_salida, 'wb') as f_out:\n",
    "                f_out.write(parte)\n",
    "            print(f\"Parte {i} creada en: {archivo_salida}\")\n",
    "\n",
    "# Tama√±o m√°ximo por parte (1GB)\n",
    "tamano_maximo = 810 * 1024 * 1024\n",
    "\n",
    "try:\n",
    "    # Ruta al archivo original\n",
    "    archivo_original = Path(r\"C:\\Users\\Usuario\\Downloads\\ITPDE_R03\\ITPDE_R03.csv\")\n",
    "    \n",
    "    # Comprimir y dividir el archivo original\n",
    "    comprimir_dividir_archivo(archivo_original, tamano_maximo)\n",
    "    print(\"Proceso completado con √©xito\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el proceso: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Descomprimir, carga de datos y borrado de archivo***\n",
    "\n",
    "La compresi√≥n se hace para poder trabajar con git sin porblemas de tama√±o de ficheros.\n",
    "Se descomprime, se importa y luego se borra el fichero descomprimido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINO EL A√ëO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anio = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Directorio fuente: c:\\Users\\Usuario\\Documents\\Github\\Seguridad Economica\\data\\raw\\ITP\\ITPD_E_R03\n",
      "Directorio destino: c:\\Users\\Usuario\\Documents\\Github\\Seguridad Economica\\data\\processed\n",
      "Combinando archivos comprimidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:54<01:21, 13.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error durante el procesamiento: \n",
      "Error en la ejecuci√≥n principal: \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FASE 1: PREPARACI√ìN Y CARGA DE DATOS\n",
    "Este script procesa la base de datos International Trade and Production Database (ITP)\n",
    "que viene dividida en m√∫ltiples archivos comprimidos, utilizando aceleraci√≥n GPU\n",
    "cuando est√° disponible.\n",
    "\"\"\"\n",
    "\n",
    "def procesar_datos_itp(year: int = anio):\n",
    "    try:\n",
    "        # Verificar si GPU est√° disponible (solo informativo en esta fase)\n",
    "        gpu_disponible = torch.cuda.is_available()\n",
    "        if gpu_disponible:\n",
    "            print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            print(\"GPU no disponible, se usar√° CPU\")\n",
    "\n",
    "        # Definici√≥n de rutas usando Path y la estructura de tu proyecto\n",
    "        try:\n",
    "            base_path = Path(__file__).parent\n",
    "        except NameError:  # Estamos en un notebook\n",
    "            base_path = Path.cwd().parent.parent  # Asumiendo que el notebook est√° en /notebooks/\n",
    "\n",
    "        source_directory = base_path / \"data\" / \"raw\" / \"ITP\" / \"ITPD_E_R03\"\n",
    "        target_directory = base_path / \"data\" / \"processed\"\n",
    "        target_filename = 'ITPD_E_R03.csv'\n",
    "\n",
    "        # Imprimir las rutas para verificaci√≥n\n",
    "        print(f\"Directorio fuente: {source_directory}\")\n",
    "        print(f\"Directorio destino: {target_directory}\")\n",
    "\n",
    "        # Asegurar que los directorios existen\n",
    "        target_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Verificar que el directorio fuente existe\n",
    "        if not source_directory.exists():\n",
    "            raise FileNotFoundError(f\"No se encuentra el directorio fuente: {source_directory}\")\n",
    "\n",
    "        # Listar archivos comprimidos\n",
    "        chunk_filenames = sorted([\n",
    "            f for f in os.listdir(source_directory)\n",
    "            if f.startswith('ITPD_E_R03.csv.parte') and f.endswith('.gz')\n",
    "        ])\n",
    "\n",
    "        # Control de errores: verificar que existen archivos para procesar\n",
    "        if not chunk_filenames:\n",
    "            raise FileNotFoundError(f\"No se encontraron archivos .gz en {source_directory}\")\n",
    "\n",
    "        # Construir la ruta completa para el archivo combinado\n",
    "        target_filepath = target_directory / target_filename\n",
    "\n",
    "        # Funci√≥n para descomprimir un archivo en paralelo\n",
    "        def descomprimir_archivo(chunk_filename):\n",
    "            chunk_filepath = source_directory / chunk_filename\n",
    "            with gzip.open(chunk_filepath, 'rb') as chunk_file:\n",
    "                return chunk_file.read()\n",
    "\n",
    "        print(\"Combinando archivos comprimidos...\")\n",
    "        with open(target_filepath, 'wb') as target_file:\n",
    "            # Usar ThreadPoolExecutor para paralelizar la descompresi√≥n (I/O + gzip)\n",
    "            with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "                for data in tqdm(\n",
    "                    executor.map(descomprimir_archivo, chunk_filenames),\n",
    "                    total=len(chunk_filenames),\n",
    "                    desc=\"Procesando archivos\"\n",
    "                ):\n",
    "                    target_file.write(data)\n",
    "\n",
    "                print(\"Leyendo archivo CSV...\")\n",
    "\n",
    "                # === columnas exactas que necesitas ===\n",
    "                USECOLS = [\n",
    "                    \"exporter_iso3\",\n",
    "                    \"importer_iso3\",\n",
    "                    \"year\",\n",
    "                    \"trade\",\n",
    "                    \"industry_id\",\n",
    "                    \"industry_descr\",\n",
    "                    \"importer_name\",\n",
    "                    \"exporter_name\",\n",
    "                ]\n",
    "                DTYPES = {\n",
    "                    \"exporter_iso3\": \"string\",\n",
    "                    \"importer_iso3\": \"string\",\n",
    "                    \"year\": \"int32\",\n",
    "                    \"trade\": \"float32\",        # reduce memoria sin perder precisi√≥n pr√°ctica\n",
    "                    \"industry_id\": \"int32\",\n",
    "                    \"industry_descr\": \"string\",\n",
    "                    \"importer_name\": \"string\",\n",
    "                    \"exporter_name\": \"string\",\n",
    "                }\n",
    "\n",
    "                # Dask: lectura lazy del CSV combinado\n",
    "                print(\"Usando Dask para procesamiento en paralelo (lectura filtrada)\")\n",
    "                dask_df = dd.read_csv(\n",
    "                    target_filepath,\n",
    "                    sep=\",\",\n",
    "                    usecols=USECOLS,\n",
    "                    dtype=DTYPES,\n",
    "                    blocksize=\"128MB\",\n",
    "                    assume_missing=True,  # tolerante a nulos espor√°dicos\n",
    "                )\n",
    "                dask_df['year'].unique()\n",
    "                print(f\"Filtrando datos del a√±o {year}...\")\n",
    "                dask_df = dask_df[dask_df[\"year\"] == year]\n",
    "\n",
    "                # Ejecutar el plan y materializar en pandas\n",
    "                itp_year = dask_df.compute()\n",
    "\n",
    "                # Tipos finales (por si Dask promovi√≥ algo)\n",
    "                itp_year = itp_year.astype(DTYPES)\n",
    "\n",
    "                # (Opcional) comprime memoria de las cadenas con 'category' si vas a agrupar mucho despu√©s\n",
    "                # itp_year[\"exporter_iso3\"]  = itp_year[\"exporter_iso3\"].astype(\"category\")\n",
    "                # itp_year[\"importer_iso3\"]  = itp_year[\"importer_iso3\"].astype(\"category\")\n",
    "                # itp_year[\"industry_descr\"] = itp_year[\"industry_descr\"].astype(\"category\")\n",
    "                # itp_year[\"importer_name\"]  = itp_year[\"importer_name\"].astype(\"category\")\n",
    "                # itp_year[\"exporter_name\"]  = itp_year[\"exporter_name\"].astype(\"category\")\n",
    "\n",
    "\n",
    "        # Limpieza del temporal grande\n",
    "        try:\n",
    "            os.remove(target_filepath)\n",
    "            print(\"Archivo temporal eliminado\")\n",
    "        except Exception as _e:\n",
    "            print(f\"Advertencia: no se pudo eliminar el temporal ({_e})\")\n",
    "\n",
    "        # 4) Pa√≠ses √∫nicos importadores (GPU no aporta aqu√≠; unique de pandas es muy r√°pido)\n",
    "        #    Convertimos a category para memoria/velocidad y extraemos categor√≠as ordenadas\n",
    "        itp_year[\"importer_iso3\"] = itp_year[\"importer_iso3\"].astype(\"category\")\n",
    "        codigos_countries = list(itp_year[\"importer_iso3\"].cat.categories)\n",
    "\n",
    "        print(f\"Total de pa√≠ses √∫nicos encontrados: {len(codigos_countries)}\")\n",
    "        return itp_year, codigos_countries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el procesamiento: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data, countries = procesar_datos_itp(year=anio)\n",
    "        print(\"Procesamiento completado con √©xito\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la ejecuci√≥n principal: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cat√°logo de industrias generado: 168 filas\n",
      "- CSV: c:\\Users\\Usuario\\Documents\\Github\\Seguridad Economica\\data\\processed\\Dependencias_consolidadas\\industrias_id_nombre.csv\n",
      "- Parquet: c:\\Users\\Usuario\\Documents\\Github\\Seguridad Economica\\data\\processed\\Dependencias_consolidadas\\industrias_id_nombre.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Posibles alias de columnas en ITP (ajusta si conoces los exactos)\n",
    "CAND_ID = [\"industry_id\", \"industry_code\", \"industry\", \"sector_id\", \"sector_code\", \"isic\", \"isic4\"]\n",
    "CAND_NAME = [\"industry_descr\", \"industry_name\", \"industry_label\", \"sector_name\", \"sector_descr\"]\n",
    "\n",
    "def _pick_col(candidates, cols):\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "id_col = _pick_col(CAND_ID, data.columns)\n",
    "name_col = _pick_col(CAND_NAME, data.columns)\n",
    "\n",
    "if not id_col or not name_col:\n",
    "    print(\"‚ö†Ô∏è No se encontraron columnas de industria en 'data'.\")\n",
    "    print(f\"Columnas disponibles: {list(data.columns)}\")\n",
    "    print(\n",
    "        \"Sugerencia: en la celda de carga (procesar_datos_itp), a√±ade las columnas de industria \"\n",
    "        \"a USECOLS/DTYPES, por ejemplo: 'industry_id' y 'industry_descr' (o sus equivalentes).\"\n",
    "    )\n",
    "else:\n",
    "    # Cat√°logo √∫nico, limpio y ordenado\n",
    "    industrias = (\n",
    "        data[[id_col, name_col]]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .sort_values([id_col, name_col])\n",
    "        .reset_index(drop=True)\n",
    "        .rename(columns={id_col: \"industry_id\", name_col: \"industry_descr\"})\n",
    "    )\n",
    "\n",
    "    # Tipos compactos\n",
    "    industrias[\"industry_id\"] = industrias[\"industry_id\"].astype(\"string\")\n",
    "    industrias[\"industry_descr\"] = industrias[\"industry_descr\"].astype(\"string\")\n",
    "\n",
    "    # Rutas (evitamos espacios en nombres de carpetas)\n",
    "    base_path = Path.cwd().parent.parent\n",
    "    target_directory = base_path / \"data\" / \"processed\" / \"Dependencias_consolidadas\"\n",
    "    target_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Salidas\n",
    "    csv_path = target_directory / \"industrias_id_nombre.csv\"\n",
    "    pq_path  = target_directory / \"industrias_id_nombre.parquet\"\n",
    "\n",
    "    industrias.to_csv(csv_path, sep=\";\", index=False)\n",
    "    industrias.to_parquet(pq_path, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Cat√°logo de industrias generado: {len(industrias)} filas\")\n",
    "    print(f\"- CSV: {csv_path}\")\n",
    "    print(f\"- Parquet: {pq_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices de Comercio Internacional: Creaci√≥n Optimizada con Aceleraci√≥n GPU\n",
    "\n",
    "El siguiente c√≥digo implementa una funci√≥n optimizada para crear matrices de comercio bilateral para cada industria a partir de datos comerciales internacionales. Esta implementaci√≥n utiliza t√©cnicas avanzadas para mejorar significativamente el rendimiento:\n",
    "\n",
    "### Caracter√≠sticas principales:\n",
    "\n",
    "- **Aceleraci√≥n por GPU**: Detecta autom√°ticamente si hay una GPU disponible y la utiliza para acelerar el procesamiento cuando hay suficientes datos.\n",
    "- **Operaciones vectorizadas**: Minimiza las iteraciones fila por fila usando enfoques vectorizados m√°s eficientes.\n",
    "- **Gesti√≥n de memoria optimizada**: Reutiliza estructuras para reducir la fragmentaci√≥n y el consumo de memoria.\n",
    "- **Adaptabilidad**: Ajusta autom√°ticamente la estrategia de procesamiento seg√∫n el volumen de datos y el hardware disponible.\n",
    "\n",
    "El resultado es un diccionario donde cada clave representa una industria, y cada valor es una matriz completa de comercio bilateral entre todos los pa√≠ses del conjunto de datos, lo que facilita los an√°lisis posteriores de dependencias comerciales y flujos econ√≥micos.\n",
    "\n",
    "Este enfoque es particularmente √∫til para el c√°lculo de √≠ndices de dependencia comercial, ya que proporciona una representaci√≥n eficiente de los flujos comerciales entre pa√≠ses para cada sector industrial.\n",
    "\n",
    "### Limpieza de Matrices de Comercio: Eliminaci√≥n de Flujos No Significativos\n",
    "\n",
    "El siguiente c√≥digo implementa una funci√≥n para eliminar del an√°lisis aquellos pa√≠ses con flujos comerciales no significativos. Este paso es crucial para mejorar la precisi√≥n del an√°lisis de dependencias comerciales, ya que permite enfocarse en relaciones econ√≥micas realmente relevantes.\n",
    "\n",
    "**Funcionamiento**:\n",
    "\n",
    "- **Umbral de significancia**: Establece un umbral m√≠nimo (por defecto 0.05%) del comercio mundial total. Cualquier flujo comercial por debajo de este umbral se considera no significativo y se convierte a cero.\n",
    "- **Identificaci√≥n de pa√≠ses no relevantes**: Detecta pa√≠ses que, despu√©s de aplicar el umbral, no tienen flujos comerciales significativos (tanto como exportadores como importadores).\n",
    "- **Limpieza de la matriz**: Elimina estos pa√≠ses de la matriz, reduciendo su dimensionalidad y concentrando el an√°lisis en actores relevantes del comercio internacional.\n",
    "\n",
    "Esta funci√≥n de preprocesamiento es especialmente importante para estudios de dependencia econ√≥mica, ya que:\n",
    "1. Reduce el ruido en los datos al eliminar relaciones comerciales marginales\n",
    "2. Mejora la eficiencia computacional al trabajar con matrices m√°s peque√±as\n",
    "3. Permite concentrarse en dependencias significativas que podr√≠an representar vulnerabilidades reales\n",
    "\n",
    "El resultado es una matriz \"limpia\" que contiene √∫nicamente los pa√≠ses y flujos comerciales relevantes para el an√°lisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV de totales guardado en: c:\\Users\\Usuario\\Documents\\Github\\Seguridad Economica\\data\\processed\\Dependencias_consolidadas\\totales_comercio_por_pais_industria.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "def crear_matriz_comercio_optimizado(\n",
    "    grouped_data, \n",
    "    codigos_paises: List[str],\n",
    "    target_directory: Path | None = None\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Crea matrices de comercio bilateral (exportador x importador) para cada industria,\n",
    "    usando operaciones vectorizadas (pivot) y deja un CSV de totales por pa√≠s/industria.\n",
    "\n",
    "    Args:\n",
    "        grouped_data: resultado de data.groupby('industry_descr') (o similar).\n",
    "        codigos_paises: lista de ISO3 a usar como √≠ndice/columnas de referencia.\n",
    "        target_directory: carpeta donde guardar 'totales_comercio_por_pais_industria.csv'.\n",
    "\n",
    "    Returns:\n",
    "        dict: {industry -> DataFrame (index=exporter_iso3, columns=importer_iso3)}\n",
    "    \"\"\"\n",
    "    # Resolver destino si no viene dado\n",
    "    if target_directory is None:\n",
    "        try:\n",
    "            base_path = Path(__file__).parent\n",
    "        except NameError:  # notebook\n",
    "            base_path = Path.cwd().parent.parent\n",
    "        target_directory = base_path / \"data\" / \"processed\" / \"Dependencias_consolidadas\"\n",
    "    target_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    matrices: Dict[str, pd.DataFrame] = {}\n",
    "    totales_registros = []\n",
    "\n",
    "    # Columnas requeridas con alias tolerantes\n",
    "    cols = grouped_data.obj.columns\n",
    "    trade_col = None\n",
    "    for cand in (\"trade\", \"value\", \"trade_value\"):\n",
    "        if cand in cols:\n",
    "            trade_col = cand\n",
    "            break\n",
    "    required = {\"exporter_iso3\", \"importer_iso3\"}\n",
    "    if trade_col is None or not required.issubset(cols):\n",
    "        raise ValueError(\n",
    "            f\"Faltan columnas. Necesito {required} y una de ['trade','value','trade_value'].\\n\"\n",
    "            f\"Columnas disponibles: {list(cols)}\"\n",
    "        )\n",
    "\n",
    "    # Preconstruir un DataFrame vac√≠o (para industrias sin datos v√°lidos)\n",
    "    template = pd.DataFrame(\n",
    "        0.0, index=codigos_paises, columns=codigos_paises, dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # Bucle por industria (grouped_data es p.groupby(...))\n",
    "    for industry, group in grouped_data:\n",
    "        # Filtrar a pa√≠ses en la lista (evita claves no esperadas)\n",
    "        g = group[\n",
    "            group[\"exporter_iso3\"].isin(codigos_paises) &\n",
    "            group[\"importer_iso3\"].isin(codigos_paises)\n",
    "        ]\n",
    "\n",
    "        if g.empty:\n",
    "            matrices[industry] = template.copy()\n",
    "            # totales = 0 para todos los pa√≠ses en esta industria\n",
    "            totales_registros.extend(\n",
    "                {\"pais\": p, \"industria\": industry, \"valor_importado\": 0.0}\n",
    "                for p in codigos_paises\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Pivot vectorizado y suma de duplicados\n",
    "        mat = (\n",
    "            g.pivot_table(\n",
    "                index=\"exporter_iso3\",\n",
    "                columns=\"importer_iso3\",\n",
    "                values=trade_col,\n",
    "                aggfunc=\"sum\",\n",
    "                fill_value=0.0,\n",
    "                observed=False\n",
    "            )\n",
    "            .reindex(index=codigos_paises, columns=codigos_paises, fill_value=0.0)\n",
    "            .astype(np.float32)\n",
    "        )\n",
    "\n",
    "        matrices[industry] = mat\n",
    "\n",
    "        # Totales por pa√≠s importador (columna): suma vectorizada\n",
    "        totales = mat.sum(axis=0)  # importaciones totales por pa√≠s en esta industria\n",
    "        totales_registros.extend(\n",
    "            {\"pais\": pais, \"industria\": industry, \"valor_importado\": float(totales[pais])}\n",
    "            for pais in codigos_paises\n",
    "        )\n",
    "\n",
    "    # Guardar totales\n",
    "    df_totales = pd.DataFrame(totales_registros)\n",
    "    df_totales.to_csv(\n",
    "        target_directory / \"totales_comercio_por_pais_industria.csv\",\n",
    "        index=False, sep=\";\"\n",
    "    )\n",
    "    print(f\"Archivo CSV de totales guardado en: {target_directory/'totales_comercio_por_pais_industria.csv'}\")\n",
    "\n",
    "    return matrices\n",
    "\n",
    "\n",
    "# Define la lista de c√≥digos de pa√≠ses\n",
    "codigos_paises = sorted(data['importer_iso3'].unique().tolist())\n",
    "\n",
    "# Llama a la funci√≥n optimizada (mismo API esperado)\n",
    "matrices_comercio = crear_matriz_comercio_optimizado(\n",
    "    data.groupby('industry_descr'),\n",
    "    codigos_paises=codigos_paises,\n",
    "    # target_directory=Path(\"...\")  # opcional\n",
    ")\n",
    "\n",
    "def eliminar_filas_columnas_cero(df: pd.DataFrame, threshold_pct: float = 0.005) -> pd.DataFrame:\n",
    "    \"\"\"Filtra una matriz aplicando umbral relativo por pa√≠s importador (columna).\"\"\"\n",
    "    # Umbral por columna: total_col * pct\n",
    "    col_totals = df.sum(axis=0)\n",
    "    thresholds = col_totals * float(threshold_pct)\n",
    "\n",
    "    # Aplicar m√°scara vectorizada (alineaci√≥n por columnas)\n",
    "    df_filtered = df.where(df >= thresholds, 0.0)\n",
    "\n",
    "    # Pa√≠ses con filas y columnas a cero tras umbral\n",
    "    zero_rows = df_filtered.index[df_filtered.sum(axis=1) == 0.0]\n",
    "    zero_cols = df_filtered.columns[df_filtered.sum(axis=0) == 0.0]\n",
    "    to_drop = list(set(zero_rows) & set(zero_cols))\n",
    "\n",
    "    return df_filtered.drop(index=to_drop, columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C√°lculo de dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Qu√© hacen estas funciones (y por qu√© son m√°s r√°pidas)\n",
    "\n",
    "Este bloque sustituye la enumeraci√≥n exhaustiva de combinaciones de intermediarios por una **b√∫squeda en profundidad (DFS) con poda** sobre un **grafo ralo**. Mantiene la **misma firma y estructura de salida** que tu `process_country_pair` original, pero calcula **caminos simples** (sin repetir nodos) de forma mucho m√°s eficiente.\n",
    "\n",
    "### Ideas clave\n",
    "- **Matriz de transici√≥n `T`**: se normaliza la matriz de comercio `X` por columnas:  \n",
    "  `T[u, v] = X[u, v] / sum_a X[a, v]`. Esto es exactamente el cociente que ya usabas en `calculate_path_dependency`.\n",
    "- **Grafo ralo**: solo se conservan aristas con peso `T[u,v] ‚â• eps_edge` y, opcionalmente, los **top_m** destinos m√°s fuertes por nodo. As√≠ reducimos el branching.\n",
    "- **DFS con poda por cota superior**: en cada expansi√≥n, si la **mejor contribuci√≥n posible restante** es inferior a `eps_contrib`, se corta la rama. Evita explorar caminos que no aportar√≠an nada relevante.\n",
    "\n",
    "---\n",
    "\n",
    "### Funciones\n",
    "\n",
    "#### `_prepare_graph_local(X_clean, eps_edge, top_m)`\n",
    "- **Entrada:** matriz de comercio `X_clean` (cuadrada), umbral de arista `eps_edge` y l√≠mite de vecinos `top_m`.\n",
    "- **Salida:**\n",
    "  - `T`: matriz de transici√≥n normalizada (float32).\n",
    "  - `neighbors`: lista (por nodo) de vecinos salientes **ordenados por peso descendente**, filtrados por `eps_edge` y truncados a `top_m`.\n",
    "  - `max_out`: para cada nodo, el **peso m√°ximo** de sus aristas salientes (sirve para la cota en la poda).\n",
    "- **Rol:** preparar una representaci√≥n esbelta del grafo para acelerar el DFS.\n",
    "\n",
    "#### `_sum_simple_paths_DFS(src, dst, T, neighbors, max_out, Lmax, eps_contrib)`\n",
    "- **Calcula la suma de contribuciones** de **todos los caminos simples** de `src` a `dst` con longitudes `2..Lmax`.\n",
    "- **Poda por cota:** si `producto_actual * (max_out[u] ** pasos_restantes) < eps_contrib`, no se expande esa rama.\n",
    "- **Devuelve:**\n",
    "  - `indirect_total`: suma de todas las contribuciones indirectas (longitud ‚â• 2).\n",
    "  - `deps_by_len`: diccionario `{longitud -> contribuci√≥n}`, con `deps_by_len[1] = T[src, dst]` para coherencia con tu salida.\n",
    "  - `significant_paths`: lista (opcional) con rutas fuertes si se activa su recopilaci√≥n (por defecto vac√≠a para rendimiento).\n",
    "\n",
    "#### `process_country_pair(i, j, X_clean, denominators, country_names, max_possible_length, convergence_threshold, path_strength_threshold)`\n",
    "- **Entrada:** √≠ndices `i` (importador), `j` (exportador), matriz `X_clean`, denominadores por columna, nombres de pa√≠ses y par√°metros.\n",
    "- **Pasos:**\n",
    "  1. **Dependencia directa (DD):** `X[j, i] / denominators[i]` (igual que antes).\n",
    "  2. **Grafo ralo:** crea `T`, `neighbors`, `max_out` con `_prepare_graph_local` usando  \n",
    "     `eps_edge = path_strength_threshold` (umbral de arista).\n",
    "  3. **Dependencia indirecta (DI):** suma **caminos simples** `j ‚Üí ‚Ä¶ ‚Üí i` con `_sum_simple_paths_DFS` hasta `max_possible_length`, podando por `eps_contrib = path_strength_threshold`.\n",
    "  4. **Convergencia por longitud:** si al a√±adir la contribuci√≥n de la siguiente longitud el aumento es `< convergence_threshold`, se considera convergido antes de `Lmax`.\n",
    "- **Salida (id√©ntica en forma a la original):**\n",
    "  - `result`: dict con `importador`, `exportador`, `trade_value`, `dependencia_directa`, `dependencia_indirecta`, `dependencia_total`, `longitud_optima`, `dependencias_por_longitud`.\n",
    "  - `pair_key`: `\"exportador->importador\"`.\n",
    "  - `top_dependency`: tu tupla `(importador, exportador, DD, DI, DT, longitud_optima)`.\n",
    "  - `significant_paths`: lista (vac√≠a por defecto para velocidad).\n",
    "  - `length_converged`: longitud donde se detuvo por convergencia (0 si no aplica).\n",
    "\n",
    "---\n",
    "\n",
    "### Par√°metros importantes (y c√≥mo afinarlos)\n",
    "- `max_possible_length` (`Lmax`): longitud m√°xima de los caminos simples. T√≠pico: **3‚Äì5**.\n",
    "- `path_strength_threshold`:\n",
    "  - Como **`eps_edge`**: filtra aristas d√©biles al construir el grafo.\n",
    "  - Como **`eps_contrib`**: poda ramas cuya contribuci√≥n restante es irrelevante.\n",
    "- `top_m` (en `_prepare_graph_local`): **m√°ximo de vecinos salientes** por nodo que se exploran (ordenados por peso).  \n",
    "  - 10‚Äì20 ‚áí muy r√°pido; 30‚Äì50 ‚áí m√°s cobertura; `None` ‚áí sin l√≠mite (m√°s lento).\n",
    "- `convergence_threshold`: criterio para parar por longitud si los incrementos son despreciables.\n",
    "\n",
    "---\n",
    "\n",
    "### Exactitud vs. rendimiento\n",
    "- Se siguen sumando **caminos simples** (como quer√≠as), pero:\n",
    "  - Descartamos aristas y ramas **que no mover√≠an el resultado** m√°s all√° de tus umbrales.\n",
    "  - El orden de expansi√≥n por aristas fuertes permite **podar antes**.\n",
    "- Ajusta `path_strength_threshold` y `top_m` para el mejor equilibrio **tiempo ‚Üî precisi√≥n**.  \n",
    "  Empieza con `path_strength_threshold = 1e-3` (o tu valor actual) y `top_m = 20`.\n",
    "\n",
    "> Si m√°s adelante necesitas listar **rutas** para un par concreto en el dashboard, podemos activar la captura de `significant_paths` solo **on-demand**, sin penalizar el c√°lculo batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_path_dependency(X_clean, path, denominators):\n",
    "    \"\"\"Calcula la dependencia de un camino espec√≠fico.\"\"\"\n",
    "    fuerza_camino = 1.0\n",
    "    for k in range(len(path) - 1):\n",
    "        if denominators[path[k+1]] > 0:\n",
    "            fuerza_camino *= X_clean[path[k], path[k+1]] / denominators[path[k+1]]\n",
    "        else:\n",
    "            fuerza_camino = 0  # Si el denominador es cero, la fuerza del camino es cero\n",
    "    return fuerza_camino\n",
    "\n",
    "def calculate_intermediary_centrality(intermediary_frequency, intermediary_strength, country_names):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas de centralidad para intermediarios.\n",
    "    \n",
    "    Esta funci√≥n es la misma que la original.\n",
    "    \"\"\"\n",
    "    # Implementaci√≥n existente\n",
    "    centrality = []\n",
    "    \n",
    "    # Normalizar\n",
    "    max_freq = max(intermediary_frequency.values()) if intermediary_frequency.values() else 1\n",
    "    max_strength = max(intermediary_strength.values()) if intermediary_strength.values() else 1\n",
    "    \n",
    "    for country in country_names:\n",
    "        norm_freq = intermediary_frequency[country] / max_freq if max_freq > 0 else 0\n",
    "        norm_strength = intermediary_strength[country] / max_strength if max_strength > 0 else 0\n",
    "        \n",
    "        combined_score = 0.4 * norm_freq + 0.6 * norm_strength\n",
    "        \n",
    "        centrality.append((country, intermediary_frequency[country], \n",
    "                          intermediary_strength[country], combined_score))\n",
    "    \n",
    "    centrality.sort(key=lambda x: x[3], reverse=True)\n",
    "    return centrality\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "def process_country_pair(i, j, X_clean, denominators, country_names,\n",
    "                         max_possible_length, convergence_threshold,\n",
    "                         path_strength_threshold, T):\n",
    "    \"\"\"\n",
    "    Exactamente la misma definici√≥n que antes (combinations), pero:\n",
    "    - Usa T (matriz de transici√≥n) precalculada.\n",
    "    - Vectoriza la longitud 2.\n",
    "    - Mantiene mismos retornos y campos.\n",
    "    \"\"\"\n",
    "    n = X_clean.shape[0]\n",
    "\n",
    "    # Dependencia directa (igual que antes)\n",
    "    trade_value = X_clean[j, i]\n",
    "    direct_dependency = (X_clean[j, i] / denominators[i]) if denominators[i] > 0 else 0.0\n",
    "\n",
    "    dependencies_by_length = {1: direct_dependency}\n",
    "    significant_paths = []\n",
    "    current_total = direct_dependency\n",
    "    indirect_total = 0.0\n",
    "    length = 1\n",
    "\n",
    "    # Candidatos de intermediarios (igual que antes)\n",
    "    middle = [k for k in range(n) if k != i and k != j]\n",
    "\n",
    "    # ---- L = 2 (vectorizado, EXACTO) ----\n",
    "    if max_possible_length >= 2 and middle:\n",
    "        row_j = T[j, :]\n",
    "        col_i = T[:, i]\n",
    "        mask = np.ones(n, dtype=bool)\n",
    "        mask[[i, j]] = False\n",
    "\n",
    "        # suma exacta: sum_k T[j,k]*T[k,i], k!=i,j\n",
    "        di2 = np.dot(row_j[mask], col_i[mask])\n",
    "        dependencies_by_length[2] = float(di2)\n",
    "        indirect_total += float(di2)\n",
    "        current_total = direct_dependency + indirect_total\n",
    "        length = 2\n",
    "\n",
    "        # caminos significativos para L=2 (mismo umbral)\n",
    "        if path_strength_threshold > 0:\n",
    "            ks = np.nonzero(row_j * col_i > path_strength_threshold)[0]\n",
    "            ks = [k for k in ks if k != i and k != j]\n",
    "            if ks:\n",
    "                for k in ks:\n",
    "                    significant_paths.append({\n",
    "                        'exportador': country_names[j],\n",
    "                        'importador': country_names[i],\n",
    "                        'intermediarios': [country_names[k]],\n",
    "                        'fuerza': float(row_j[k] * col_i[k]),\n",
    "                        'longitud': 2\n",
    "                    })\n",
    "\n",
    "        # criterio de convergencia\n",
    "        if abs(current_total - direct_dependency) < convergence_threshold or max_possible_length == 2:\n",
    "            # ordenar y devolver\n",
    "            significant_paths.sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "            pair_key = f\"{country_names[j]}->{country_names[i]}\"\n",
    "            result = {\n",
    "                'importador': country_names[i],\n",
    "                'exportador': country_names[j],\n",
    "                'trade_value': trade_value,\n",
    "                'dependencia_directa': direct_dependency,\n",
    "                'dependencia_indirecta': indirect_total,\n",
    "                'dependencia_total': direct_dependency + indirect_total,\n",
    "                'longitud_optima': 2,\n",
    "                'dependencias_por_longitud': dependencies_by_length\n",
    "            }\n",
    "            return {\n",
    "                'pair_key': pair_key,\n",
    "                'result': result,\n",
    "                'top_dependency': (country_names[i], country_names[j],\n",
    "                                   direct_dependency, indirect_total,\n",
    "                                   direct_dependency + indirect_total, 2),\n",
    "                'significant_paths': significant_paths,\n",
    "                'length_converged': 2\n",
    "            }\n",
    "\n",
    "    # ---- L >= 3 (exacto con combinations, pero usando T) ----\n",
    "    for L in range(3, max_possible_length + 1):\n",
    "        DI_ij_L = 0.0\n",
    "        for interms in combinations(middle, L - 1):\n",
    "            path = (j,) + interms + (i,)\n",
    "            # producto exacto de T a lo largo del camino\n",
    "            prod = 1.0\n",
    "            for a, b in zip(path[:-1], path[1:]):\n",
    "                w = T[a, b]\n",
    "                if w == 0.0:\n",
    "                    prod = 0.0\n",
    "                    break\n",
    "                prod *= w\n",
    "\n",
    "            DI_ij_L += prod\n",
    "\n",
    "            if prod > path_strength_threshold:\n",
    "                significant_paths.append({\n",
    "                    'exportador': country_names[j],\n",
    "                    'importador': country_names[i],\n",
    "                    'intermediarios': [country_names[x] for x in interms],\n",
    "                    'fuerza': float(prod),\n",
    "                    'longitud': L\n",
    "                })\n",
    "\n",
    "        dependencies_by_length[L] = float(DI_ij_L)\n",
    "        indirect_total += float(DI_ij_L)\n",
    "\n",
    "        prev_total = current_total\n",
    "        current_total = direct_dependency + indirect_total\n",
    "        length = L\n",
    "\n",
    "        if L > 1 and abs(current_total - prev_total) < convergence_threshold:\n",
    "            break\n",
    "\n",
    "    # Final\n",
    "    significant_paths.sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    total_dependency = direct_dependency + indirect_total\n",
    "    pair_key = f\"{country_names[j]}->{country_names[i]}\"\n",
    "    result = {\n",
    "        'importador': country_names[i],\n",
    "        'exportador': country_names[j],\n",
    "        'trade_value': trade_value,\n",
    "        'dependencia_directa': direct_dependency,\n",
    "        'dependencia_indirecta': indirect_total,\n",
    "        'dependencia_total': total_dependency,\n",
    "        'longitud_optima': length,\n",
    "        'dependencias_por_longitud': dependencies_by_length\n",
    "    }\n",
    "    return {\n",
    "        'pair_key': pair_key,\n",
    "        'result': result,\n",
    "        'top_dependency': (country_names[i], country_names[j],\n",
    "                           direct_dependency, indirect_total, total_dependency, length),\n",
    "        'significant_paths': significant_paths,\n",
    "        'length_converged': length if length > 1 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_all_dependencies_parallel(X, country_names=None, convergence_threshold=0.01, \n",
    "                                       max_possible_length=3, \n",
    "                                       path_strength_threshold=0.001, n_jobs=None, use_gpu=True, \n",
    "                                       debug_mode=False):\n",
    "    \"\"\"\n",
    "    Versi√≥n paralelizada del c√°lculo de dependencias que mantiene EXACTAMENTE\n",
    "    la misma salida que la versi√≥n original.\n",
    "    \n",
    "    El par√°metro debug_mode permite verificar que el n√∫mero de dependencias\n",
    "    coincida con la versi√≥n original.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Versi√≥n paralelizada del c√°lculo de dependencias.\n",
    "    \n",
    "    Parameters adicionales:\n",
    "    -----------------------\n",
    "    n_jobs : int, opcional\n",
    "        N√∫mero de trabajos paralelos. Si es None, usa todos los n√∫cleos disponibles.\n",
    "    use_gpu : bool, default=True\n",
    "        Si se debe intentar usar GPU para acelerar algunos c√°lculos.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "\n",
    "    if country_names is None:\n",
    "        country_names = [f\"Pa√≠s {i}\" for i in range(n)]\n",
    "\n",
    "    if len(country_names) != n:\n",
    "        raise ValueError(f\"La longitud de country_names ({len(country_names)}) no coincide con la dimensi√≥n de X ({n})\")\n",
    "\n",
    "    # Configurar paralelizaci√≥n\n",
    "    if n_jobs is None:\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "    \n",
    "    # Verificar disponibilidad de GPU\n",
    "    gpu_available = torch.cuda.is_available() and use_gpu\n",
    "    X_clean = X\n",
    "\n",
    "    denom = X_clean.sum(axis=0, dtype=np.float64)\n",
    "    denom[denom == 0.0] = np.inf\n",
    "    T = (X_clean / denom).astype(np.float64, copy=False)\n",
    "\n",
    "    denominators = np.sum(X, axis=0)\n",
    "\n",
    "    # Acelerar c√°lculos directos con GPU si est√° disponible\n",
    "    if gpu_available:\n",
    "        # Transferir datos a GPU\n",
    "        X_gpu = torch.tensor(X_clean, device='cuda', dtype=torch.float32)\n",
    "        denom_gpu = torch.tensor(denominators, device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        # Calcular dependencias directas en forma vectorizada\n",
    "        direct_deps = torch.zeros_like(X_gpu)\n",
    "        for i in range(n):\n",
    "            # Evitar divisi√≥n por cero\n",
    "            if denom_gpu[i] > 0:\n",
    "                direct_deps[:, i] = X_gpu[:, i] / denom_gpu[i]\n",
    "        \n",
    "        # Transferir resultados de vuelta a CPU\n",
    "        direct_dependencies = direct_deps.cpu().numpy()\n",
    "        \n",
    "        # Usar estas dependencias directas precalculadas en el procesamiento posterior\n",
    "        # (Aunque en esta implementaci√≥n seguimos calcul√°ndolas en process_country_pair para\n",
    "        # mantener cambios m√≠nimos en el c√≥digo)\n",
    "\n",
    "    # Estructura de resultados extendida\n",
    "    results = {\n",
    "        'dependencies': [],\n",
    "        'top_dependencies': [],\n",
    "        'avg_dependencies': {},\n",
    "        'length_distribution': np.zeros(max_possible_length),\n",
    "        'critical_intermediaries': {},     # Intermediarios cr√≠ticos por relaci√≥n\n",
    "        'intermediary_frequency': {},      # Frecuencia de pa√≠ses como intermediarios\n",
    "        'critical_paths': [],              # Rutas cr√≠ticas completas\n",
    "        'intermediary_strength': {}        # Fuerza de cada pa√≠s como intermediario\n",
    "    }\n",
    "    \n",
    "    # Inicializar contadores para intermediarios\n",
    "    for country in country_names:\n",
    "        results['intermediary_frequency'][country] = 0\n",
    "        results['intermediary_strength'][country] = 0.0\n",
    "\n",
    "    # Preparar pares de pa√≠ses para procesamiento paralelo \n",
    "    # Mantenemos la misma estructura de iteraci√≥n del c√≥digo original\n",
    "    # Primero por importador (i) y luego por exportador (j)\n",
    "        # Preparar pares de pa√≠ses (sin tqdm)\n",
    "    country_pairs = [(i, j) for i in range(n) for j in range(n) if i != j]\n",
    "\n",
    "    # Procesar pares de pa√≠ses en paralelo (sin barra de progreso)\n",
    "    with Parallel(n_jobs=n_jobs) as parallel:\n",
    "        pair_results = parallel(\n",
    "            delayed(process_country_pair)(\n",
    "                i, j, X_clean, denom, country_names, \n",
    "                max_possible_length, convergence_threshold, path_strength_threshold,\n",
    "                T  # <-- NUEVO ARGUMENTO\n",
    "            )\n",
    "            for i, j in country_pairs\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "    # Agrupar resultados por pa√≠s importador\n",
    "    results_by_importer = {}\n",
    "    for res in pair_results:\n",
    "        importer = res['result']['importador']\n",
    "        if importer not in results_by_importer:\n",
    "            results_by_importer[importer] = []\n",
    "        results_by_importer[importer].append(res)\n",
    "    \n",
    "    # Recolectar critical paths de todos los pares para ordenarlos despu√©s\n",
    "    all_critical_paths = []\n",
    "    \n",
    "    # Procesar los resultados manteniendo el mismo orden que el c√≥digo original\n",
    "    for i in range(n):\n",
    "        importer = country_names[i]\n",
    "        total_dep = 0.0\n",
    "        num_deps = 0\n",
    "        \n",
    "        if importer in results_by_importer:\n",
    "            for res in results_by_importer[importer]:\n",
    "                # Agregar a dependencies\n",
    "                results['dependencies'].append(res['result'])\n",
    "                \n",
    "                # Agregar a top_dependencies\n",
    "                results['top_dependencies'].append(res['top_dependency'])\n",
    "                \n",
    "                # Actualizar critical_intermediaries\n",
    "                results['critical_intermediaries'][res['pair_key']] = res['significant_paths']\n",
    "                \n",
    "                # Recolectar critical paths\n",
    "                all_critical_paths.extend(res['significant_paths'])\n",
    "                \n",
    "                # Actualizar length_distribution si convergi√≥\n",
    "                if res['length_converged'] > 1:\n",
    "                    results['length_distribution'][res['length_converged'] - 1] += 1\n",
    "                \n",
    "                # Actualizar dependencia promedio\n",
    "                total_dep += res['result']['dependencia_total']\n",
    "                num_deps += 1\n",
    "                \n",
    "                # Actualizar estad√≠sticas de intermediarios\n",
    "                for path in res['significant_paths']:\n",
    "                    for idx, interm in enumerate(path['intermediarios']):\n",
    "                        # Incrementar frecuencia\n",
    "                        results['intermediary_frequency'][interm] += 1\n",
    "                        \n",
    "                        # Incrementar fuerza ponderada\n",
    "                        weight_factor = 1.0 / (idx + 1)\n",
    "                        results['intermediary_strength'][interm] += path['fuerza'] * weight_factor\n",
    "        \n",
    "        # Guardar dependencia promedio para este importador\n",
    "        results['avg_dependencies'][importer] = total_dep / num_deps if num_deps > 0 else 0\n",
    "    \n",
    "    # A√±adir y ordenar los critical paths (igual que el original)\n",
    "    results['critical_paths'] = all_critical_paths\n",
    "    \n",
    "    # Ordenar top dependencies\n",
    "    results['top_dependencies'].sort(key=lambda x: x[4], reverse=True)\n",
    "    results['top_dependencies'] = results['top_dependencies'][:90]\n",
    "    \n",
    "    # Ordenar critical_paths por fuerza\n",
    "    results['critical_paths'].sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    \n",
    "    # Calcular m√©tricas de centralidad para intermediarios\n",
    "    results['intermediary_centrality'] = calculate_intermediary_centrality(\n",
    "        results['intermediary_frequency'], \n",
    "        results['intermediary_strength'],\n",
    "        country_names\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Para mantener compatibilidad, redefinimos la funci√≥n original\n",
    "# para que utilice la versi√≥n paralelizada\n",
    "def calculate_all_dependencies(X, country_names=None, convergence_threshold=0.01, \n",
    "                              max_possible_length=3, threshold_pct=0.01, \n",
    "                              path_strength_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Calcula todas las dependencias entre pa√≠ses con an√°lisis de intermediarios cr√≠ticos.\n",
    "    \n",
    "    Esta funci√≥n mantiene EXACTAMENTE la misma firma y resultados que la original,\n",
    "    pero utiliza internamente paralelizaci√≥n y GPU para acelerar los c√°lculos.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Matriz de comercio\n",
    "    country_names : list, opcional\n",
    "        Nombres de los pa√≠ses\n",
    "    convergence_threshold : float, default=0.01\n",
    "        Umbral para determinar la convergencia\n",
    "    max_possible_length : int, default=5\n",
    "        Longitud m√°xima de caminos a considerar\n",
    "    threshold_pct : float, default=0.01\n",
    "        Umbral para filtrar valores de comercio insignificantes (porcentaje)\n",
    "    path_strength_threshold : float, default=0.001\n",
    "        Umbral m√≠nimo para considerar una ruta como significativa\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Diccionario con todos los resultados del an√°lisis\n",
    "    \"\"\"\n",
    "    # Determinar si usar paralelizaci√≥n basado en el tama√±o del problema\n",
    "    use_parallel = X.shape[0] > 5  # Para matrices muy peque√±as no vale la pena paralelizar\n",
    "    \n",
    "    # Verificar disponibilidad de GPU\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    # Aqu√≠ agregas el segundo log\n",
    "    #print(f\"Usando paralelizaci√≥n: {use_parallel}, GPU disponible: {use_gpu}\")\n",
    "    #if use_gpu:\n",
    "    #    print(f\"GPU en uso: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Configurar n√∫mero de trabajos para paralelizaci√≥n\n",
    "    n_countries = X.shape[0]\n",
    "    n_jobs = min(multiprocessing.cpu_count(), n_countries)  # Limitar al n√∫mero de pa√≠ses\n",
    "    \n",
    "    \n",
    "    if use_parallel:\n",
    "        # Usar la versi√≥n paralelizada\n",
    "        return calculate_all_dependencies_parallel(\n",
    "            X, country_names, convergence_threshold, \n",
    "            max_possible_length, path_strength_threshold,\n",
    "            n_jobs=n_jobs, use_gpu=use_gpu, debug_mode=False\n",
    "        )\n",
    "    else:\n",
    "        # Para matrices muy peque√±as, usar un solo proceso\n",
    "        return calculate_all_dependencies_parallel(\n",
    "            X, country_names, convergence_threshold, \n",
    "            max_possible_length, path_strength_threshold,\n",
    "            n_jobs=1, use_gpu=use_gpu, debug_mode=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar y guardar resultados de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completadas: 31/168\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m country_names \u001b[38;5;241m=\u001b[39m mat_clean\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calcular dependencias\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m results \u001b[38;5;241m=\u001b[39m calculate_all_dependencies(X, country_names)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Guardar resultados\u001b[39;00m\n\u001b[0;32m     26\u001b[0m all_results[industry] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m: results,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_names\u001b[39m\u001b[38;5;124m'\u001b[39m: country_names,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatrix_shape\u001b[39m\u001b[38;5;124m'\u001b[39m: mat_clean\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     30\u001b[0m }\n",
      "Cell \u001b[1;32mIn[19], line 397\u001b[0m, in \u001b[0;36mcalculate_all_dependencies\u001b[1;34m(X, country_names, convergence_threshold, max_possible_length, threshold_pct, path_strength_threshold)\u001b[0m\n\u001b[0;32m    392\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count(), n_countries)  \u001b[38;5;66;03m# Limitar al n√∫mero de pa√≠ses\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_parallel:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# Usar la versi√≥n paralelizada\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m calculate_all_dependencies_parallel(\n\u001b[0;32m    398\u001b[0m         X, country_names, convergence_threshold, \n\u001b[0;32m    399\u001b[0m         max_possible_length, path_strength_threshold,\n\u001b[0;32m    400\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, use_gpu\u001b[38;5;241m=\u001b[39muse_gpu, debug_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# Para matrices muy peque√±as, usar un solo proceso\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m calculate_all_dependencies_parallel(\n\u001b[0;32m    405\u001b[0m         X, country_names, convergence_threshold, \n\u001b[0;32m    406\u001b[0m         max_possible_length, path_strength_threshold,\n\u001b[0;32m    407\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, use_gpu\u001b[38;5;241m=\u001b[39muse_gpu, debug_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[19], line 232\u001b[0m, in \u001b[0;36mcalculate_all_dependencies_parallel\u001b[1;34m(X, country_names, convergence_threshold, max_possible_length, path_strength_threshold, n_jobs, use_gpu, debug_mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m# Evitar divisi√≥n por cero\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m denom_gpu[i] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 232\u001b[0m         direct_deps[:, i] \u001b[38;5;241m=\u001b[39m X_gpu[:, i] \u001b[38;5;241m/\u001b[39m denom_gpu[i]\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Transferir resultados de vuelta a CPU\u001b[39;00m\n\u001b[0;32m    235\u001b[0m direct_dependencies \u001b[38;5;241m=\u001b[39m direct_deps\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Diccionario para guardar todos los resultados\n",
    "all_results = {}\n",
    "\n",
    "total_industrias = len(matrices_comercio)\n",
    "completadas = 0\n",
    "print(f\"Completadas: {completadas}/{total_industrias}\", end=\"\\r\", flush=True)\n",
    "\n",
    "# Procesar cada industria\n",
    "for industry, mat in matrices_comercio.items():\n",
    "    # Limpiar matriz\n",
    "    mat_clean = eliminar_filas_columnas_cero(mat)\n",
    "    \n",
    "    # Si la matriz limpia est√° vac√≠a o es muy peque√±a, continuamos con la siguiente\n",
    "    if mat_clean.shape[0] < 2:\n",
    "        # no cuenta como completada (no se calculan dependencias)\n",
    "        continue\n",
    "        \n",
    "    # Convertir a numpy array y obtener nombres de pa√≠ses\n",
    "    X = mat_clean.values\n",
    "    country_names = mat_clean.columns.tolist()\n",
    "\n",
    "    # Calcular dependencias\n",
    "    results = calculate_all_dependencies(X, country_names)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    all_results[industry] = {\n",
    "        'results': results,\n",
    "        'country_names': country_names,\n",
    "        'matrix_shape': mat_clean.shape\n",
    "    }\n",
    "\n",
    "    # Actualizar contador y mostrarlo en la misma l√≠nea\n",
    "    completadas += 1\n",
    "    print(f\"Completadas: {completadas}/{total_industrias}\", end=\"\\r\", flush=True)\n",
    "\n",
    "# Imprimir l√≠nea final (nuevo salto) al terminar\n",
    "print(f\"\\nCompletadas: {completadas}/{total_industrias} ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar y guadar datos de intermediarios Cr√≠ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_intermediaries(results, top_n=10):\n",
    "    \"\"\"\n",
    "    Obtiene los pa√≠ses m√°s importantes como intermediarios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del an√°lisis con intermediarios\n",
    "    top_n : int\n",
    "        N√∫mero de pa√≠ses a mostrar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Lista de tuplas (pa√≠s, score, frecuencia, fuerza) ordenadas por importancia\n",
    "    \"\"\"\n",
    "    # La estructura de intermediary_centrality ha cambiado en la nueva implementaci√≥n\n",
    "    # Ahora es una lista de tuplas (pa√≠s, frecuencia, fuerza, score_combinado)\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementaci√≥n: ya est√° ordenada por score combinado\n",
    "        return results['intermediary_centrality'][:top_n]\n",
    "    else:\n",
    "        # Implementaci√≥n anterior (por compatibilidad)\n",
    "        centrality_scores = [(country, metrics) \n",
    "                            for country, metrics in results['intermediary_centrality'].items()]\n",
    "        centrality_scores.sort(key=lambda x: x[1]['centrality_score'], reverse=True)\n",
    "        return centrality_scores[:top_n]\n",
    "\n",
    "def analyze_country_intermediary_role(results, country):\n",
    "    \"\"\"\n",
    "    Analiza el papel de un pa√≠s espec√≠fico como intermediario.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del an√°lisis con intermediarios\n",
    "    country : str\n",
    "        Pa√≠s a analizar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Informaci√≥n detallada sobre el rol del pa√≠s como intermediario\n",
    "    \"\"\"\n",
    "    # Verificar si el pa√≠s existe en los resultados\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementaci√≥n - lista de tuplas\n",
    "        country_entry = next((entry for entry in results['intermediary_centrality'] \n",
    "                             if entry[0] == country), None)\n",
    "        \n",
    "        if country_entry is None:\n",
    "            return {\"error\": f\"El pa√≠s {country} no est√° en los resultados\"}\n",
    "            \n",
    "        # Extraer m√©tricas\n",
    "        centrality = {\n",
    "            \"frequency\": country_entry[1],\n",
    "            \"strength\": country_entry[2],\n",
    "            \"centrality_score\": country_entry[3]\n",
    "        }\n",
    "    else:\n",
    "        # Implementaci√≥n anterior\n",
    "        if country not in results['intermediary_centrality']:\n",
    "            return {\"error\": f\"El pa√≠s {country} no est√° en los resultados\"}\n",
    "        centrality = results['intermediary_centrality'][country]\n",
    "    \n",
    "    # Encontrar las rutas m√°s importantes donde este pa√≠s act√∫a como intermediario\n",
    "    top_paths = []\n",
    "    for path in results['critical_paths']:\n",
    "        if country in path['intermediarios']:\n",
    "            top_paths.append(path)\n",
    "    \n",
    "    # Ordenar por fuerza del camino\n",
    "    top_paths.sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    \n",
    "    # Filtrar los 10 caminos m√°s importantes\n",
    "    top_paths = top_paths[:10]\n",
    "    \n",
    "    return {\n",
    "        \"metrics\": centrality,\n",
    "        \"top_paths\": top_paths,\n",
    "        \"role_summary\": {\n",
    "            \"total_paths\": len(top_paths),\n",
    "            \"average_path_strength\": sum(p['fuerza'] for p in top_paths) / len(top_paths) if top_paths else 0,\n",
    "            \"max_path_strength\": max(p['fuerza'] for p in top_paths) if top_paths else 0,\n",
    "            \"unique_exporters\": len(set(p['exportador'] for p in top_paths)),\n",
    "            \"unique_importers\": len(set(p['importador'] for p in top_paths))\n",
    "        }\n",
    "    }\n",
    "\n",
    "def summarize_country_dependencies(results, country, top_n=5):\n",
    "    \"\"\"\n",
    "    Genera un resumen de las dependencias comerciales de un pa√≠s espec√≠fico.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del an√°lisis de dependencias\n",
    "    country : str\n",
    "        Pa√≠s a analizar\n",
    "    top_n : int\n",
    "        N√∫mero de dependencias principales a mostrar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Resumen de dependencias del pa√≠s\n",
    "    \"\"\"\n",
    "    # Dependencias como importador (otros pa√≠ses exportan a este pa√≠s)\n",
    "    import_dependencies = []\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['importador'] == country:\n",
    "            import_dependencies.append(dep)\n",
    "    \n",
    "    # Ordenar por dependencia total\n",
    "    import_dependencies.sort(key=lambda x: x['dependencia_total'], reverse=True)\n",
    "    \n",
    "    # Dependencias como exportador (este pa√≠s exporta a otros)\n",
    "    export_dependencies = []\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['exportador'] == country:\n",
    "            export_dependencies.append(dep)\n",
    "    \n",
    "    # Ordenar por dependencia total\n",
    "    export_dependencies.sort(key=lambda x: x['dependencia_total'], reverse=True)\n",
    "    \n",
    "    # Calcular dependencia promedio\n",
    "    avg_dependency = results['avg_dependencies'].get(country, 0)\n",
    "    \n",
    "    # Analizar el papel como intermediario\n",
    "    intermediary_role = None\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementaci√≥n\n",
    "        intermediary_info = next((x for x in results['intermediary_centrality'] if x[0] == country), None)\n",
    "        if intermediary_info:\n",
    "            intermediary_role = {\n",
    "                \"frequency\": intermediary_info[1],\n",
    "                \"strength\": intermediary_info[2],\n",
    "                \"centrality_score\": intermediary_info[3],\n",
    "                \"rank\": next((i+1 for i, x in enumerate(results['intermediary_centrality']) \n",
    "                             if x[0] == country), None)\n",
    "            }\n",
    "    else:\n",
    "        # Implementaci√≥n anterior\n",
    "        if country in results['intermediary_centrality']:\n",
    "            intermediary_role = results['intermediary_centrality'][country]\n",
    "            # Calcular rango\n",
    "            countries_sorted = sorted(results['intermediary_centrality'].keys(), \n",
    "                                     key=lambda x: results['intermediary_centrality'][x]['centrality_score'],\n",
    "                                     reverse=True)\n",
    "            intermediary_role[\"rank\"] = countries_sorted.index(country) + 1\n",
    "    \n",
    "    return {\n",
    "        \"country\": country,\n",
    "        \"avg_dependency\": avg_dependency,\n",
    "        \"top_import_dependencies\": import_dependencies[:top_n],\n",
    "        \"top_export_dependencies\": export_dependencies[:top_n],\n",
    "        \"total_import_dependencies\": len(import_dependencies),\n",
    "        \"total_export_dependencies\": len(export_dependencies),\n",
    "        \"intermediary_role\": intermediary_role\n",
    "    }\n",
    "\n",
    "def identify_critical_trade_relationships(results, threshold=0.7, min_paths=3):\n",
    "    \"\"\"\n",
    "    Identifica relaciones comerciales cr√≠ticas con alta dependencia.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del an√°lisis de dependencias\n",
    "    threshold : float\n",
    "        Umbral de dependencia para considerar una relaci√≥n como cr√≠tica\n",
    "    min_paths : int\n",
    "        N√∫mero m√≠nimo de caminos alternativos para evitar criticidad\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Lista de relaciones cr√≠ticas\n",
    "    \"\"\"\n",
    "    critical_relationships = []\n",
    "    \n",
    "    # Analizar cada dependencia\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['dependencia_total'] >= threshold:\n",
    "            # Buscar caminos alternativos\n",
    "            pair_key = f\"{dep['exportador']}->{dep['importador']}\"\n",
    "            alternative_paths = []\n",
    "            \n",
    "            if pair_key in results['critical_intermediaries']:\n",
    "                alternative_paths = results['critical_intermediaries'][pair_key]\n",
    "            \n",
    "            # Si hay pocos caminos alternativos, es una relaci√≥n cr√≠tica\n",
    "            if len(alternative_paths) < min_paths:\n",
    "                critical_relationships.append({\n",
    "                    \"exportador\": dep['exportador'],\n",
    "                    \"importador\": dep['importador'],\n",
    "                    \"dependencia_total\": dep['dependencia_total'],\n",
    "                    \"dependencia_directa\": dep['dependencia_directa'],\n",
    "                    \"caminos_alternativos\": len(alternative_paths),\n",
    "                    \"criticidad\": 1.0 - (len(alternative_paths) / min_paths) \n",
    "                                     if min_paths > 0 else 1.0\n",
    "                })\n",
    "    \n",
    "    # Ordenar por criticidad\n",
    "    critical_relationships.sort(key=lambda x: x['criticidad'], reverse=True)\n",
    "    \n",
    "    return critical_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm = get_top_intermediaries(all_results['Aircraft and spacecraft']['results'])\n",
    "role = analyze_country_intermediary_role(results, 'DEU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo Dataframes para guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado correctamente en: c:\\Users\\Usuario\\Documents\\Github\\Seguridad Economica\\data\\processed\\dependencias_consolidadas\\dependencias2001.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dependencies_dataframe(all_results):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con los resultados de dependencias para todas las industrias.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_results : dict\n",
    "        Diccionario con los resultados por industria\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame con las columnas: industria, importador, exportador, \n",
    "        dependencia_total, dependencia_directa, dependencia_indirecta, longitud_optima\n",
    "    \"\"\"\n",
    "    # Lista para almacenar los datos de todas las industrias\n",
    "    all_data = []\n",
    "    \n",
    "    # Procesar cada industria\n",
    "    for industry, data in all_results.items():\n",
    "        # Obtener los resultados de esta industria\n",
    "        industry_results = data['results']['dependencies']\n",
    "        \n",
    "        # A√±adir cada fila de resultados\n",
    "        for result in industry_results:\n",
    "            row = {\n",
    "                'industria': industry,\n",
    "                'importador': result['importador'],\n",
    "                'exportador': result['exportador'],\n",
    "                'dependencia_total': result['dependencia_total'],\n",
    "                'dependencia_directa': result['dependencia_directa'],\n",
    "                'dependencia_indirecta': result['dependencia_indirecta'],\n",
    "                'trade_value': result['trade_value'],\n",
    "                'longitud_optima': result['longitud_optima']\n",
    "            }\n",
    "            all_data.append(row)\n",
    "    \n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Ordenar el DataFrame\n",
    "    df = df.sort_values(['industria', 'dependencia_total'], ascending=[True, False])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso:\n",
    "df = create_dependencies_dataframe(all_results)\n",
    "\n",
    "# Definir el nuevo mapeo de nombres de columnas\n",
    "nuevo_nombres = {\n",
    "    'industria': 'industry',\n",
    "    'importador': 'dependent_country',\n",
    "    'exportador': 'supplier_country',\n",
    "    'dependencia_total': 'dependency_value',\n",
    "    'dependencia_directa': 'direct_dependency',\n",
    "    'dependencia_indirecta': 'indirect_dependency',\n",
    "    'trade_value': 'trade_value',\n",
    "    'longitud_optima': 'longitud_optima'\n",
    "}\n",
    "\n",
    "# Renombrar las columnas\n",
    "df = df.rename(columns=nuevo_nombres)\n",
    "\n",
    "# Rutas (evitamos espacios en nombres de carpetas)\n",
    "base_path = Path.cwd().parent.parent\n",
    "target_directory = base_path / \"data\" / \"processed\" / \"dependencias_consolidadas\"\n",
    "\n",
    "ruta_archivo = target_directory / f\"dependencias{anio}.csv.gz\"\n",
    "\n",
    "\n",
    "# Guardar como gzip\n",
    "with gzip.open(ruta_archivo, 'wt', encoding='utf-8') as f:\n",
    "    df.to_csv(f, sep=\";\", index=False)\n",
    "\n",
    "print(f\"DataFrame guardado correctamente en: {ruta_archivo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lo que propone CHATGPT para hacer la seccion 6 del paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = Path.cwd().parent.parent\n",
    "output_dir = current_dir / \"data\" / \"processed\" / \"ficheros_paper\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 1. Dependencias completas (industria-pa√≠s-par)\n",
    "df = create_dependencies_dataframe(all_results)\n",
    "\n",
    "# Renombrado (ya implementado antes)\n",
    "df = df.rename(columns={\n",
    "    'industria': 'industry',\n",
    "    'importador': 'dependent_country',\n",
    "    'exportador': 'supplier_country',\n",
    "    'dependencia_total': 'dependency_value',\n",
    "    'dependencia_directa': 'direct_dependency',\n",
    "    'dependencia_indirecta': 'indirect_dependency',\n",
    "    'trade_value': 'trade_value',\n",
    "    'longitud_optima': 'optimal_length'\n",
    "})\n",
    "\n",
    "# Guardar\n",
    "df.to_csv(os.path.join(output_dir, \"dependencies_full.csv.gz\"), sep=\";\", index=False, compression=\"gzip\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 2. Dependencias ponderadas bilateralmente\n",
    "def safe_weighted_average(group):\n",
    "    if group['trade_value'].sum() == 0:\n",
    "        return np.nan\n",
    "    return np.average(group['dependency_value'], weights=group['trade_value'])\n",
    "\n",
    "weighted_dependencies = df.groupby(['dependent_country', 'supplier_country']).apply(\n",
    "    safe_weighted_average\n",
    ").reset_index(name='weighted_dependency')\n",
    "\n",
    "weighted_dependencies.dropna().to_csv(\n",
    "    os.path.join(output_dir, \"weighted_dependencies.csv.gz\"), sep=\";\", index=False, compression=\"gzip\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 3. Relaciones cr√≠ticas\n",
    "critical_relationships = []\n",
    "\n",
    "for industry, data in all_results.items():\n",
    "    crits = identify_critical_trade_relationships(data['results'], threshold=0.7, min_paths=3)\n",
    "    for c in crits:\n",
    "        c['industry'] = industry\n",
    "    critical_relationships.extend(crits)\n",
    "\n",
    "df_critical = pd.DataFrame(critical_relationships)\n",
    "df_critical.to_csv(os.path.join(output_dir, \"critical_relations.csv.gz\"), sep=\";\", index=False, compression=\"gzip\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 4. Intermediarios por industria\n",
    "intermediary_centrality_all = []\n",
    "\n",
    "for industry, data in all_results.items():\n",
    "    for c in data['results']['intermediary_centrality']:\n",
    "        intermediary_centrality_all.append({\n",
    "            'industry': industry,\n",
    "            'country': c[0],\n",
    "            'frequency': c[1],\n",
    "            'strength': c[2],\n",
    "            'centrality_score': c[3]\n",
    "        })\n",
    "\n",
    "df_centrality = pd.DataFrame(intermediary_centrality_all)\n",
    "df_centrality.to_csv(os.path.join(output_dir, \"intermediary_roles.csv.gz\"), sep=\";\", index=False, compression=\"gzip\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 5. Centralidad global agregada\n",
    "global_centrality = df_centrality.groupby(\"country\")[[\"frequency\", \"strength\", \"centrality_score\"]].sum()\n",
    "global_centrality[\"centrality_rank\"] = global_centrality[\"centrality_score\"].rank(ascending=False)\n",
    "global_centrality = global_centrality.sort_values(\"centrality_score\", ascending=False)\n",
    "global_centrality.reset_index().to_csv(\n",
    "    os.path.join(output_dir, \"intermediary_summary.csv.gz\"), sep=\";\", index=False, compression=\"gzip\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Todos los archivos CSV han sido generados en:\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secci√≥n 6.3 - Agregados de dependencia ponderada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Promedio ponderado de dependencia por industria\n",
    "industry_avg = df.groupby(\"industry\").apply(safe_weighted_average).reset_index(name=\"avg_weighted_dependency\")\n",
    "\n",
    "# 2. Top 10 dependencias ponderadas bilaterales\n",
    "top_10_bilateral = weighted_dependencies.sort_values(\"weighted_dependency\", ascending=False).head(10)\n",
    "\n",
    "# 3. Clasificaci√≥n por tramos de dependencia\n",
    "df[\"dependency_level\"] = pd.cut(\n",
    "    df[\"dependency_value\"],\n",
    "    bins=[0, 0.3, 0.7, 0.9, 1],\n",
    "    labels=[\"baja\", \"media\", \"alta\", \"cr√≠tica\"]\n",
    ")\n",
    "\n",
    "dependency_levels_summary = df[\"dependency_level\"].value_counts().reset_index(name=\"num_relaciones\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secci√≥n 6.4 - Relaciones cr√≠ticas con baja redundancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asumiendo que 'all_results' est√° disponible (output del c√°lculo de dependencias por industria)\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1. Extraer relaciones cr√≠ticas por industria\n",
    "critical_relations_all = []\n",
    "\n",
    "for industry, data in all_results.items():\n",
    "    crits = identify_critical_trade_relationships(data['results'], threshold=0.7, min_paths=3)\n",
    "    for c in crits:\n",
    "        c[\"industry\"] = industry\n",
    "    critical_relations_all.extend(crits)\n",
    "\n",
    "# 2. Crear dataframe consolidado\n",
    "df_critical = pd.DataFrame(critical_relations_all)\n",
    "\n",
    "# 3. Ranking de pa√≠ses m√°s afectados como importadores\n",
    "importadores_riesgo = df_critical.groupby(\"importador\").size().reset_index(name=\"relaciones_criticas\")\n",
    "\n",
    "# 4. Ranking como exportadores\n",
    "exportadores_riesgo = df_critical.groupby(\"exportador\").size().reset_index(name=\"exportaciones_criticas\")\n",
    "\n",
    "# 5. Exportar si se desea\n",
    "# df_critical.to_csv(\"relaciones_criticas.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secci√≥n 6.5 - Intermediarios cr√≠ticos y centralidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Consolidar centralidad de intermediarios para todas las industrias\n",
    "all_centrality = []\n",
    "\n",
    "for industry, data in all_results.items():\n",
    "    cent = data['results']['intermediary_centrality']\n",
    "    for entry in cent:\n",
    "        all_centrality.append({\n",
    "            \"industry\": industry,\n",
    "            \"country\": entry[0],\n",
    "            \"frequency\": entry[1],\n",
    "            \"strength\": entry[2],\n",
    "            \"centrality_score\": entry[3]\n",
    "        })\n",
    "\n",
    "df_centrality = pd.DataFrame(all_centrality)\n",
    "\n",
    "# 2. Top 10 intermediarios globales (acumulando por pa√≠s)\n",
    "global_centrality = df_centrality.groupby(\"country\")[[\"frequency\", \"strength\", \"centrality_score\"]].sum()\n",
    "global_centrality[\"centrality_rank\"] = global_centrality[\"centrality_score\"].rank(ascending=False)\n",
    "global_centrality = global_centrality.sort_values(\"centrality_score\", ascending=False)\n",
    "\n",
    "# 3. Exportar si se desea\n",
    "# df_centrality.to_csv(\"centralidad_intermediarios_por_industria.csv\", index=False)\n",
    "# global_centrality.to_csv(\"intermediarios_globales.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizaciones sugeridas (iniciales en matplotlib/seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Heatmap: dependencia promedio por industria y pa√≠s\n",
    "pivot_df = df.pivot_table(index=\"dependent_country\", columns=\"industry\", values=\"dependency_value\", aggfunc=\"mean\")\n",
    "plt.figure(figsize=(18, 10))\n",
    "sns.heatmap(pivot_df, cmap=\"YlGnBu\", center=0.5)\n",
    "plt.title(\"Dependencia media por pa√≠s e industria\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Burbujas: Intermediarios\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=global_centrality.reset_index(),\n",
    "    x=\"frequency\", y=\"strength\", size=\"centrality_score\", hue=\"centrality_score\", sizes=(100, 1000), palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Intermediarios clave seg√∫n frecuencia y fuerza\")\n",
    "plt.xlabel(\"Frecuencia como intermediario\")\n",
    "plt.ylabel(\"Fuerza acumulada\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dispersi√≥n: criticidad vs. caminos alternativos\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df_critical, x=\"caminos_alternativos\", y=\"dependencia_total\", hue=\"criticidad\", palette=\"coolwarm\")\n",
    "plt.title(\"Relaciones cr√≠ticas: dependencia vs. redundancia\")\n",
    "plt.xlabel(\"N√∫mero de caminos alternativos\")\n",
    "plt.ylabel(\"Dependencia total\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dependencias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
