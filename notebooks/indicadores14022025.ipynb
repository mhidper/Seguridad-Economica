{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 align=\"center\"><strong>Cálculo de dependencia cruzada para todos los países</strong></h1>\n",
    "<h4 align=\"center\"><strong>Manuel Alejandro Hidalgo y Jorge Díaz Lanchas</strong></h4>\n",
    "<h4 align=\"center\"><strong>Fundación Real Instituto Elcano</strong></h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esquema para Índice de Seguridad Económica - Real Instituto Elcano\n",
    "\n",
    "## 1. Introducción y Marco Conceptual\n",
    "\n",
    "- **Objetivo**: Desarrollar un índice que cuantifique la seguridad económica de los países\n",
    "- **Definición**: La seguridad económica como la capacidad de un país para resistir disrupciones en sus cadenas de suministro y comercio internacional\n",
    "- **Relevancia**: Contexto actual de fragmentación geoeconómica y tensiones comerciales\n",
    "\n",
    "## 2. Metodología\n",
    "\n",
    "### 2.1 Fuentes de Datos\n",
    "- Base de datos International Trade and Production Database (ITP)\n",
    "- Datos comerciales bilaterales por industria (año 2019)\n",
    "- Otros indicadores macroeconómicos complementarios\n",
    "\n",
    "### 2.2 Procesamiento de Datos\n",
    "```python\n",
    "# Procesamiento y carga de datos ITP\n",
    "itp2019, codigos_countries = procesar_datos_itp()\n",
    "```\n",
    "\n",
    "### 2.3 Creación de Matrices de Comercio\n",
    "```python\n",
    "# Generación de matrices bilaterales por industria\n",
    "matrices_comercio = crear_matriz_comercio(data.groupby('industry_descr'), codigos_paises)\n",
    "```\n",
    "\n",
    "### 2.4 Limpieza y Filtrado\n",
    "```python\n",
    "# Eliminar relaciones comerciales insignificantes\n",
    "mat_clean = eliminar_filas_columnas_cero(mat, threshold_pct=0.05)\n",
    "```\n",
    "\n",
    "### 2.5 Cálculo de Dependencias Económicas\n",
    "```python\n",
    "# Cálculo de dependencias directas e indirectas\n",
    "results = analyze_dependencies(X, country_names)\n",
    "```\n",
    "\n",
    "## 3. Componentes del Índice\n",
    "\n",
    "### 3.1 Dependencia Directa\n",
    "- Medición de la dependencia inmediata entre países\n",
    "- Fórmula: $D_{ij} = \\frac{X_{ji}}{∑_k X_{ki}}$, donde $X_{ji}$ es el comercio del país j al país i\n",
    "\n",
    "### 3.2 Dependencia Indirecta\n",
    "- Medición de dependencias a través de cadenas de suministro\n",
    "- Incorporación de países intermediarios en las relaciones comerciales\n",
    "- Análisis de caminos hasta longitud 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import torch\n",
    "import dask.dataframe as dd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***No ejecutar este código a menos que se quiera comprimir***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error durante el proceso: No se encuentra el archivo: Datos\\ITP\\ITPD_E_R02.csv\n"
     ]
    }
   ],
   "source": [
    "def comprimir_dividir_archivo(archivo_original, tamano_maximo=100, directorio_salida=None):\n",
    "    # Asegúrate de que el archivo original existe\n",
    "    archivo_original = Path(archivo_original)\n",
    "    if not archivo_original.exists():\n",
    "        raise FileNotFoundError(f\"No se encuentra el archivo: {archivo_original}\")\n",
    "    \n",
    "    # Si no se especifica directorio de salida, usar src/data/raw/ITP/\n",
    "    if directorio_salida is None:\n",
    "        # Obtener el directorio raíz del proyecto (donde está src/)\n",
    "        proyecto_root = Path(__file__).parent.parent.parent\n",
    "        directorio_salida = proyecto_root / 'src' / 'data' / 'raw' / 'ITP'\n",
    "    else:\n",
    "        directorio_salida = Path(directorio_salida)\n",
    "    \n",
    "    # Crear el directorio de salida si no existe\n",
    "    directorio_salida.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Abre el archivo original en modo de lectura binaria\n",
    "    with open(archivo_original, 'rb') as f_in:\n",
    "        # Lee el contenido del archivo original\n",
    "        contenido = f_in.read()\n",
    "        \n",
    "        # Determina el número de partes necesarias\n",
    "        num_partes = (len(contenido) + tamano_maximo - 1) // tamano_maximo\n",
    "        \n",
    "        # Divide el contenido en partes y escribe cada parte comprimida\n",
    "        for i in range(num_partes):\n",
    "            parte = contenido[i * tamano_maximo: (i + 1) * tamano_maximo]\n",
    "            archivo_salida = directorio_salida / f'ITPD_E_R02.csv.parte{i}.gz'\n",
    "            with gzip.open(archivo_salida, 'wb') as f_out:\n",
    "                f_out.write(parte)\n",
    "            print(f\"Parte {i} creada en: {archivo_salida}\")\n",
    "\n",
    "# Tamaño máximo por parte (1GB)\n",
    "tamano_maximo = 1000 * 1024 * 1024\n",
    "\n",
    "try:\n",
    "    # Ruta al archivo original\n",
    "    archivo_original = Path(r'Datos/ITP/ITPD_E_R02.csv')\n",
    "    \n",
    "    # Comprimir y dividir el archivo original\n",
    "    comprimir_dividir_archivo(archivo_original, tamano_maximo)\n",
    "    print(\"Proceso completado con éxito\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el proceso: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Descomprimir, carga de datos y borrado de archivo***\n",
    "\n",
    "La compresión se hace para poder trabajar con git sin porblemas de tamaño de ficheros.\n",
    "Se descomprime, se importa y luego se borra el fichero descomprimido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Directorio fuente: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\raw\\ITP\n",
      "Directorio destino: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\n",
      "Combinando archivos comprimidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|██████████| 7/7 [00:11<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo CSV...\n",
      "Usando Dask para procesamiento en paralelo\n",
      "Filtrando datos de 2019...\n",
      "Archivo temporal eliminado\n",
      "Total de países únicos encontrados: 237\n",
      "Procesamiento completado con éxito\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FASE 1: PREPARACIÓN Y CARGA DE DATOS\n",
    "Este script procesa la base de datos International Trade and Production Database (ITP)\n",
    "que viene dividida en múltiples archivos comprimidos, utilizando aceleración GPU\n",
    "cuando está disponible.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def procesar_datos_itp():\n",
    "   try:\n",
    "       # Verificar si GPU está disponible\n",
    "       gpu_disponible = torch.cuda.is_available()\n",
    "       if gpu_disponible:\n",
    "           print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "       else:\n",
    "           print(\"GPU no disponible, se usará CPU\")\n",
    "       \n",
    "       # Definición de rutas usando Path y la estructura de tu proyecto\n",
    "       try:\n",
    "           base_path = Path(__file__).parent.parent.parent\n",
    "       except NameError:  # Estamos en un notebook\n",
    "           base_path = Path.cwd().parent  # Asumiendo que el notebook está en /notebooks/\n",
    "\n",
    "       source_directory = base_path / \"src\" / \"data\" / \"raw\" / \"ITP\"\n",
    "       target_directory = base_path / \"src\" / \"data\" / \"processed\"\n",
    "       target_filename = 'ITPD_E_R02.csv'\n",
    "\n",
    "       # Imprimir las rutas para verificación\n",
    "       print(f\"Directorio fuente: {source_directory}\")\n",
    "       print(f\"Directorio destino: {target_directory}\")\n",
    "\n",
    "       # Asegurar que los directorios existen\n",
    "       target_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "       # Verificar que el directorio fuente existe\n",
    "       if not source_directory.exists():\n",
    "           raise FileNotFoundError(f\"No se encuentra el directorio fuente: {source_directory}\")\n",
    "\n",
    "       # Listar archivos comprimidos\n",
    "       chunk_filenames = sorted([f for f in os.listdir(source_directory) \n",
    "                        if f.startswith('ITPD_E_R02.csv.parte') and f.endswith('.gz')])\n",
    "\n",
    "       # Control de errores: verificar que existen archivos para procesar\n",
    "       if not chunk_filenames:\n",
    "           raise FileNotFoundError(f\"No se encontraron archivos .gz en {source_directory}\")\n",
    "\n",
    "       # Construir la ruta completa para el archivo combinado\n",
    "       target_filepath = target_directory / target_filename\n",
    "\n",
    "       # Función para descomprimir un archivo en paralelo\n",
    "       def descomprimir_archivo(chunk_filename):\n",
    "           chunk_filepath = source_directory / chunk_filename\n",
    "           with gzip.open(chunk_filepath, 'rb') as chunk_file:\n",
    "               return chunk_file.read()\n",
    "\n",
    "       print(\"Combinando archivos comprimidos...\")\n",
    "       with open(target_filepath, 'wb') as target_file:\n",
    "           # Usar ThreadPoolExecutor para paralelizar la descompresión\n",
    "           with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "               for data in tqdm(\n",
    "                   executor.map(descomprimir_archivo, chunk_filenames),\n",
    "                   total=len(chunk_filenames),\n",
    "                   desc=\"Procesando archivos\"\n",
    "               ):\n",
    "                   target_file.write(data)\n",
    "\n",
    "       print(\"Leyendo archivo CSV...\")\n",
    "       \n",
    "       # Usar Dask para procesamiento paralelo\n",
    "       print(\"Usando Dask para procesamiento en paralelo\")\n",
    "       dask_df = dd.read_csv(target_filepath, sep=\",\")\n",
    "       \n",
    "       # Filtrar año 2019 eficientemente\n",
    "       print(\"Filtrando datos de 2019...\")\n",
    "       itp2019 = dask_df[dask_df['year'] == 2019].compute()\n",
    "       \n",
    "       # Limpieza: eliminar archivo temporal\n",
    "       os.remove(target_filepath)\n",
    "       print(f\"Archivo temporal eliminado\")\n",
    "\n",
    "       # Obtener lista única de países importadores - usando GPU si disponible\n",
    "       if gpu_disponible:\n",
    "           # Transferir a GPU para operaciones de unique más rápidas\n",
    "           importer_tensor = torch.tensor(\n",
    "               pd.factorize(itp2019['importer_iso3'])[0], \n",
    "               device='cuda'\n",
    "           )\n",
    "           unique_indices = torch.unique(importer_tensor).cpu().numpy()\n",
    "           # Mapear índices únicos de vuelta a códigos ISO\n",
    "           factorize_result = pd.factorize(itp2019['importer_iso3'])\n",
    "           codigos_countries = [factorize_result[1][idx] for idx in unique_indices]\n",
    "       else:\n",
    "           codigos_countries = list(itp2019['importer_iso3'].unique())\n",
    "           \n",
    "       print(f\"Total de países únicos encontrados: {len(codigos_countries)}\")\n",
    "\n",
    "       return itp2019, codigos_countries\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"Error durante el procesamiento: {e}\")\n",
    "       raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data, countries = procesar_datos_itp()\n",
    "        print(\"Procesamiento completado con éxito\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la ejecución principal: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices de Comercio Internacional: Creación Optimizada con Aceleración GPU\n",
    "\n",
    "El siguiente código implementa una función optimizada para crear matrices de comercio bilateral para cada industria a partir de datos comerciales internacionales. Esta implementación utiliza técnicas avanzadas para mejorar significativamente el rendimiento:\n",
    "\n",
    "### Características principales:\n",
    "\n",
    "- **Aceleración por GPU**: Detecta automáticamente si hay una GPU disponible y la utiliza para acelerar el procesamiento cuando hay suficientes datos.\n",
    "- **Operaciones vectorizadas**: Minimiza las iteraciones fila por fila usando enfoques vectorizados más eficientes.\n",
    "- **Gestión de memoria optimizada**: Reutiliza estructuras para reducir la fragmentación y el consumo de memoria.\n",
    "- **Adaptabilidad**: Ajusta automáticamente la estrategia de procesamiento según el volumen de datos y el hardware disponible.\n",
    "\n",
    "El resultado es un diccionario donde cada clave representa una industria, y cada valor es una matriz completa de comercio bilateral entre todos los países del conjunto de datos, lo que facilita los análisis posteriores de dependencias comerciales y flujos económicos.\n",
    "\n",
    "Este enfoque es particularmente útil para el cálculo de índices de dependencia comercial, ya que proporciona una representación eficiente de los flujos comerciales entre países para cada sector industrial.\n",
    "\n",
    "### Limpieza de Matrices de Comercio: Eliminación de Flujos No Significativos\n",
    "\n",
    "El siguiente código implementa una función para eliminar del análisis aquellos países con flujos comerciales no significativos. Este paso es crucial para mejorar la precisión del análisis de dependencias comerciales, ya que permite enfocarse en relaciones económicas realmente relevantes.\n",
    "\n",
    "**Funcionamiento**:\n",
    "\n",
    "- **Umbral de significancia**: Establece un umbral mínimo (por defecto 0.05%) del comercio mundial total. Cualquier flujo comercial por debajo de este umbral se considera no significativo y se convierte a cero.\n",
    "- **Identificación de países no relevantes**: Detecta países que, después de aplicar el umbral, no tienen flujos comerciales significativos (tanto como exportadores como importadores).\n",
    "- **Limpieza de la matriz**: Elimina estos países de la matriz, reduciendo su dimensionalidad y concentrando el análisis en actores relevantes del comercio internacional.\n",
    "\n",
    "Esta función de preprocesamiento es especialmente importante para estudios de dependencia económica, ya que:\n",
    "1. Reduce el ruido en los datos al eliminar relaciones comerciales marginales\n",
    "2. Mejora la eficiencia computacional al trabajar con matrices más pequeñas\n",
    "3. Permite concentrarse en dependencias significativas que podrían representar vulnerabilidades reales\n",
    "\n",
    "El resultado es una matriz \"limpia\" que contiene únicamente los países y flujos comerciales relevantes para el análisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU para procesamiento\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando matrices de comercio: 100%|██████████| 170/170 [00:02<00:00, 74.47it/s]\n"
     ]
    }
   ],
   "source": [
    "def crear_matriz_comercio_optimizado(grouped_data, codigos_paises: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Crea matrices de comercio bilateral para cada industria a partir de datos agrupados,\n",
    "    aprovechando operaciones vectorizadas y aceleración por GPU cuando está disponible.\n",
    "\n",
    "    Args:\n",
    "        grouped_data: DataFrame agrupado por industria.\n",
    "        codigos_paises: Lista de códigos de países ISO3.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: Diccionario de matrices de comercio.\n",
    "    \"\"\"\n",
    "    matrices = {}\n",
    "    required_columns = {'exporter_iso3', 'importer_iso3', 'trade'}\n",
    "    \n",
    "    # Verificar columnas requeridas\n",
    "    if not required_columns.issubset(grouped_data.obj.columns):\n",
    "        raise ValueError(f\"Los datos deben contener las columnas: {required_columns}\")\n",
    "    \n",
    "    # Crear un mapeo de códigos de países a índices para operaciones más rápidas\n",
    "    pais_a_indice = {pais: i for i, pais in enumerate(codigos_paises)}\n",
    "    \n",
    "    # Verificar si hay GPU disponible\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_gpu else 'cpu')\n",
    "    print(f\"Usando {'GPU' if use_gpu else 'CPU'} para procesamiento\")\n",
    "    \n",
    "    # Crear una matriz vacía template para reutilizar\n",
    "    matriz_template = pd.DataFrame(\n",
    "        0.0,\n",
    "        index=codigos_paises,\n",
    "        columns=codigos_paises\n",
    "    )\n",
    "    \n",
    "    # Preprocesar datos para evitar iteraciones lentas\n",
    "    for industry, group in tqdm(grouped_data, desc=\"Creando matrices de comercio\"):\n",
    "        # Filtrar solo las filas con países válidos\n",
    "        valid_trades = group[\n",
    "            group['exporter_iso3'].isin(codigos_paises) & \n",
    "            group['importer_iso3'].isin(codigos_paises)\n",
    "        ]\n",
    "        \n",
    "        if valid_trades.empty:\n",
    "            # Reutilizar la matriz template (hacer una copia para evitar modificar el original)\n",
    "            matrices[industry] = matriz_template.copy()\n",
    "            continue\n",
    "        \n",
    "        # Enfoque vectorizado para construir la matriz\n",
    "        if use_gpu and len(valid_trades) > 1000:  # Solo usar GPU si hay suficientes datos\n",
    "            # Convertir a índices numéricos para operaciones más rápidas\n",
    "            exporters = valid_trades['exporter_iso3'].map(pais_a_indice).values\n",
    "            importers = valid_trades['importer_iso3'].map(pais_a_indice).values\n",
    "            values = valid_trades['trade'].values\n",
    "            \n",
    "            # Crear tensor en GPU\n",
    "            matrix = torch.zeros((len(codigos_paises), len(codigos_paises)), device=device)\n",
    "            \n",
    "            # Usar scatter_add_ para construir la matriz eficientemente\n",
    "            indices = torch.tensor(np.vstack([exporters, importers]), device=device, dtype=torch.long)\n",
    "            matrix.index_put_(\n",
    "                indices=(torch.tensor(exporters, device=device, dtype=torch.long),\n",
    "                         torch.tensor(importers, device=device, dtype=torch.long)),\n",
    "                values=torch.tensor(values, device=device, dtype=torch.float),\n",
    "                accumulate=True\n",
    "            )\n",
    "            \n",
    "            # Convertir de vuelta a DataFrame\n",
    "            matrix_df = pd.DataFrame(\n",
    "                matrix.cpu().numpy(),\n",
    "                index=codigos_paises,\n",
    "                columns=codigos_paises\n",
    "            )\n",
    "        else:\n",
    "            # Crear una matriz completa de ceros y luego llenarla de una sola vez\n",
    "            # Empezamos con una copia de la matriz template\n",
    "            matrix_df = matriz_template.copy()\n",
    "            \n",
    "            # Preparar datos para actualización rápida\n",
    "            for _, row in valid_trades.iterrows():\n",
    "                exp = row['exporter_iso3']\n",
    "                imp = row['importer_iso3']\n",
    "                matrix_df.at[exp, imp] = row['trade']\n",
    "                \n",
    "        matrices[industry] = matrix_df\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "# Define la lista de códigos de países\n",
    "codigos_paises = sorted(data['importer_iso3'].unique().tolist())\n",
    "\n",
    "# Llama a la función optimizada\n",
    "matrices_comercio = crear_matriz_comercio_optimizado(data.groupby('industry_descr'), codigos_paises)\n",
    "\n",
    "def eliminar_filas_columnas_cero(df, threshold_pct=0.01):\n",
    "    \"\"\"Filtra una matriz de comercio aplicando un umbral relativo a cada país importador.\"\"\"\n",
    "    # Crear una copia para no modificar el original\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Para cada país importador (columna), aplicar su propio umbral\n",
    "    for country in df.columns:\n",
    "        # Calcular el total de importaciones del país\n",
    "        country_total_imports = df[country].sum()\n",
    "        \n",
    "        # Calcular el umbral específico para este país\n",
    "        country_threshold = country_total_imports * (threshold_pct)\n",
    "        \n",
    "        # Convertir a cero los valores por debajo del umbral solo para este país\n",
    "        df_filtered.loc[:, country] = df[country].where(df[country] >= country_threshold, 0)\n",
    "    \n",
    "    # Ahora identificar países con todos ceros\n",
    "    zero_rows = df_filtered.index[df_filtered.sum(axis=1) == 0]\n",
    "    zero_cols = df_filtered.columns[df_filtered.sum(axis=0) == 0]\n",
    "    countries_to_drop = list(set(zero_rows) & set(zero_cols))\n",
    "    \n",
    "    # Eliminar los países identificados\n",
    "    mat_clean = df_filtered.drop(countries_to_drop, axis=0).drop(countries_to_drop, axis=1)\n",
    "    \n",
    "    return mat_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo de dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `calculate_path_dependency` está diseñada para calcular la dependencia a lo largo de una cadena específica de nodos (países o entidades) en un análisis de flujos económicos.\n",
    "\n",
    "Paso a paso:\n",
    "\n",
    "1. **Entrada de la función**:\n",
    "   - `X_clean`: Una matriz que representa los flujos entre nodos (países). Cada valor `X_clean[i, j]` representa el flujo desde el nodo `i` al nodo `j`.\n",
    "   - `path`: Una lista que define un camino específico a través de varios nodos (países).\n",
    "   - `denominators`: Un array que contiene los valores de normalización para cada nodo.\n",
    "\n",
    "2. **Proceso**:\n",
    "   - Inicia calculando el flujo directo entre los dos primeros nodos del camino: `flujo_actual = X_clean[path[0], path[1]]`.\n",
    "   \n",
    "   - Luego itera a través de cada nodo intermedio en el camino para calcular cómo el flujo se propaga:\n",
    "     ```python\n",
    "     for i in range(1, len(path)-1):\n",
    "         nodo_actual = path[i]\n",
    "     ```\n",
    "\n",
    "   - Para cada nodo, verifica si puede haber flujo saliente (si el denominador es 0, no puede haber flujo):\n",
    "     ```python\n",
    "     if denominators[nodo_actual] == 0:\n",
    "         return 0\n",
    "     ```\n",
    "\n",
    "   - Calcula qué proporción del flujo que llega al nodo actual continúa hacia el siguiente nodo:\n",
    "     ```python\n",
    "     proporcion = X_clean[path[i], path[i+1]] / denominators[nodo_actual]\n",
    "     ```\n",
    "     Esta proporción representa la fracción del flujo total saliente del nodo actual que va específicamente al siguiente nodo.\n",
    "\n",
    "   - Multiplica el flujo actual por esta proporción para actualizar el valor:\n",
    "     ```python\n",
    "     flujo_actual = flujo_actual * proporcion\n",
    "     ```\n",
    "     Este paso acumula el efecto de cada transición a lo largo del camino.\n",
    "\n",
    "3. **Resultado**:\n",
    "   - Finalmente, normaliza el flujo resultante por el denominador del nodo final:\n",
    "     ```python\n",
    "     return flujo_actual / denominators[path[-1]] if denominators[path[-1]] > 0 else 0\n",
    "     ```\n",
    "     Esto garantiza que la dependencia se exprese como una proporción relativa al flujo total del nodo destino.\n",
    "\n",
    "En resumen, esta función calcula cuánto de un flujo inicial sobrevive a través de toda una cadena de nodos, considerando en cada paso qué proporción del flujo continúa al siguiente nodo. Es útil para analizar dependencias en cadenas de suministro, flujos comerciales o relaciones económicas donde los recursos o bienes fluyen a través de múltiples intermediarios.\n",
    "\n",
    "La función `calculate_path_dependency` puede expresarse formalmente usando la siguiente formulación en LaTeX:\n",
    "\n",
    "$$\\text{Dependencia}(p) = \\frac{f_{p_1, p_2}}{d_{p_n}} \\prod_{i=2}^{n-1} \\frac{f_{p_i, p_{i+1}}}{d_{p_i}}$$\n",
    "\n",
    "Donde:\n",
    "- $p = [p_1, p_2, \\ldots, p_n]$ es el camino (path) que consiste en $n$ nodos\n",
    "- $f_{i,j}$ representa el flujo desde el nodo $i$ al nodo $j$ (corresponde a `X_clean[i, j]`)\n",
    "- $d_i$ es el denominador para el nodo $i$ (el valor de normalización, típicamente la suma de todos los flujos salientes)\n",
    "\n",
    "Esta fórmula se puede interpretar de la siguiente manera:\n",
    "1. Comenzamos con el flujo inicial $f_{p_1, p_2}$ del primer nodo al segundo nodo\n",
    "2. Para cada nodo intermedio $p_i$ (donde $2 \\leq i \\leq n-1$), calculamos la proporción del flujo que continúa al siguiente nodo $p_{i+1}$, expresado como $\\frac{f_{p_i, p_{i+1}}}{d_{p_i}}$\n",
    "3. Multiplicamos todas estas proporciones junto con el flujo inicial\n",
    "4. Finalmente, normalizamos por el denominador del nodo final $d_{p_n}$\n",
    "\n",
    "Si cualquiera de los denominadores $d_{p_i}$ es cero, entonces la dependencia completa se define como cero, ya que no puede haber flujo a través de ese nodo.\n",
    "\n",
    "Formalmente, la condición completa se puede expresar como:\n",
    "\n",
    "$$\\text{Dependencia}(p) = \n",
    "\\begin{cases} \n",
    "0 & \\text{si } \\exists i \\in \\{2, 3, \\ldots, n\\} : d_{p_i} = 0 \\\\\n",
    "\\frac{f_{p_1, p_2}}{d_{p_n}} \\prod_{i=2}^{n-1} \\frac{f_{p_i, p_{i+1}}}{d_{p_i}} & \\text{en otro caso}\n",
    "\\end{cases}$$\n",
    "\n",
    "Esta formulación captura la naturaleza secuencial de la propagación del flujo a lo largo de todo el camino, considerando las proporciones de transición en cada etapa.\n",
    "\n",
    "## Ejemplo sencillo de dependencia de camino\n",
    "\n",
    "Imaginemos un sistema con 4 países (A, B, C, D) con los siguientes flujos comerciales entre ellos:\n",
    "\n",
    "- De A a B: 100 unidades\n",
    "- De B a C: 50 unidades\n",
    "- De C a D: 30 unidades\n",
    "- De B a otros destinos: 150 unidades (total saliente de B = 200)\n",
    "- De C a otros destinos: 70 unidades (total saliente de C = 100)\n",
    "- Flujo total que recibe D: 80 unidades\n",
    "\n",
    "Queremos calcular la dependencia a lo largo del camino p = [A, B, C, D].\n",
    "\n",
    "### Paso 1: Definimos nuestros datos\n",
    "- Matriz de flujos `X_clean`:\n",
    "  ```\n",
    "  X_clean = [\n",
    "    [0, 100, 0, 0],  # Flujos desde A\n",
    "    [0, 0, 50, 0],   # Flujos desde B\n",
    "    [0, 0, 0, 30],   # Flujos desde C\n",
    "    [0, 0, 0, 0]     # Flujos desde D\n",
    "  ]\n",
    "  ```\n",
    "\n",
    "- Denominadores (totales de flujos salientes/entrantes según corresponda):\n",
    "  ```\n",
    "  denominators = [100, 200, 100, 80]\n",
    "  ```\n",
    "\n",
    "- Camino a analizar:\n",
    "  ```\n",
    "  path = [0, 1, 2, 3]  # Corresponde a A→B→C→D\n",
    "  ```\n",
    "\n",
    "### Paso 2: Calculamos la dependencia\n",
    "\n",
    "1. Flujo inicial: flujo_actual = X_clean[0, 1] = 100\n",
    "\n",
    "2. Para el nodo B (índice 1):\n",
    "   - Proporción que va a C = X_clean[1, 2] / denominators[1] = 50/200 = 0.25\n",
    "   - flujo_actual = 100 * 0.25 = 25\n",
    "\n",
    "3. Para el nodo C (índice 2):\n",
    "   - Proporción que va a D = X_clean[2, 3] / denominators[2] = 30/100 = 0.3\n",
    "   - flujo_actual = 25 * 0.3 = 7.5\n",
    "\n",
    "4. Normalización final:\n",
    "   - Dependencia = flujo_actual / denominators[3] = 7.5/80 = 0.09375\n",
    "\n",
    "### Interpretación\n",
    "\n",
    "La dependencia de 0.09375 (o 9.375%) indica que este camino específico A→B→C→D representa aproximadamente el 9.4% del flujo total que recibe D.\n",
    "\n",
    "En términos prácticos, esto podría significar que:\n",
    "- El 9.4% de los bienes que llegan a D provienen originalmente de A, pasando por B y C.\n",
    "- Si hubiera una interrupción en cualquier punto de esta cadena, aproximadamente el 9.4% de las importaciones de D estarían en riesgo.\n",
    "\n",
    "Este cálculo captura la \"dilución\" del flujo original a medida que pasa por múltiples intermediarios, donde en cada paso solo una fracción continúa por el camino específico que estamos analizando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "def calculate_path_dependency(X_clean, path, denominators):\n",
    "    \"\"\"Calcula la dependencia de un camino específico.\"\"\"\n",
    "    fuerza_camino = 1.0\n",
    "    for k in range(len(path) - 1):\n",
    "        if denominators[path[k+1]] > 0:\n",
    "            fuerza_camino *= X_clean[path[k], path[k+1]] / denominators[path[k+1]]\n",
    "        else:\n",
    "            fuerza_camino = 0  # Si el denominador es cero, la fuerza del camino es cero\n",
    "    return fuerza_camino\n",
    "\n",
    "def process_country(i, X_clean, denominators, country_names, convergence_threshold, max_possible_length, n, progress=None):\n",
    "    \"\"\"Procesa un país individual.\"\"\"\n",
    "    country_results = {\n",
    "        'dependencies': [],\n",
    "        'top_dependencies': [],\n",
    "        'avg_dependency': None,\n",
    "        'length_distribution': np.zeros(max_possible_length),\n",
    "    }\n",
    "    \n",
    "    total_dep = 0.0\n",
    "    num_deps = 0\n",
    "    \n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            continue\n",
    "        \n",
    "        trade_value = X_clean[j, i]\n",
    "        direct_dependency = X_clean[j, i] / denominators[i] if denominators[i] > 0 else 0\n",
    "        dependencies_by_length = {1: direct_dependency}\n",
    "        current_total = direct_dependency\n",
    "        indirect_total = 0\n",
    "        \n",
    "        for length in range(2, max_possible_length + 1):\n",
    "            DI_ij_l = 0\n",
    "            middle_countries = [k for k in range(n) if k != i and k != j]\n",
    "            \n",
    "            for intermediaries in combinations(middle_countries, length - 1):\n",
    "                path = (j,) + intermediaries + (i,)\n",
    "                fuerza_camino = calculate_path_dependency(X_clean, path, denominators)\n",
    "                DI_ij_l += fuerza_camino\n",
    "            \n",
    "            dependencies_by_length[length] = DI_ij_l\n",
    "            indirect_total += DI_ij_l\n",
    "            \n",
    "            prev_total = current_total\n",
    "            current_total = direct_dependency + indirect_total\n",
    "            \n",
    "            if length > 1 and abs(current_total - prev_total) < convergence_threshold:\n",
    "                country_results['length_distribution'][length - 1] += 1\n",
    "                break\n",
    "        \n",
    "        total_dependency = direct_dependency + indirect_total\n",
    "        \n",
    "        result = {\n",
    "            'importador': country_names[i],\n",
    "            'exportador': country_names[j],\n",
    "            'trade_value': trade_value,\n",
    "            'dependencia_directa': direct_dependency,\n",
    "            'dependencia_indirecta': indirect_total,\n",
    "            'dependencia_total': total_dependency,\n",
    "            'longitud_optima': length,\n",
    "            'dependencias_por_longitud': dependencies_by_length\n",
    "        }\n",
    "        \n",
    "        country_results['dependencies'].append(result)\n",
    "        country_results['top_dependencies'].append(\n",
    "            (country_names[i], country_names[j], direct_dependency,\n",
    "             indirect_total, total_dependency, length))\n",
    "        total_dep += total_dependency\n",
    "        num_deps += 1\n",
    "    \n",
    "    country_results['avg_dependency'] = (country_names[i], total_dep / num_deps if num_deps > 0 else 0)\n",
    "    \n",
    "    # Actualiza la barra de progreso si se proporcionó una\n",
    "    if progress is not None:\n",
    "        progress.update(1)\n",
    "    \n",
    "    return country_results\n",
    "\n",
    "def calculate_all_dependencies(X, country_names=None, convergence_threshold=0.01, max_possible_length=5, threshold_pct=0.1):\n",
    "    \"\"\"Calcula todas las dependencias entre países con paralelización.\"\"\"\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    if country_names is None:\n",
    "        country_names = [f\"País {i}\" for i in range(n)]\n",
    "    \n",
    "    if len(country_names) != n:\n",
    "        raise ValueError(f\"La longitud de country_names ({len(country_names)}) no coincide con la dimensión de X ({n})\")\n",
    "    \n",
    "    total_trade = np.sum(X)\n",
    "    threshold = total_trade * (threshold_pct / 100)\n",
    "    X_clean = np.where(X < threshold, 0, X)\n",
    "    denominators = np.sum(X, axis=0)\n",
    "    \n",
    "    results = {\n",
    "        'dependencies': [],\n",
    "        'top_dependencies': [],\n",
    "        'avg_dependencies': {},\n",
    "        'length_distribution': np.zeros(max_possible_length),\n",
    "    }\n",
    "    \n",
    "    # Determinar el número de procesos a utilizar\n",
    "    num_cores = max(1, min(mp.cpu_count() - 1, 8))  # No más de 8 procesos, dejando un núcleo libre\n",
    "    print(f\"Utilizando {num_cores} núcleos para procesar {n} países\")\n",
    "    \n",
    "    # Preparar la lista de países para procesar\n",
    "    country_indices = list(range(n))\n",
    "    \n",
    "    # Configurar multiprocessing sin tqdm inicialmente\n",
    "    process_func = partial(\n",
    "        process_country,\n",
    "        X_clean=X_clean,\n",
    "        denominators=denominators,\n",
    "        country_names=country_names,\n",
    "        convergence_threshold=convergence_threshold,\n",
    "        max_possible_length=max_possible_length,\n",
    "        n=n,\n",
    "        progress=None  # Sin progreso dentro de los procesos\n",
    "    )\n",
    "    \n",
    "    # Crear un Pool sin tqdm para los procesos individuales\n",
    "    pool = mp.Pool(processes=num_cores)\n",
    "    \n",
    "    # Configurar tqdm para mostrar progreso general\n",
    "    with tqdm(total=n, desc=\"Calculando dependencias\") as progress_bar:\n",
    "        # Procesar países uno por uno y actualizar la barra de progreso\n",
    "        for i in country_indices:\n",
    "            # Procesar país\n",
    "            country_result = process_func(i)\n",
    "            \n",
    "            # Actualizar resultados\n",
    "            results['dependencies'].extend(country_result['dependencies'])\n",
    "            results['top_dependencies'].extend(country_result['top_dependencies'])\n",
    "            country, avg_dep = country_result['avg_dependency']\n",
    "            results['avg_dependencies'][country] = avg_dep\n",
    "            results['length_distribution'] += country_result['length_distribution']\n",
    "            \n",
    "            # Actualizar barra de progreso\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    # Cerrar el pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Ordenar top dependencies\n",
    "    results['top_dependencies'].sort(key=lambda x: x[4], reverse=True)\n",
    "    results['top_dependencies'] = results['top_dependencies'][:90]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "\n",
    "def calculate_path_dependency(X_clean, path, denominators):\n",
    "    \"\"\"Calcula la dependencia de un camino específico.\"\"\"\n",
    "    fuerza_camino = 1.0\n",
    "    for k in range(len(path) - 1):\n",
    "        if denominators[path[k+1]] > 0:\n",
    "            fuerza_camino *= X_clean[path[k], path[k+1]] / denominators[path[k+1]]\n",
    "        else:\n",
    "            fuerza_camino = 0  # Si el denominador es cero, la fuerza del camino es cero\n",
    "    return fuerza_camino\n",
    "\n",
    "def calculate_intermediary_centrality(intermediary_frequency, intermediary_strength, country_names):\n",
    "    \"\"\"\n",
    "    Calcula métricas de centralidad para intermediarios.\n",
    "    \n",
    "    Esta función es la misma que la original.\n",
    "    \"\"\"\n",
    "    # Implementación existente\n",
    "    centrality = []\n",
    "    \n",
    "    # Normalizar\n",
    "    max_freq = max(intermediary_frequency.values()) if intermediary_frequency.values() else 1\n",
    "    max_strength = max(intermediary_strength.values()) if intermediary_strength.values() else 1\n",
    "    \n",
    "    for country in country_names:\n",
    "        norm_freq = intermediary_frequency[country] / max_freq if max_freq > 0 else 0\n",
    "        norm_strength = intermediary_strength[country] / max_strength if max_strength > 0 else 0\n",
    "        \n",
    "        combined_score = 0.4 * norm_freq + 0.6 * norm_strength\n",
    "        \n",
    "        centrality.append((country, intermediary_frequency[country], \n",
    "                          intermediary_strength[country], combined_score))\n",
    "    \n",
    "    centrality.sort(key=lambda x: x[3], reverse=True)\n",
    "    return centrality\n",
    "\n",
    "def process_country_pair(i, j, X_clean, denominators, country_names, max_possible_length, \n",
    "                        convergence_threshold, path_strength_threshold):\n",
    "    \"\"\"\n",
    "    Procesa un par de países para calcular sus dependencias.\n",
    "    Esta función será ejecutada en paralelo.\n",
    "    \"\"\"\n",
    "    # Inicialización\n",
    "    current_total = 0\n",
    "    indirect_total = 0\n",
    "    dependencies_by_length = {}\n",
    "    significant_paths = []\n",
    "    length = 1\n",
    "    \n",
    "    # Dependencia directa\n",
    "    trade_value = X_clean[j, i]\n",
    "    direct_dependency = X_clean[j, i] / denominators[i] if denominators[i] > 0 else 0\n",
    "    dependencies_by_length[1] = direct_dependency\n",
    "    current_total = direct_dependency\n",
    "    \n",
    "    # Clave para identificar el par de países\n",
    "    pair_key = f\"{country_names[j]}->{country_names[i]}\"\n",
    "    \n",
    "    # Dependencias indirectas\n",
    "    for length in range(2, max_possible_length + 1):\n",
    "        DI_ij_l = 0\n",
    "        middle_countries = [k for k in range(X_clean.shape[0]) if k != i and k != j]\n",
    "\n",
    "        # Examinar cada combinación posible de intermediarios\n",
    "        for intermediaries in combinations(middle_countries, length - 1):\n",
    "            path = (j,) + intermediaries + (i,)\n",
    "            fuerza_camino = calculate_path_dependency(X_clean, path, denominators)\n",
    "            DI_ij_l += fuerza_camino\n",
    "            \n",
    "            # Registrar caminos significativos\n",
    "            if fuerza_camino > path_strength_threshold:\n",
    "                # Convertir índices a nombres de países\n",
    "                path_countries = [country_names[idx] for idx in path]\n",
    "                \n",
    "                # Registrar este camino\n",
    "                path_info = {\n",
    "                    'exportador': path_countries[0],\n",
    "                    'importador': path_countries[-1],\n",
    "                    'intermediarios': path_countries[1:-1],\n",
    "                    'fuerza': fuerza_camino,\n",
    "                    'longitud': length\n",
    "                }\n",
    "                \n",
    "                significant_paths.append(path_info)\n",
    "                \n",
    "                # No podemos actualizar directamente las estructuras globales en paralelo\n",
    "                # Devolveremos estos valores para actualizar después\n",
    "                intermediary_updates = []\n",
    "                for interm_idx, interm_country in enumerate(path_countries[1:-1]):\n",
    "                    weight_factor = 1.0 / (interm_idx + 1)\n",
    "                    intermediary_updates.append((interm_country, weight_factor * fuerza_camino))\n",
    "                \n",
    "        dependencies_by_length[length] = DI_ij_l\n",
    "        indirect_total += DI_ij_l\n",
    "\n",
    "        prev_total = current_total\n",
    "        current_total = direct_dependency + indirect_total\n",
    "\n",
    "        # Criterio de convergencia\n",
    "        if length > 1 and abs(current_total - prev_total) < convergence_threshold:\n",
    "            break\n",
    "\n",
    "    # La dependencia total es la suma de la directa y la indirecta\n",
    "    total_dependency = direct_dependency + indirect_total\n",
    "\n",
    "    result = {\n",
    "        'importador': country_names[i],\n",
    "        'exportador': country_names[j],\n",
    "        'trade_value': trade_value,\n",
    "        'dependencia_directa': direct_dependency,\n",
    "        'dependencia_indirecta': indirect_total,\n",
    "        'dependencia_total': total_dependency,\n",
    "        'longitud_optima': length,\n",
    "        'dependencias_por_longitud': dependencies_by_length\n",
    "    }\n",
    "    \n",
    "    # Ordenar caminos significativos\n",
    "    significant_paths.sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    \n",
    "    # Resultados a devolver para este par de países\n",
    "    return {\n",
    "        'pair_key': pair_key,\n",
    "        'result': result,\n",
    "        'top_dependency': (country_names[i], country_names[j], direct_dependency,\n",
    "                         indirect_total, total_dependency, length),\n",
    "        'significant_paths': significant_paths,\n",
    "        'length_converged': length if length > 1 else 0\n",
    "    }\n",
    "\n",
    "def calculate_all_dependencies_parallel(X, country_names=None, convergence_threshold=0.01, \n",
    "                                       max_possible_length=5, threshold_pct=0.1, \n",
    "                                       path_strength_threshold=0.001, n_jobs=None, use_gpu=True, \n",
    "                                       debug_mode=False):\n",
    "    \"\"\"\n",
    "    Versión paralelizada del cálculo de dependencias que mantiene EXACTAMENTE\n",
    "    la misma salida que la versión original.\n",
    "    \n",
    "    El parámetro debug_mode permite verificar que el número de dependencias\n",
    "    coincida con la versión original.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Versión paralelizada del cálculo de dependencias.\n",
    "    \n",
    "    Parameters adicionales:\n",
    "    -----------------------\n",
    "    n_jobs : int, opcional\n",
    "        Número de trabajos paralelos. Si es None, usa todos los núcleos disponibles.\n",
    "    use_gpu : bool, default=True\n",
    "        Si se debe intentar usar GPU para acelerar algunos cálculos.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "\n",
    "    if country_names is None:\n",
    "        country_names = [f\"País {i}\" for i in range(n)]\n",
    "\n",
    "    if len(country_names) != n:\n",
    "        raise ValueError(f\"La longitud de country_names ({len(country_names)}) no coincide con la dimensión de X ({n})\")\n",
    "\n",
    "    # Configurar paralelización\n",
    "    if n_jobs is None:\n",
    "        n_jobs = multiprocessing.cpu_count()\n",
    "    \n",
    "    # Verificar disponibilidad de GPU\n",
    "    gpu_available = torch.cuda.is_available() and use_gpu\n",
    "\n",
    "    total_trade = np.sum(X)\n",
    "    threshold = total_trade * (threshold_pct / 100)\n",
    "    X_clean = np.where(X < threshold, 0, X)\n",
    "    denominators = np.sum(X, axis=0)\n",
    "\n",
    "    # Acelerar cálculos directos con GPU si está disponible\n",
    "    if gpu_available:\n",
    "        # Transferir datos a GPU\n",
    "        X_gpu = torch.tensor(X_clean, device='cuda', dtype=torch.float32)\n",
    "        denom_gpu = torch.tensor(denominators, device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        # Calcular dependencias directas en forma vectorizada\n",
    "        direct_deps = torch.zeros_like(X_gpu)\n",
    "        for i in range(n):\n",
    "            # Evitar división por cero\n",
    "            if denom_gpu[i] > 0:\n",
    "                direct_deps[:, i] = X_gpu[:, i] / denom_gpu[i]\n",
    "        \n",
    "        # Transferir resultados de vuelta a CPU\n",
    "        direct_dependencies = direct_deps.cpu().numpy()\n",
    "        \n",
    "        # Usar estas dependencias directas precalculadas en el procesamiento posterior\n",
    "        # (Aunque en esta implementación seguimos calculándolas en process_country_pair para\n",
    "        # mantener cambios mínimos en el código)\n",
    "\n",
    "    # Estructura de resultados extendida\n",
    "    results = {\n",
    "        'dependencies': [],\n",
    "        'top_dependencies': [],\n",
    "        'avg_dependencies': {},\n",
    "        'length_distribution': np.zeros(max_possible_length),\n",
    "        'critical_intermediaries': {},     # Intermediarios críticos por relación\n",
    "        'intermediary_frequency': {},      # Frecuencia de países como intermediarios\n",
    "        'critical_paths': [],              # Rutas críticas completas\n",
    "        'intermediary_strength': {}        # Fuerza de cada país como intermediario\n",
    "    }\n",
    "    \n",
    "    # Inicializar contadores para intermediarios\n",
    "    for country in country_names:\n",
    "        results['intermediary_frequency'][country] = 0\n",
    "        results['intermediary_strength'][country] = 0.0\n",
    "\n",
    "    # Preparar pares de países para procesamiento paralelo \n",
    "    # Mantenemos la misma estructura de iteración del código original\n",
    "    # Primero por importador (i) y luego por exportador (j)\n",
    "    country_pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                country_pairs.append((i, j))\n",
    "    \n",
    "    # Procesar pares de países en paralelo\n",
    "    with Parallel(n_jobs=n_jobs) as parallel:\n",
    "        pair_results = parallel(\n",
    "            delayed(process_country_pair)(\n",
    "                i, j, X_clean, denominators, country_names, \n",
    "                max_possible_length, convergence_threshold, path_strength_threshold\n",
    "            ) \n",
    "            for i, j in tqdm(country_pairs, desc=\"Calculando dependencias\")\n",
    "        )\n",
    "        \n",
    "    # Agrupar resultados por país importador\n",
    "    results_by_importer = {}\n",
    "    for res in pair_results:\n",
    "        importer = res['result']['importador']\n",
    "        if importer not in results_by_importer:\n",
    "            results_by_importer[importer] = []\n",
    "        results_by_importer[importer].append(res)\n",
    "    \n",
    "    # Recolectar critical paths de todos los pares para ordenarlos después\n",
    "    all_critical_paths = []\n",
    "    \n",
    "    # Procesar los resultados manteniendo el mismo orden que el código original\n",
    "    for i in range(n):\n",
    "        importer = country_names[i]\n",
    "        total_dep = 0.0\n",
    "        num_deps = 0\n",
    "        \n",
    "        if importer in results_by_importer:\n",
    "            for res in results_by_importer[importer]:\n",
    "                # Agregar a dependencies\n",
    "                results['dependencies'].append(res['result'])\n",
    "                \n",
    "                # Agregar a top_dependencies\n",
    "                results['top_dependencies'].append(res['top_dependency'])\n",
    "                \n",
    "                # Actualizar critical_intermediaries\n",
    "                results['critical_intermediaries'][res['pair_key']] = res['significant_paths']\n",
    "                \n",
    "                # Recolectar critical paths\n",
    "                all_critical_paths.extend(res['significant_paths'])\n",
    "                \n",
    "                # Actualizar length_distribution si convergió\n",
    "                if res['length_converged'] > 1:\n",
    "                    results['length_distribution'][res['length_converged'] - 1] += 1\n",
    "                \n",
    "                # Actualizar dependencia promedio\n",
    "                total_dep += res['result']['dependencia_total']\n",
    "                num_deps += 1\n",
    "                \n",
    "                # Actualizar estadísticas de intermediarios\n",
    "                for path in res['significant_paths']:\n",
    "                    for idx, interm in enumerate(path['intermediarios']):\n",
    "                        # Incrementar frecuencia\n",
    "                        results['intermediary_frequency'][interm] += 1\n",
    "                        \n",
    "                        # Incrementar fuerza ponderada\n",
    "                        weight_factor = 1.0 / (idx + 1)\n",
    "                        results['intermediary_strength'][interm] += path['fuerza'] * weight_factor\n",
    "        \n",
    "        # Guardar dependencia promedio para este importador\n",
    "        results['avg_dependencies'][importer] = total_dep / num_deps if num_deps > 0 else 0\n",
    "    \n",
    "    # Añadir y ordenar los critical paths (igual que el original)\n",
    "    results['critical_paths'] = all_critical_paths\n",
    "    \n",
    "    # Ordenar top dependencies\n",
    "    results['top_dependencies'].sort(key=lambda x: x[4], reverse=True)\n",
    "    results['top_dependencies'] = results['top_dependencies'][:90]\n",
    "    \n",
    "    # Ordenar critical_paths por fuerza\n",
    "    results['critical_paths'].sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    \n",
    "    # Calcular métricas de centralidad para intermediarios\n",
    "    results['intermediary_centrality'] = calculate_intermediary_centrality(\n",
    "        results['intermediary_frequency'], \n",
    "        results['intermediary_strength'],\n",
    "        country_names\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Para mantener compatibilidad, redefinimos la función original\n",
    "# para que utilice la versión paralelizada\n",
    "def calculate_all_dependencies(X, country_names=None, convergence_threshold=0.01, \n",
    "                              max_possible_length=5, threshold_pct=0.1, \n",
    "                              path_strength_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Calcula todas las dependencias entre países con análisis de intermediarios críticos.\n",
    "    \n",
    "    Esta función mantiene EXACTAMENTE la misma firma y resultados que la original,\n",
    "    pero utiliza internamente paralelización y GPU para acelerar los cálculos.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Matriz de comercio\n",
    "    country_names : list, opcional\n",
    "        Nombres de los países\n",
    "    convergence_threshold : float, default=0.01\n",
    "        Umbral para determinar la convergencia\n",
    "    max_possible_length : int, default=5\n",
    "        Longitud máxima de caminos a considerar\n",
    "    threshold_pct : float, default=0.1\n",
    "        Umbral para filtrar valores de comercio insignificantes (porcentaje)\n",
    "    path_strength_threshold : float, default=0.001\n",
    "        Umbral mínimo para considerar una ruta como significativa\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Diccionario con todos los resultados del análisis\n",
    "    \"\"\"\n",
    "    # Determinar si usar paralelización basado en el tamaño del problema\n",
    "    use_parallel = X.shape[0] > 5  # Para matrices muy pequeñas no vale la pena paralelizar\n",
    "    \n",
    "    # Verificar disponibilidad de GPU\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    # Configurar número de trabajos para paralelización\n",
    "    n_countries = X.shape[0]\n",
    "    n_jobs = min(multiprocessing.cpu_count(), n_countries)  # Limitar al número de países\n",
    "    \n",
    "    if use_parallel:\n",
    "        # Usar la versión paralelizada\n",
    "        return calculate_all_dependencies_parallel(\n",
    "            X, country_names, convergence_threshold, \n",
    "            max_possible_length, threshold_pct, path_strength_threshold,\n",
    "            n_jobs=n_jobs, use_gpu=use_gpu, debug_mode=False\n",
    "        )\n",
    "    else:\n",
    "        # Para matrices muy pequeñas, usar un solo proceso\n",
    "        return calculate_all_dependencies_parallel(\n",
    "            X, country_names, convergence_threshold, \n",
    "            max_possible_length, threshold_pct, path_strength_threshold,\n",
    "            n_jobs=1, use_gpu=use_gpu, debug_mode=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar y guardar resultados de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando industria: Accumulators primary cells and batteries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando dependencias:  16%|█▌        | 8720/54056 [00:30<01:43, 437.54it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m country_names \u001b[38;5;241m=\u001b[39m mat_clean\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Calcular dependencias\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m calculate_all_dependencies(X, country_names)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Guardar resultados\u001b[39;00m\n\u001b[0;32m     25\u001b[0m all_results[industry] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m: results,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry_names\u001b[39m\u001b[38;5;124m'\u001b[39m: country_names,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatrix_shape\u001b[39m\u001b[38;5;124m'\u001b[39m: mat_clean\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     29\u001b[0m }\n",
      "Cell \u001b[1;32mIn[5], line 347\u001b[0m, in \u001b[0;36mcalculate_all_dependencies\u001b[1;34m(X, country_names, convergence_threshold, max_possible_length, threshold_pct, path_strength_threshold)\u001b[0m\n\u001b[0;32m    343\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(multiprocessing\u001b[38;5;241m.\u001b[39mcpu_count(), n_countries)  \u001b[38;5;66;03m# Limitar al número de países\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_parallel:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;66;03m# Usar la versión paralelizada\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m calculate_all_dependencies_parallel(\n\u001b[0;32m    348\u001b[0m         X, country_names, convergence_threshold, \n\u001b[0;32m    349\u001b[0m         max_possible_length, threshold_pct, path_strength_threshold,\n\u001b[0;32m    350\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, use_gpu\u001b[38;5;241m=\u001b[39muse_gpu, debug_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     )\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;66;03m# Para matrices muy pequeñas, usar un solo proceso\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m calculate_all_dependencies_parallel(\n\u001b[0;32m    355\u001b[0m         X, country_names, convergence_threshold, \n\u001b[0;32m    356\u001b[0m         max_possible_length, threshold_pct, path_strength_threshold,\n\u001b[0;32m    357\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, use_gpu\u001b[38;5;241m=\u001b[39muse_gpu, debug_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[5], line 225\u001b[0m, in \u001b[0;36mcalculate_all_dependencies_parallel\u001b[1;34m(X, country_names, convergence_threshold, max_possible_length, threshold_pct, path_strength_threshold, n_jobs, use_gpu, debug_mode)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Procesar pares de países en paralelo\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs) \u001b[38;5;28;01mas\u001b[39;00m parallel:\n\u001b[1;32m--> 225\u001b[0m     pair_results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    226\u001b[0m         delayed(process_country_pair)(\n\u001b[0;32m    227\u001b[0m             i, j, X_clean, denominators, country_names, \n\u001b[0;32m    228\u001b[0m             max_possible_length, convergence_threshold, path_strength_threshold\n\u001b[0;32m    229\u001b[0m         ) \n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m tqdm(country_pairs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculando dependencias\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     )\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Agrupar resultados por país importador\u001b[39;00m\n\u001b[0;32m    234\u001b[0m results_by_importer \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\envs\\dependencias\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\envs\\dependencias\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\envs\\dependencias\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Diccionario para guardar todos los resultados\n",
    "all_results = {}\n",
    "\n",
    "# Procesar cada industria\n",
    "for industry, mat in matrices_comercio.items():\n",
    "    print(f\"\\nProcesando industria: {industry}\")\n",
    "    \n",
    "    # Limpiar matriz\n",
    "    mat_clean = eliminar_filas_columnas_cero(mat)\n",
    "    \n",
    "    # Si la matriz limpia está vacía o es muy pequeña, continuamos con la siguiente\n",
    "    if mat_clean.shape[0] < 2:\n",
    "        print(f\"Matriz demasiado pequeña para industria {industry}\")\n",
    "        continue\n",
    "        \n",
    "    # Convertir a numpy array y obtener nombres de países\n",
    "    X = mat_clean.values\n",
    "    country_names = mat_clean.columns.tolist()\n",
    "\n",
    "     \n",
    "    # Calcular dependencias\n",
    "    results = calculate_all_dependencies(X, country_names)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    all_results[industry] = {\n",
    "        'results': results,\n",
    "        'country_names': country_names,\n",
    "        'matrix_shape': mat_clean.shape\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar y guadar datos de intermediarios Críticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_intermediaries(results, top_n=10):\n",
    "    \"\"\"\n",
    "    Obtiene los países más importantes como intermediarios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del análisis con intermediarios\n",
    "    top_n : int\n",
    "        Número de países a mostrar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Lista de tuplas (país, score, frecuencia, fuerza) ordenadas por importancia\n",
    "    \"\"\"\n",
    "    # La estructura de intermediary_centrality ha cambiado en la nueva implementación\n",
    "    # Ahora es una lista de tuplas (país, frecuencia, fuerza, score_combinado)\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementación: ya está ordenada por score combinado\n",
    "        return results['intermediary_centrality'][:top_n]\n",
    "    else:\n",
    "        # Implementación anterior (por compatibilidad)\n",
    "        centrality_scores = [(country, metrics) \n",
    "                            for country, metrics in results['intermediary_centrality'].items()]\n",
    "        centrality_scores.sort(key=lambda x: x[1]['centrality_score'], reverse=True)\n",
    "        return centrality_scores[:top_n]\n",
    "\n",
    "def analyze_country_intermediary_role(results, country):\n",
    "    \"\"\"\n",
    "    Analiza el papel de un país específico como intermediario.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del análisis con intermediarios\n",
    "    country : str\n",
    "        País a analizar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Información detallada sobre el rol del país como intermediario\n",
    "    \"\"\"\n",
    "    # Verificar si el país existe en los resultados\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementación - lista de tuplas\n",
    "        country_entry = next((entry for entry in results['intermediary_centrality'] \n",
    "                             if entry[0] == country), None)\n",
    "        \n",
    "        if country_entry is None:\n",
    "            return {\"error\": f\"El país {country} no está en los resultados\"}\n",
    "            \n",
    "        # Extraer métricas\n",
    "        centrality = {\n",
    "            \"frequency\": country_entry[1],\n",
    "            \"strength\": country_entry[2],\n",
    "            \"centrality_score\": country_entry[3]\n",
    "        }\n",
    "    else:\n",
    "        # Implementación anterior\n",
    "        if country not in results['intermediary_centrality']:\n",
    "            return {\"error\": f\"El país {country} no está en los resultados\"}\n",
    "        centrality = results['intermediary_centrality'][country]\n",
    "    \n",
    "    # Encontrar las rutas más importantes donde este país actúa como intermediario\n",
    "    top_paths = []\n",
    "    for path in results['critical_paths']:\n",
    "        if country in path['intermediarios']:\n",
    "            top_paths.append(path)\n",
    "    \n",
    "    # Ordenar por fuerza del camino\n",
    "    top_paths.sort(key=lambda x: x['fuerza'], reverse=True)\n",
    "    \n",
    "    # Filtrar los 10 caminos más importantes\n",
    "    top_paths = top_paths[:10]\n",
    "    \n",
    "    return {\n",
    "        \"metrics\": centrality,\n",
    "        \"top_paths\": top_paths,\n",
    "        \"role_summary\": {\n",
    "            \"total_paths\": len(top_paths),\n",
    "            \"average_path_strength\": sum(p['fuerza'] for p in top_paths) / len(top_paths) if top_paths else 0,\n",
    "            \"max_path_strength\": max(p['fuerza'] for p in top_paths) if top_paths else 0,\n",
    "            \"unique_exporters\": len(set(p['exportador'] for p in top_paths)),\n",
    "            \"unique_importers\": len(set(p['importador'] for p in top_paths))\n",
    "        }\n",
    "    }\n",
    "\n",
    "def summarize_country_dependencies(results, country, top_n=5):\n",
    "    \"\"\"\n",
    "    Genera un resumen de las dependencias comerciales de un país específico.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del análisis de dependencias\n",
    "    country : str\n",
    "        País a analizar\n",
    "    top_n : int\n",
    "        Número de dependencias principales a mostrar\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Resumen de dependencias del país\n",
    "    \"\"\"\n",
    "    # Dependencias como importador (otros países exportan a este país)\n",
    "    import_dependencies = []\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['importador'] == country:\n",
    "            import_dependencies.append(dep)\n",
    "    \n",
    "    # Ordenar por dependencia total\n",
    "    import_dependencies.sort(key=lambda x: x['dependencia_total'], reverse=True)\n",
    "    \n",
    "    # Dependencias como exportador (este país exporta a otros)\n",
    "    export_dependencies = []\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['exportador'] == country:\n",
    "            export_dependencies.append(dep)\n",
    "    \n",
    "    # Ordenar por dependencia total\n",
    "    export_dependencies.sort(key=lambda x: x['dependencia_total'], reverse=True)\n",
    "    \n",
    "    # Calcular dependencia promedio\n",
    "    avg_dependency = results['avg_dependencies'].get(country, 0)\n",
    "    \n",
    "    # Analizar el papel como intermediario\n",
    "    intermediary_role = None\n",
    "    if isinstance(results['intermediary_centrality'], list):\n",
    "        # Nueva implementación\n",
    "        intermediary_info = next((x for x in results['intermediary_centrality'] if x[0] == country), None)\n",
    "        if intermediary_info:\n",
    "            intermediary_role = {\n",
    "                \"frequency\": intermediary_info[1],\n",
    "                \"strength\": intermediary_info[2],\n",
    "                \"centrality_score\": intermediary_info[3],\n",
    "                \"rank\": next((i+1 for i, x in enumerate(results['intermediary_centrality']) \n",
    "                             if x[0] == country), None)\n",
    "            }\n",
    "    else:\n",
    "        # Implementación anterior\n",
    "        if country in results['intermediary_centrality']:\n",
    "            intermediary_role = results['intermediary_centrality'][country]\n",
    "            # Calcular rango\n",
    "            countries_sorted = sorted(results['intermediary_centrality'].keys(), \n",
    "                                     key=lambda x: results['intermediary_centrality'][x]['centrality_score'],\n",
    "                                     reverse=True)\n",
    "            intermediary_role[\"rank\"] = countries_sorted.index(country) + 1\n",
    "    \n",
    "    return {\n",
    "        \"country\": country,\n",
    "        \"avg_dependency\": avg_dependency,\n",
    "        \"top_import_dependencies\": import_dependencies[:top_n],\n",
    "        \"top_export_dependencies\": export_dependencies[:top_n],\n",
    "        \"total_import_dependencies\": len(import_dependencies),\n",
    "        \"total_export_dependencies\": len(export_dependencies),\n",
    "        \"intermediary_role\": intermediary_role\n",
    "    }\n",
    "\n",
    "def identify_critical_trade_relationships(results, threshold=0.7, min_paths=3):\n",
    "    \"\"\"\n",
    "    Identifica relaciones comerciales críticas con alta dependencia.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Resultados del análisis de dependencias\n",
    "    threshold : float\n",
    "        Umbral de dependencia para considerar una relación como crítica\n",
    "    min_paths : int\n",
    "        Número mínimo de caminos alternativos para evitar criticidad\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Lista de relaciones críticas\n",
    "    \"\"\"\n",
    "    critical_relationships = []\n",
    "    \n",
    "    # Analizar cada dependencia\n",
    "    for dep in results['dependencies']:\n",
    "        if dep['dependencia_total'] >= threshold:\n",
    "            # Buscar caminos alternativos\n",
    "            pair_key = f\"{dep['exportador']}->{dep['importador']}\"\n",
    "            alternative_paths = []\n",
    "            \n",
    "            if pair_key in results['critical_intermediaries']:\n",
    "                alternative_paths = results['critical_intermediaries'][pair_key]\n",
    "            \n",
    "            # Si hay pocos caminos alternativos, es una relación crítica\n",
    "            if len(alternative_paths) < min_paths:\n",
    "                critical_relationships.append({\n",
    "                    \"exportador\": dep['exportador'],\n",
    "                    \"importador\": dep['importador'],\n",
    "                    \"dependencia_total\": dep['dependencia_total'],\n",
    "                    \"dependencia_directa\": dep['dependencia_directa'],\n",
    "                    \"caminos_alternativos\": len(alternative_paths),\n",
    "                    \"criticidad\": 1.0 - (len(alternative_paths) / min_paths) \n",
    "                                     if min_paths > 0 else 1.0\n",
    "                })\n",
    "    \n",
    "    # Ordenar por criticidad\n",
    "    critical_relationships.sort(key=lambda x: x['criticidad'], reverse=True)\n",
    "    \n",
    "    return critical_relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm = get_top_intermediaries(all_results['Aircraft and spacecraft']['results'])\n",
    "role = analyze_country_intermediary_role(results, 'DEU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creo Dataframes para guardar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_dependencies_dataframe(all_results):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con los resultados de dependencias para todas las industrias.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_results : dict\n",
    "        Diccionario con los resultados por industria\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame con las columnas: industria, importador, exportador, \n",
    "        dependencia_total, dependencia_directa, dependencia_indirecta, longitud_optima\n",
    "    \"\"\"\n",
    "    # Lista para almacenar los datos de todas las industrias\n",
    "    all_data = []\n",
    "    \n",
    "    # Procesar cada industria\n",
    "    for industry, data in all_results.items():\n",
    "        # Obtener los resultados de esta industria\n",
    "        industry_results = data['results']['dependencies']\n",
    "        \n",
    "        # Añadir cada fila de resultados\n",
    "        for result in industry_results:\n",
    "            row = {\n",
    "                'industria': industry,\n",
    "                'importador': result['importador'],\n",
    "                'exportador': result['exportador'],\n",
    "                'dependencia_total': result['dependencia_total'],\n",
    "                'dependencia_directa': result['dependencia_directa'],\n",
    "                'dependencia_indirecta': result['dependencia_indirecta'],\n",
    "                'trade_value': result['trade_value'],\n",
    "                'longitud_optima': result['longitud_optima']\n",
    "            }\n",
    "            all_data.append(row)\n",
    "    \n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Ordenar el DataFrame\n",
    "    df = df.sort_values(['industria', 'dependencia_total'], ascending=[True, False])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ejemplo de uso:\n",
    "df = create_dependencies_dataframe(all_results)\n",
    "\n",
    "# Definir el nuevo mapeo de nombres de columnas\n",
    "nuevo_nombres = {\n",
    "    'industria': 'industry',\n",
    "    'importador': 'dependent_country',\n",
    "    'exportador': 'supplier_country',\n",
    "    'dependencia_total': 'dependency_value',\n",
    "    'dependencia_directa': 'direct_dependency',\n",
    "    'dependencia_indirecta': 'indirect_dependency',\n",
    "    'trade_value': 'trade_value',\n",
    "    'longitud_optima': 'longitud_optima'\n",
    "}\n",
    "\n",
    "# Renombrar las columnas\n",
    "df = df.rename(columns=nuevo_nombres)\n",
    "\n",
    "# Guardar como gzip\n",
    "with gzip.open(r\"C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\Dependencias consolidadas\\dependencias.csv.gz\", 'wt') as f:\n",
    "    df.to_csv(f, sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la media ponderada de forma segura\n",
    "def safe_weighted_average(group):\n",
    "    # Si los pesos suman cero, retornamos NaN\n",
    "    if group['trade_value'].sum() == 0:\n",
    "        return np.nan\n",
    "    return np.average(group['dependency_value'], weights=group['trade_value'])\n",
    "\n",
    "# 1. Agrupar por dependent_country y supplier_country\n",
    "weighted_dependencies = df.groupby(['dependent_country', 'supplier_country']).apply(\n",
    "    safe_weighted_average\n",
    ").reset_index(name='weighted_dependency')\n",
    "\n",
    "# 2. Eliminar los NaN si quieres (opcional)\n",
    "weighted_dependencies = weighted_dependencies.dropna()\n",
    "\n",
    "# 3. Ordenar los resultados\n",
    "weighted_dependencies = weighted_dependencies.sort_values(\n",
    "    ['dependent_country', 'weighted_dependency'], \n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Mostrar los primeros resultados\n",
    "print(\"\\nPrimeras 10 relaciones de dependencia:\")\n",
    "print(weighted_dependencies.head(10))\n",
    "\n",
    "# Mostrar algunos stats básicos\n",
    "print(\"\\nEstadísticas básicas de las dependencias ponderadas:\")\n",
    "print(weighted_dependencies['weighted_dependency'].describe())\n",
    "\n",
    "# Ver cuántas relaciones tenemos en total\n",
    "print(f\"\\nNúmero total de relaciones bilateral: {len(weighted_dependencies)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros originales: 738520\n",
      "Registros después de eliminar filas con ceros: 76747\n",
      "Se eliminaron 661773 registros\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer el archivo CSV\n",
    "# Nota: Uso sep=';' porque veo que las columnas están separadas por punto y coma\n",
    "df = pd.read_csv(r\"C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\Dependencias consolidadas\\dependencias.csv\\dependencias.csv\", sep=';')\n",
    "\n",
    "# Definir las columnas a revisar\n",
    "columnas_check = ['dependency_value', 'direct_dependency', 'indirect_dependency', 'trade_value']\n",
    "\n",
    "# Eliminar las filas donde todas las columnas especificadas son 0\n",
    "df_limpio = df[~(df[columnas_check] == 0).all(axis=1)]\n",
    "\n",
    "# Guardar el resultado en un nuevo archivo\n",
    "# Lo guardamos en la misma ubicación pero con un nombre diferente\n",
    "ruta_salida = r\"C:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\Dependencias consolidadas\\dependencias_limpio.csv\"\n",
    "df_limpio.to_csv(ruta_salida, sep=';', index=False)\n",
    "\n",
    "# Imprimir información sobre el proceso\n",
    "print(f\"Registros originales: {len(df)}\")\n",
    "print(f\"Registros después de eliminar filas con ceros: {len(df_limpio)}\")\n",
    "print(f\"Se eliminaron {len(df) - len(df_limpio)} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Leer el CSV\n",
    "df = df_limpio\n",
    "\n",
    "\n",
    "# Lista de países de interés\n",
    "paises_interes = ['CAN', 'MEX', 'CHN', 'JPN', 'GBR', 'ESP']\n",
    "\n",
    "# Crear un DataFrame con las principales dependencias por país\n",
    "top_dependencies = []\n",
    "\n",
    "for pais in paises_interes:\n",
    "    # Filtrar datos para el país y USA como proveedor\n",
    "    country_data = df[(df['dependent_country'] == pais) & \n",
    "                     (df['supplier_country'] == 'USA')]\n",
    "    \n",
    "    # Calcular dependencia total ponderada por valor comercial\n",
    "    country_top = (country_data\n",
    "                  .assign(total_dependency=lambda x: x['dependency_value'],\n",
    "                         weighted_dependency=lambda x: x['dependency_value'] * x['trade_value'])\n",
    "                  .nlargest(6, 'weighted_dependency')\n",
    "                  [['industry', 'trade_value', 'dependency_value', \n",
    "                    'direct_dependency', 'indirect_dependency']]\n",
    "                  .assign(country=pais))\n",
    "    \n",
    "    top_dependencies.append(country_top)\n",
    "\n",
    "# Combinar todos los resultados\n",
    "result_df = pd.concat(top_dependencies)\n",
    "\n",
    "# Formatear los valores para mejor visualización\n",
    "result_df['trade_value'] = result_df['trade_value'].round(2)\n",
    "result_df['dependency_value'] = (result_df['dependency_value'] * 100).round(2)\n",
    "result_df['direct_dependency'] = (result_df['direct_dependency'] * 100).round(2)\n",
    "result_df['indirect_dependency'] = (result_df['indirect_dependency'] * 100).round(2)\n",
    "\n",
    "# Mostrar la tabla\n",
    "print(\"\\nTop 6 industrias por país con mayor dependencia ponderada de USA:\")\n",
    "print(result_df.to_string(index=False))\n",
    "\n",
    "# Crear visualización\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Crear un heatmap usando plt.imshow\n",
    "pivot_data = result_df.pivot(index='country', \n",
    "                           columns='industry', \n",
    "                           values='dependency_value')\n",
    "\n",
    "plt.imshow(pivot_data, aspect='auto', cmap='YlOrRd')\n",
    "plt.colorbar(label='Dependencia (%)')\n",
    "\n",
    "# Configurar ejes\n",
    "plt.xticks(range(len(pivot_data.columns)), \n",
    "           pivot_data.columns, \n",
    "           rotation=45, \n",
    "           ha='right')\n",
    "plt.yticks(range(len(pivot_data.index)), \n",
    "           pivot_data.index)\n",
    "\n",
    "# Añadir valores en cada celda\n",
    "for i in range(len(pivot_data.index)):\n",
    "    for j in range(len(pivot_data.columns)):\n",
    "        value = pivot_data.iloc[i, j]\n",
    "        if not np.isnan(value):\n",
    "            plt.text(j, i, f'{value:.1f}%',\n",
    "                    ha='center', va='center',\n",
    "                    color='black' if value < 50 else 'white')\n",
    "\n",
    "plt.title('Dependencia por País e Industria (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Guardar los resultados en un CSV\n",
    "result_df.to_csv('top_dependencies_by_country.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dependencias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
